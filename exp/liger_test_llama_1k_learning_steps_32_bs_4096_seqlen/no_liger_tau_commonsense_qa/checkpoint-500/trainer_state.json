{
  "best_metric": 0.4377073347568512,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_32_bs_4096_seqlen/no_liger_tau_commonsense_qa/checkpoint-500",
  "epoch": 1.639344262295082,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003278688524590164,
      "grad_norm": 6.5188140869140625,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.61,
      "num_input_tokens_seen": 3584,
      "step": 1
    },
    {
      "epoch": 0.006557377049180328,
      "grad_norm": 6.1607842445373535,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.3704,
      "num_input_tokens_seen": 7168,
      "step": 2
    },
    {
      "epoch": 0.009836065573770493,
      "grad_norm": 6.837653160095215,
      "learning_rate": 1.5e-06,
      "loss": 1.4191,
      "num_input_tokens_seen": 11264,
      "step": 3
    },
    {
      "epoch": 0.013114754098360656,
      "grad_norm": 5.949800968170166,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.3399,
      "num_input_tokens_seen": 14336,
      "step": 4
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 6.162797927856445,
      "learning_rate": 2.5e-06,
      "loss": 1.4602,
      "num_input_tokens_seen": 17920,
      "step": 5
    },
    {
      "epoch": 0.019672131147540985,
      "grad_norm": 6.442310333251953,
      "learning_rate": 3e-06,
      "loss": 1.8,
      "num_input_tokens_seen": 20992,
      "step": 6
    },
    {
      "epoch": 0.022950819672131147,
      "grad_norm": 5.895863056182861,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.2743,
      "num_input_tokens_seen": 24576,
      "step": 7
    },
    {
      "epoch": 0.02622950819672131,
      "grad_norm": 7.062893867492676,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.5737,
      "num_input_tokens_seen": 28160,
      "step": 8
    },
    {
      "epoch": 0.029508196721311476,
      "grad_norm": 6.450771331787109,
      "learning_rate": 4.5e-06,
      "loss": 1.8743,
      "num_input_tokens_seen": 32256,
      "step": 9
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 6.192459583282471,
      "learning_rate": 5e-06,
      "loss": 1.6985,
      "num_input_tokens_seen": 36352,
      "step": 10
    },
    {
      "epoch": 0.036065573770491806,
      "grad_norm": 6.120199680328369,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.1876,
      "num_input_tokens_seen": 39936,
      "step": 11
    },
    {
      "epoch": 0.03934426229508197,
      "grad_norm": 5.444657325744629,
      "learning_rate": 6e-06,
      "loss": 0.8286,
      "num_input_tokens_seen": 43008,
      "step": 12
    },
    {
      "epoch": 0.04262295081967213,
      "grad_norm": 6.0980610847473145,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.4079,
      "num_input_tokens_seen": 46592,
      "step": 13
    },
    {
      "epoch": 0.04590163934426229,
      "grad_norm": 6.3448309898376465,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.5579,
      "num_input_tokens_seen": 49664,
      "step": 14
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 6.4367265701293945,
      "learning_rate": 7.5e-06,
      "loss": 1.5891,
      "num_input_tokens_seen": 53248,
      "step": 15
    },
    {
      "epoch": 0.05245901639344262,
      "grad_norm": 6.578485488891602,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.307,
      "num_input_tokens_seen": 57344,
      "step": 16
    },
    {
      "epoch": 0.05573770491803279,
      "grad_norm": 5.843918323516846,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.1047,
      "num_input_tokens_seen": 60416,
      "step": 17
    },
    {
      "epoch": 0.05901639344262295,
      "grad_norm": 6.7087812423706055,
      "learning_rate": 9e-06,
      "loss": 1.3899,
      "num_input_tokens_seen": 63488,
      "step": 18
    },
    {
      "epoch": 0.06229508196721312,
      "grad_norm": 6.829065322875977,
      "learning_rate": 9.5e-06,
      "loss": 1.5047,
      "num_input_tokens_seen": 67072,
      "step": 19
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 6.776569366455078,
      "learning_rate": 1e-05,
      "loss": 1.8484,
      "num_input_tokens_seen": 70144,
      "step": 20
    },
    {
      "epoch": 0.06885245901639345,
      "grad_norm": 6.1396484375,
      "learning_rate": 1.05e-05,
      "loss": 1.0967,
      "num_input_tokens_seen": 74240,
      "step": 21
    },
    {
      "epoch": 0.07213114754098361,
      "grad_norm": 6.064910888671875,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1533,
      "num_input_tokens_seen": 77824,
      "step": 22
    },
    {
      "epoch": 0.07540983606557378,
      "grad_norm": 7.388423442840576,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.3551,
      "num_input_tokens_seen": 81408,
      "step": 23
    },
    {
      "epoch": 0.07868852459016394,
      "grad_norm": 7.7296881675720215,
      "learning_rate": 1.2e-05,
      "loss": 2.3296,
      "num_input_tokens_seen": 84480,
      "step": 24
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 6.5341715812683105,
      "learning_rate": 1.25e-05,
      "loss": 1.459,
      "num_input_tokens_seen": 88064,
      "step": 25
    },
    {
      "epoch": 0.08524590163934426,
      "grad_norm": 5.700592517852783,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.0956,
      "num_input_tokens_seen": 91648,
      "step": 26
    },
    {
      "epoch": 0.08852459016393442,
      "grad_norm": 6.023982048034668,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.0474,
      "num_input_tokens_seen": 95232,
      "step": 27
    },
    {
      "epoch": 0.09180327868852459,
      "grad_norm": 5.9264607429504395,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.1124,
      "num_input_tokens_seen": 98816,
      "step": 28
    },
    {
      "epoch": 0.09508196721311475,
      "grad_norm": 5.680011749267578,
      "learning_rate": 1.45e-05,
      "loss": 1.0867,
      "num_input_tokens_seen": 102400,
      "step": 29
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 5.204654693603516,
      "learning_rate": 1.5e-05,
      "loss": 0.9171,
      "num_input_tokens_seen": 105984,
      "step": 30
    },
    {
      "epoch": 0.10163934426229508,
      "grad_norm": 5.815591812133789,
      "learning_rate": 1.55e-05,
      "loss": 1.1882,
      "num_input_tokens_seen": 109568,
      "step": 31
    },
    {
      "epoch": 0.10491803278688525,
      "grad_norm": 6.560079574584961,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.643,
      "num_input_tokens_seen": 112640,
      "step": 32
    },
    {
      "epoch": 0.10819672131147541,
      "grad_norm": 4.993837356567383,
      "learning_rate": 1.65e-05,
      "loss": 1.2167,
      "num_input_tokens_seen": 115712,
      "step": 33
    },
    {
      "epoch": 0.11147540983606558,
      "grad_norm": 4.165971755981445,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.7138,
      "num_input_tokens_seen": 118784,
      "step": 34
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 5.522929668426514,
      "learning_rate": 1.75e-05,
      "loss": 1.7186,
      "num_input_tokens_seen": 122368,
      "step": 35
    },
    {
      "epoch": 0.1180327868852459,
      "grad_norm": 3.8233683109283447,
      "learning_rate": 1.8e-05,
      "loss": 0.861,
      "num_input_tokens_seen": 125440,
      "step": 36
    },
    {
      "epoch": 0.12131147540983607,
      "grad_norm": 4.832708835601807,
      "learning_rate": 1.85e-05,
      "loss": 1.1416,
      "num_input_tokens_seen": 128512,
      "step": 37
    },
    {
      "epoch": 0.12459016393442623,
      "grad_norm": 4.314800262451172,
      "learning_rate": 1.9e-05,
      "loss": 1.1607,
      "num_input_tokens_seen": 132608,
      "step": 38
    },
    {
      "epoch": 0.12786885245901639,
      "grad_norm": 4.170958042144775,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.3409,
      "num_input_tokens_seen": 135680,
      "step": 39
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 3.8544294834136963,
      "learning_rate": 2e-05,
      "loss": 0.8999,
      "num_input_tokens_seen": 139264,
      "step": 40
    },
    {
      "epoch": 0.13442622950819672,
      "grad_norm": 3.2661173343658447,
      "learning_rate": 2.05e-05,
      "loss": 0.9225,
      "num_input_tokens_seen": 142848,
      "step": 41
    },
    {
      "epoch": 0.1377049180327869,
      "grad_norm": 2.5363047122955322,
      "learning_rate": 2.1e-05,
      "loss": 0.6297,
      "num_input_tokens_seen": 146432,
      "step": 42
    },
    {
      "epoch": 0.14098360655737704,
      "grad_norm": 3.090078353881836,
      "learning_rate": 2.15e-05,
      "loss": 0.7441,
      "num_input_tokens_seen": 150016,
      "step": 43
    },
    {
      "epoch": 0.14426229508196722,
      "grad_norm": 2.4363160133361816,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.6247,
      "num_input_tokens_seen": 153600,
      "step": 44
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 2.6434903144836426,
      "learning_rate": 2.25e-05,
      "loss": 0.9205,
      "num_input_tokens_seen": 157184,
      "step": 45
    },
    {
      "epoch": 0.15081967213114755,
      "grad_norm": 2.63490891456604,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8709,
      "num_input_tokens_seen": 160256,
      "step": 46
    },
    {
      "epoch": 0.1540983606557377,
      "grad_norm": 2.7512128353118896,
      "learning_rate": 2.35e-05,
      "loss": 0.4842,
      "num_input_tokens_seen": 163840,
      "step": 47
    },
    {
      "epoch": 0.15737704918032788,
      "grad_norm": 3.081547737121582,
      "learning_rate": 2.4e-05,
      "loss": 0.8966,
      "num_input_tokens_seen": 167424,
      "step": 48
    },
    {
      "epoch": 0.16065573770491803,
      "grad_norm": 3.214806318283081,
      "learning_rate": 2.45e-05,
      "loss": 0.9162,
      "num_input_tokens_seen": 171520,
      "step": 49
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 2.1501266956329346,
      "learning_rate": 2.5e-05,
      "loss": 0.7244,
      "num_input_tokens_seen": 174592,
      "step": 50
    },
    {
      "epoch": 0.16721311475409836,
      "grad_norm": 4.3434319496154785,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.0765,
      "num_input_tokens_seen": 178176,
      "step": 51
    },
    {
      "epoch": 0.17049180327868851,
      "grad_norm": 3.1733264923095703,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.63,
      "num_input_tokens_seen": 181760,
      "step": 52
    },
    {
      "epoch": 0.1737704918032787,
      "grad_norm": 2.987110137939453,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.8438,
      "num_input_tokens_seen": 185344,
      "step": 53
    },
    {
      "epoch": 0.17704918032786884,
      "grad_norm": 2.861588716506958,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.6466,
      "num_input_tokens_seen": 188928,
      "step": 54
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 3.503826141357422,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.9702,
      "num_input_tokens_seen": 192512,
      "step": 55
    },
    {
      "epoch": 0.18360655737704917,
      "grad_norm": 2.595539093017578,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.5731,
      "num_input_tokens_seen": 196096,
      "step": 56
    },
    {
      "epoch": 0.18688524590163935,
      "grad_norm": 3.186889171600342,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.8658,
      "num_input_tokens_seen": 200192,
      "step": 57
    },
    {
      "epoch": 0.1901639344262295,
      "grad_norm": 2.853811025619507,
      "learning_rate": 2.9e-05,
      "loss": 0.5278,
      "num_input_tokens_seen": 203264,
      "step": 58
    },
    {
      "epoch": 0.19344262295081968,
      "grad_norm": 2.7248032093048096,
      "learning_rate": 2.95e-05,
      "loss": 0.719,
      "num_input_tokens_seen": 206848,
      "step": 59
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 2.558488368988037,
      "learning_rate": 3e-05,
      "loss": 0.9559,
      "num_input_tokens_seen": 209920,
      "step": 60
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.995851993560791,
      "learning_rate": 3.05e-05,
      "loss": 0.5461,
      "num_input_tokens_seen": 213504,
      "step": 61
    },
    {
      "epoch": 0.20327868852459016,
      "grad_norm": 2.672010898590088,
      "learning_rate": 3.1e-05,
      "loss": 0.8242,
      "num_input_tokens_seen": 217600,
      "step": 62
    },
    {
      "epoch": 0.20655737704918034,
      "grad_norm": 3.5827887058258057,
      "learning_rate": 3.15e-05,
      "loss": 0.5215,
      "num_input_tokens_seen": 220672,
      "step": 63
    },
    {
      "epoch": 0.2098360655737705,
      "grad_norm": 3.5276291370391846,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.8876,
      "num_input_tokens_seen": 224256,
      "step": 64
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 3.0868122577667236,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.7228,
      "num_input_tokens_seen": 227840,
      "step": 65
    },
    {
      "epoch": 0.21639344262295082,
      "grad_norm": 2.653287887573242,
      "learning_rate": 3.3e-05,
      "loss": 0.654,
      "num_input_tokens_seen": 230912,
      "step": 66
    },
    {
      "epoch": 0.21967213114754097,
      "grad_norm": 2.917292356491089,
      "learning_rate": 3.35e-05,
      "loss": 0.7775,
      "num_input_tokens_seen": 234496,
      "step": 67
    },
    {
      "epoch": 0.22295081967213115,
      "grad_norm": 2.8175699710845947,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6791,
      "num_input_tokens_seen": 238080,
      "step": 68
    },
    {
      "epoch": 0.2262295081967213,
      "grad_norm": 3.0821950435638428,
      "learning_rate": 3.45e-05,
      "loss": 0.7878,
      "num_input_tokens_seen": 241152,
      "step": 69
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 3.608927011489868,
      "learning_rate": 3.5e-05,
      "loss": 0.8666,
      "num_input_tokens_seen": 244736,
      "step": 70
    },
    {
      "epoch": 0.23278688524590163,
      "grad_norm": 2.3972785472869873,
      "learning_rate": 3.55e-05,
      "loss": 0.6026,
      "num_input_tokens_seen": 248320,
      "step": 71
    },
    {
      "epoch": 0.2360655737704918,
      "grad_norm": 3.5404255390167236,
      "learning_rate": 3.6e-05,
      "loss": 0.7545,
      "num_input_tokens_seen": 251904,
      "step": 72
    },
    {
      "epoch": 0.23934426229508196,
      "grad_norm": 2.90112566947937,
      "learning_rate": 3.65e-05,
      "loss": 0.7535,
      "num_input_tokens_seen": 255488,
      "step": 73
    },
    {
      "epoch": 0.24262295081967214,
      "grad_norm": 3.1934704780578613,
      "learning_rate": 3.7e-05,
      "loss": 0.6899,
      "num_input_tokens_seen": 259072,
      "step": 74
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 3.3919546604156494,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.8654,
      "num_input_tokens_seen": 262656,
      "step": 75
    },
    {
      "epoch": 0.24918032786885247,
      "grad_norm": 2.8823843002319336,
      "learning_rate": 3.8e-05,
      "loss": 0.4239,
      "num_input_tokens_seen": 265728,
      "step": 76
    },
    {
      "epoch": 0.25245901639344265,
      "grad_norm": 2.8673617839813232,
      "learning_rate": 3.85e-05,
      "loss": 0.5531,
      "num_input_tokens_seen": 268800,
      "step": 77
    },
    {
      "epoch": 0.25573770491803277,
      "grad_norm": 4.5001678466796875,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5096,
      "num_input_tokens_seen": 271872,
      "step": 78
    },
    {
      "epoch": 0.25901639344262295,
      "grad_norm": 4.011695861816406,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.8952,
      "num_input_tokens_seen": 275456,
      "step": 79
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 2.678591251373291,
      "learning_rate": 4e-05,
      "loss": 0.5052,
      "num_input_tokens_seen": 279040,
      "step": 80
    },
    {
      "epoch": 0.26557377049180325,
      "grad_norm": 4.096640586853027,
      "learning_rate": 4.05e-05,
      "loss": 0.9682,
      "num_input_tokens_seen": 282624,
      "step": 81
    },
    {
      "epoch": 0.26885245901639343,
      "grad_norm": 4.38685941696167,
      "learning_rate": 4.1e-05,
      "loss": 0.6684,
      "num_input_tokens_seen": 285696,
      "step": 82
    },
    {
      "epoch": 0.2721311475409836,
      "grad_norm": 3.503343343734741,
      "learning_rate": 4.15e-05,
      "loss": 0.6842,
      "num_input_tokens_seen": 288768,
      "step": 83
    },
    {
      "epoch": 0.2754098360655738,
      "grad_norm": 3.357501983642578,
      "learning_rate": 4.2e-05,
      "loss": 0.6529,
      "num_input_tokens_seen": 292352,
      "step": 84
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 3.267526149749756,
      "learning_rate": 4.25e-05,
      "loss": 0.6655,
      "num_input_tokens_seen": 295936,
      "step": 85
    },
    {
      "epoch": 0.2819672131147541,
      "grad_norm": 2.7180981636047363,
      "learning_rate": 4.3e-05,
      "loss": 0.5365,
      "num_input_tokens_seen": 299520,
      "step": 86
    },
    {
      "epoch": 0.28524590163934427,
      "grad_norm": 3.8179781436920166,
      "learning_rate": 4.35e-05,
      "loss": 0.5025,
      "num_input_tokens_seen": 303104,
      "step": 87
    },
    {
      "epoch": 0.28852459016393445,
      "grad_norm": 3.6661059856414795,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.8183,
      "num_input_tokens_seen": 306688,
      "step": 88
    },
    {
      "epoch": 0.29180327868852457,
      "grad_norm": 3.1517367362976074,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.6814,
      "num_input_tokens_seen": 310272,
      "step": 89
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 5.341402530670166,
      "learning_rate": 4.5e-05,
      "loss": 0.7912,
      "num_input_tokens_seen": 313344,
      "step": 90
    },
    {
      "epoch": 0.2983606557377049,
      "grad_norm": 3.4602622985839844,
      "learning_rate": 4.55e-05,
      "loss": 0.8486,
      "num_input_tokens_seen": 317952,
      "step": 91
    },
    {
      "epoch": 0.3016393442622951,
      "grad_norm": 4.679762840270996,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.8906,
      "num_input_tokens_seen": 321024,
      "step": 92
    },
    {
      "epoch": 0.30491803278688523,
      "grad_norm": 3.108459949493408,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.3621,
      "num_input_tokens_seen": 324096,
      "step": 93
    },
    {
      "epoch": 0.3081967213114754,
      "grad_norm": 5.3925909996032715,
      "learning_rate": 4.7e-05,
      "loss": 0.7633,
      "num_input_tokens_seen": 327168,
      "step": 94
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 5.428933143615723,
      "learning_rate": 4.75e-05,
      "loss": 0.868,
      "num_input_tokens_seen": 330240,
      "step": 95
    },
    {
      "epoch": 0.31475409836065577,
      "grad_norm": 4.349754333496094,
      "learning_rate": 4.8e-05,
      "loss": 0.6429,
      "num_input_tokens_seen": 333824,
      "step": 96
    },
    {
      "epoch": 0.3180327868852459,
      "grad_norm": 3.7787842750549316,
      "learning_rate": 4.85e-05,
      "loss": 0.6172,
      "num_input_tokens_seen": 337408,
      "step": 97
    },
    {
      "epoch": 0.32131147540983607,
      "grad_norm": 3.267171859741211,
      "learning_rate": 4.9e-05,
      "loss": 0.2952,
      "num_input_tokens_seen": 340480,
      "step": 98
    },
    {
      "epoch": 0.32459016393442625,
      "grad_norm": 4.033388137817383,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.503,
      "num_input_tokens_seen": 343552,
      "step": 99
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 4.21160888671875,
      "learning_rate": 5e-05,
      "loss": 0.6006,
      "num_input_tokens_seen": 346624,
      "step": 100
    },
    {
      "epoch": 0.33114754098360655,
      "grad_norm": 3.6441245079040527,
      "learning_rate": 4.999984769144476e-05,
      "loss": 0.6267,
      "num_input_tokens_seen": 350208,
      "step": 101
    },
    {
      "epoch": 0.3344262295081967,
      "grad_norm": 5.110936164855957,
      "learning_rate": 4.999939076763487e-05,
      "loss": 0.94,
      "num_input_tokens_seen": 353280,
      "step": 102
    },
    {
      "epoch": 0.3377049180327869,
      "grad_norm": 4.2067790031433105,
      "learning_rate": 4.999862923413781e-05,
      "loss": 0.4607,
      "num_input_tokens_seen": 356864,
      "step": 103
    },
    {
      "epoch": 0.34098360655737703,
      "grad_norm": 4.892857074737549,
      "learning_rate": 4.999756310023261e-05,
      "loss": 0.6554,
      "num_input_tokens_seen": 359936,
      "step": 104
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 3.9959664344787598,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 0.3451,
      "num_input_tokens_seen": 363520,
      "step": 105
    },
    {
      "epoch": 0.3475409836065574,
      "grad_norm": 3.9227654933929443,
      "learning_rate": 4.999451708687114e-05,
      "loss": 0.6546,
      "num_input_tokens_seen": 366592,
      "step": 106
    },
    {
      "epoch": 0.35081967213114756,
      "grad_norm": 4.844535827636719,
      "learning_rate": 4.999253724452958e-05,
      "loss": 0.7813,
      "num_input_tokens_seen": 370176,
      "step": 107
    },
    {
      "epoch": 0.3540983606557377,
      "grad_norm": 5.3878960609436035,
      "learning_rate": 4.999025287600886e-05,
      "loss": 0.5866,
      "num_input_tokens_seen": 373248,
      "step": 108
    },
    {
      "epoch": 0.35737704918032787,
      "grad_norm": 4.534088134765625,
      "learning_rate": 4.998766400914329e-05,
      "loss": 0.5014,
      "num_input_tokens_seen": 376832,
      "step": 109
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 4.837634563446045,
      "learning_rate": 4.99847706754774e-05,
      "loss": 0.4816,
      "num_input_tokens_seen": 380416,
      "step": 110
    },
    {
      "epoch": 0.3639344262295082,
      "grad_norm": 4.334227561950684,
      "learning_rate": 4.998157291026553e-05,
      "loss": 0.5981,
      "num_input_tokens_seen": 384000,
      "step": 111
    },
    {
      "epoch": 0.36721311475409835,
      "grad_norm": 5.841100215911865,
      "learning_rate": 4.997807075247146e-05,
      "loss": 0.88,
      "num_input_tokens_seen": 387584,
      "step": 112
    },
    {
      "epoch": 0.3704918032786885,
      "grad_norm": 3.342902898788452,
      "learning_rate": 4.997426424476787e-05,
      "loss": 0.2691,
      "num_input_tokens_seen": 390656,
      "step": 113
    },
    {
      "epoch": 0.3737704918032787,
      "grad_norm": 5.532558441162109,
      "learning_rate": 4.997015343353585e-05,
      "loss": 0.7097,
      "num_input_tokens_seen": 393728,
      "step": 114
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 4.48861837387085,
      "learning_rate": 4.996573836886435e-05,
      "loss": 0.6508,
      "num_input_tokens_seen": 397312,
      "step": 115
    },
    {
      "epoch": 0.380327868852459,
      "grad_norm": 6.277336597442627,
      "learning_rate": 4.996101910454953e-05,
      "loss": 0.9525,
      "num_input_tokens_seen": 400896,
      "step": 116
    },
    {
      "epoch": 0.3836065573770492,
      "grad_norm": 5.347492694854736,
      "learning_rate": 4.995599569809414e-05,
      "loss": 0.8199,
      "num_input_tokens_seen": 403968,
      "step": 117
    },
    {
      "epoch": 0.38688524590163936,
      "grad_norm": 4.895506381988525,
      "learning_rate": 4.995066821070679e-05,
      "loss": 0.8019,
      "num_input_tokens_seen": 407040,
      "step": 118
    },
    {
      "epoch": 0.3901639344262295,
      "grad_norm": 3.8138327598571777,
      "learning_rate": 4.994503670730125e-05,
      "loss": 0.5839,
      "num_input_tokens_seen": 410112,
      "step": 119
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 3.953990936279297,
      "learning_rate": 4.993910125649561e-05,
      "loss": 0.5414,
      "num_input_tokens_seen": 413184,
      "step": 120
    },
    {
      "epoch": 0.39672131147540984,
      "grad_norm": 5.0134758949279785,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 0.6979,
      "num_input_tokens_seen": 416256,
      "step": 121
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.794315814971924,
      "learning_rate": 4.992631880567301e-05,
      "loss": 0.45,
      "num_input_tokens_seen": 419328,
      "step": 122
    },
    {
      "epoch": 0.40327868852459015,
      "grad_norm": 4.902629852294922,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.7424,
      "num_input_tokens_seen": 422912,
      "step": 123
    },
    {
      "epoch": 0.4065573770491803,
      "grad_norm": 4.354006290435791,
      "learning_rate": 4.991232148123761e-05,
      "loss": 0.6411,
      "num_input_tokens_seen": 426496,
      "step": 124
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 4.148015022277832,
      "learning_rate": 4.990486745229364e-05,
      "loss": 0.5694,
      "num_input_tokens_seen": 430080,
      "step": 125
    },
    {
      "epoch": 0.4131147540983607,
      "grad_norm": 4.548699378967285,
      "learning_rate": 4.989710996539926e-05,
      "loss": 0.6103,
      "num_input_tokens_seen": 433664,
      "step": 126
    },
    {
      "epoch": 0.4163934426229508,
      "grad_norm": 4.804324150085449,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 0.952,
      "num_input_tokens_seen": 437248,
      "step": 127
    },
    {
      "epoch": 0.419672131147541,
      "grad_norm": 3.7948429584503174,
      "learning_rate": 4.988068499954578e-05,
      "loss": 0.5677,
      "num_input_tokens_seen": 440832,
      "step": 128
    },
    {
      "epoch": 0.42295081967213116,
      "grad_norm": 3.9965672492980957,
      "learning_rate": 4.987201772071971e-05,
      "loss": 0.4669,
      "num_input_tokens_seen": 443904,
      "step": 129
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 4.920975208282471,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 0.5157,
      "num_input_tokens_seen": 447488,
      "step": 130
    },
    {
      "epoch": 0.42950819672131146,
      "grad_norm": 4.079237461090088,
      "learning_rate": 4.985377409930789e-05,
      "loss": 0.6709,
      "num_input_tokens_seen": 451072,
      "step": 131
    },
    {
      "epoch": 0.43278688524590164,
      "grad_norm": 3.565727710723877,
      "learning_rate": 4.984419797901491e-05,
      "loss": 0.4215,
      "num_input_tokens_seen": 454656,
      "step": 132
    },
    {
      "epoch": 0.4360655737704918,
      "grad_norm": 5.525984764099121,
      "learning_rate": 4.983431914000991e-05,
      "loss": 0.6163,
      "num_input_tokens_seen": 458240,
      "step": 133
    },
    {
      "epoch": 0.43934426229508194,
      "grad_norm": 5.6732025146484375,
      "learning_rate": 4.982413770266342e-05,
      "loss": 0.707,
      "num_input_tokens_seen": 461824,
      "step": 134
    },
    {
      "epoch": 0.4426229508196721,
      "grad_norm": 5.125631332397461,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 0.7351,
      "num_input_tokens_seen": 465920,
      "step": 135
    },
    {
      "epoch": 0.4459016393442623,
      "grad_norm": 5.5610432624816895,
      "learning_rate": 4.980286753286195e-05,
      "loss": 0.752,
      "num_input_tokens_seen": 468992,
      "step": 136
    },
    {
      "epoch": 0.4491803278688525,
      "grad_norm": 6.878386497497559,
      "learning_rate": 4.979177905957726e-05,
      "loss": 0.6869,
      "num_input_tokens_seen": 472064,
      "step": 137
    },
    {
      "epoch": 0.4524590163934426,
      "grad_norm": 4.347536087036133,
      "learning_rate": 4.978038850628854e-05,
      "loss": 0.6578,
      "num_input_tokens_seen": 475648,
      "step": 138
    },
    {
      "epoch": 0.4557377049180328,
      "grad_norm": 4.2351813316345215,
      "learning_rate": 4.976869601178609e-05,
      "loss": 0.4674,
      "num_input_tokens_seen": 478720,
      "step": 139
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 4.389028072357178,
      "learning_rate": 4.975670171853926e-05,
      "loss": 0.6101,
      "num_input_tokens_seen": 482304,
      "step": 140
    },
    {
      "epoch": 0.46229508196721314,
      "grad_norm": 4.995089054107666,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 0.3409,
      "num_input_tokens_seen": 485888,
      "step": 141
    },
    {
      "epoch": 0.46557377049180326,
      "grad_norm": 5.993484020233154,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 0.697,
      "num_input_tokens_seen": 488960,
      "step": 142
    },
    {
      "epoch": 0.46885245901639344,
      "grad_norm": 5.5560078620910645,
      "learning_rate": 4.971890952617515e-05,
      "loss": 0.3945,
      "num_input_tokens_seen": 492032,
      "step": 143
    },
    {
      "epoch": 0.4721311475409836,
      "grad_norm": 5.223941802978516,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 0.6027,
      "num_input_tokens_seen": 495616,
      "step": 144
    },
    {
      "epoch": 0.47540983606557374,
      "grad_norm": 4.371402740478516,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.5278,
      "num_input_tokens_seen": 499712,
      "step": 145
    },
    {
      "epoch": 0.4786885245901639,
      "grad_norm": 4.904158592224121,
      "learning_rate": 4.96784066268247e-05,
      "loss": 0.4992,
      "num_input_tokens_seen": 502784,
      "step": 146
    },
    {
      "epoch": 0.4819672131147541,
      "grad_norm": 4.863302230834961,
      "learning_rate": 4.966430404017424e-05,
      "loss": 0.4875,
      "num_input_tokens_seen": 506368,
      "step": 147
    },
    {
      "epoch": 0.4852459016393443,
      "grad_norm": 6.973005294799805,
      "learning_rate": 4.964990092676263e-05,
      "loss": 0.7039,
      "num_input_tokens_seen": 509952,
      "step": 148
    },
    {
      "epoch": 0.4885245901639344,
      "grad_norm": 7.7862467765808105,
      "learning_rate": 4.963519746208726e-05,
      "loss": 0.9275,
      "num_input_tokens_seen": 513536,
      "step": 149
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 9.967371940612793,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.9213,
      "num_input_tokens_seen": 517120,
      "step": 150
    },
    {
      "epoch": 0.49508196721311476,
      "grad_norm": 4.43303918838501,
      "learning_rate": 4.960489019923105e-05,
      "loss": 0.4908,
      "num_input_tokens_seen": 520192,
      "step": 151
    },
    {
      "epoch": 0.49836065573770494,
      "grad_norm": 4.340467929840088,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 0.5522,
      "num_input_tokens_seen": 523264,
      "step": 152
    },
    {
      "epoch": 0.5016393442622951,
      "grad_norm": 9.783637046813965,
      "learning_rate": 4.957338372873886e-05,
      "loss": 0.8988,
      "num_input_tokens_seen": 526336,
      "step": 153
    },
    {
      "epoch": 0.5049180327868853,
      "grad_norm": 7.405479431152344,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.8671,
      "num_input_tokens_seen": 529408,
      "step": 154
    },
    {
      "epoch": 0.5081967213114754,
      "grad_norm": 7.566676616668701,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 0.8191,
      "num_input_tokens_seen": 532480,
      "step": 155
    },
    {
      "epoch": 0.5114754098360655,
      "grad_norm": 5.669047832489014,
      "learning_rate": 4.952387888372979e-05,
      "loss": 0.6983,
      "num_input_tokens_seen": 536576,
      "step": 156
    },
    {
      "epoch": 0.5147540983606558,
      "grad_norm": 9.245692253112793,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 0.9368,
      "num_input_tokens_seen": 539648,
      "step": 157
    },
    {
      "epoch": 0.5180327868852459,
      "grad_norm": 5.488253116607666,
      "learning_rate": 4.94893812399836e-05,
      "loss": 0.5159,
      "num_input_tokens_seen": 542720,
      "step": 158
    },
    {
      "epoch": 0.521311475409836,
      "grad_norm": 5.797016620635986,
      "learning_rate": 4.947168471904213e-05,
      "loss": 0.6855,
      "num_input_tokens_seen": 545792,
      "step": 159
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 4.687489032745361,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 0.5948,
      "num_input_tokens_seen": 549888,
      "step": 160
    },
    {
      "epoch": 0.5278688524590164,
      "grad_norm": 3.5998899936676025,
      "learning_rate": 4.94353973571524e-05,
      "loss": 0.5063,
      "num_input_tokens_seen": 552960,
      "step": 161
    },
    {
      "epoch": 0.5311475409836065,
      "grad_norm": 4.9403977394104,
      "learning_rate": 4.94168069583542e-05,
      "loss": 0.652,
      "num_input_tokens_seen": 556544,
      "step": 162
    },
    {
      "epoch": 0.5344262295081967,
      "grad_norm": 5.310815334320068,
      "learning_rate": 4.939791904846869e-05,
      "loss": 0.6,
      "num_input_tokens_seen": 559616,
      "step": 163
    },
    {
      "epoch": 0.5377049180327869,
      "grad_norm": 4.192232608795166,
      "learning_rate": 4.937873385763908e-05,
      "loss": 0.7229,
      "num_input_tokens_seen": 563200,
      "step": 164
    },
    {
      "epoch": 0.5409836065573771,
      "grad_norm": 5.808932781219482,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 0.7061,
      "num_input_tokens_seen": 566784,
      "step": 165
    },
    {
      "epoch": 0.5442622950819672,
      "grad_norm": 4.780704975128174,
      "learning_rate": 4.933947257182901e-05,
      "loss": 0.5936,
      "num_input_tokens_seen": 570368,
      "step": 166
    },
    {
      "epoch": 0.5475409836065573,
      "grad_norm": 6.161563873291016,
      "learning_rate": 4.931939695523492e-05,
      "loss": 0.7473,
      "num_input_tokens_seen": 573440,
      "step": 167
    },
    {
      "epoch": 0.5508196721311476,
      "grad_norm": 4.44199800491333,
      "learning_rate": 4.929902501446366e-05,
      "loss": 0.4781,
      "num_input_tokens_seen": 577024,
      "step": 168
    },
    {
      "epoch": 0.5540983606557377,
      "grad_norm": 5.012207984924316,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 0.63,
      "num_input_tokens_seen": 580608,
      "step": 169
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 3.7073068618774414,
      "learning_rate": 4.925739315689991e-05,
      "loss": 0.3947,
      "num_input_tokens_seen": 584192,
      "step": 170
    },
    {
      "epoch": 0.5606557377049181,
      "grad_norm": 4.461935520172119,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 0.3936,
      "num_input_tokens_seen": 587776,
      "step": 171
    },
    {
      "epoch": 0.5639344262295082,
      "grad_norm": 4.048818111419678,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 0.3686,
      "num_input_tokens_seen": 591360,
      "step": 172
    },
    {
      "epoch": 0.5672131147540984,
      "grad_norm": 5.325460910797119,
      "learning_rate": 4.919272926204929e-05,
      "loss": 0.502,
      "num_input_tokens_seen": 594944,
      "step": 173
    },
    {
      "epoch": 0.5704918032786885,
      "grad_norm": 6.300634384155273,
      "learning_rate": 4.917058471511149e-05,
      "loss": 0.6601,
      "num_input_tokens_seen": 599040,
      "step": 174
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 5.7577362060546875,
      "learning_rate": 4.914814565722671e-05,
      "loss": 0.622,
      "num_input_tokens_seen": 602624,
      "step": 175
    },
    {
      "epoch": 0.5770491803278689,
      "grad_norm": 7.509003162384033,
      "learning_rate": 4.912541236180779e-05,
      "loss": 0.5527,
      "num_input_tokens_seen": 606208,
      "step": 176
    },
    {
      "epoch": 0.580327868852459,
      "grad_norm": 8.136812210083008,
      "learning_rate": 4.910238510585276e-05,
      "loss": 0.7397,
      "num_input_tokens_seen": 609792,
      "step": 177
    },
    {
      "epoch": 0.5836065573770491,
      "grad_norm": 7.426252365112305,
      "learning_rate": 4.907906416994146e-05,
      "loss": 0.6912,
      "num_input_tokens_seen": 613376,
      "step": 178
    },
    {
      "epoch": 0.5868852459016394,
      "grad_norm": 5.108124256134033,
      "learning_rate": 4.905544983823214e-05,
      "loss": 0.398,
      "num_input_tokens_seen": 616448,
      "step": 179
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 5.348730087280273,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 0.5443,
      "num_input_tokens_seen": 620032,
      "step": 180
    },
    {
      "epoch": 0.5934426229508196,
      "grad_norm": 5.7495436668396,
      "learning_rate": 4.900734214192358e-05,
      "loss": 0.551,
      "num_input_tokens_seen": 623104,
      "step": 181
    },
    {
      "epoch": 0.5967213114754099,
      "grad_norm": 7.654952049255371,
      "learning_rate": 4.898284936350144e-05,
      "loss": 0.6866,
      "num_input_tokens_seen": 626176,
      "step": 182
    },
    {
      "epoch": 0.6,
      "grad_norm": 5.458403587341309,
      "learning_rate": 4.895806436162833e-05,
      "loss": 0.5498,
      "num_input_tokens_seen": 630272,
      "step": 183
    },
    {
      "epoch": 0.6032786885245902,
      "grad_norm": 5.883691310882568,
      "learning_rate": 4.893298743830168e-05,
      "loss": 0.5368,
      "num_input_tokens_seen": 633344,
      "step": 184
    },
    {
      "epoch": 0.6065573770491803,
      "grad_norm": 5.370351314544678,
      "learning_rate": 4.890761889907589e-05,
      "loss": 0.479,
      "num_input_tokens_seen": 636416,
      "step": 185
    },
    {
      "epoch": 0.6098360655737705,
      "grad_norm": 7.350253105163574,
      "learning_rate": 4.888195905305859e-05,
      "loss": 0.723,
      "num_input_tokens_seen": 640000,
      "step": 186
    },
    {
      "epoch": 0.6131147540983607,
      "grad_norm": 5.029209136962891,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 0.617,
      "num_input_tokens_seen": 643072,
      "step": 187
    },
    {
      "epoch": 0.6163934426229508,
      "grad_norm": 4.740163803100586,
      "learning_rate": 4.882976669482367e-05,
      "loss": 0.3613,
      "num_input_tokens_seen": 646144,
      "step": 188
    },
    {
      "epoch": 0.6196721311475409,
      "grad_norm": 5.149991989135742,
      "learning_rate": 4.880323481855347e-05,
      "loss": 0.434,
      "num_input_tokens_seen": 649728,
      "step": 189
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 6.716485977172852,
      "learning_rate": 4.877641290737884e-05,
      "loss": 0.7403,
      "num_input_tokens_seen": 653312,
      "step": 190
    },
    {
      "epoch": 0.6262295081967213,
      "grad_norm": 6.617429256439209,
      "learning_rate": 4.874930128811631e-05,
      "loss": 0.4818,
      "num_input_tokens_seen": 656896,
      "step": 191
    },
    {
      "epoch": 0.6295081967213115,
      "grad_norm": 6.314476013183594,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 0.699,
      "num_input_tokens_seen": 659968,
      "step": 192
    },
    {
      "epoch": 0.6327868852459017,
      "grad_norm": 7.172500133514404,
      "learning_rate": 4.869421025023965e-05,
      "loss": 0.8157,
      "num_input_tokens_seen": 663040,
      "step": 193
    },
    {
      "epoch": 0.6360655737704918,
      "grad_norm": 8.897881507873535,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 1.035,
      "num_input_tokens_seen": 667136,
      "step": 194
    },
    {
      "epoch": 0.639344262295082,
      "grad_norm": 5.758449077606201,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 0.6128,
      "num_input_tokens_seen": 670720,
      "step": 195
    },
    {
      "epoch": 0.6426229508196721,
      "grad_norm": 6.319700241088867,
      "learning_rate": 4.860940925593703e-05,
      "loss": 0.5585,
      "num_input_tokens_seen": 673792,
      "step": 196
    },
    {
      "epoch": 0.6459016393442623,
      "grad_norm": 8.765830993652344,
      "learning_rate": 4.858056644869002e-05,
      "loss": 1.0511,
      "num_input_tokens_seen": 676864,
      "step": 197
    },
    {
      "epoch": 0.6491803278688525,
      "grad_norm": 5.348104476928711,
      "learning_rate": 4.855143631968242e-05,
      "loss": 0.6436,
      "num_input_tokens_seen": 679936,
      "step": 198
    },
    {
      "epoch": 0.6524590163934426,
      "grad_norm": 6.061436653137207,
      "learning_rate": 4.852201922385564e-05,
      "loss": 0.6535,
      "num_input_tokens_seen": 683520,
      "step": 199
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 5.132827281951904,
      "learning_rate": 4.849231551964771e-05,
      "loss": 0.558,
      "num_input_tokens_seen": 686592,
      "step": 200
    },
    {
      "epoch": 0.659016393442623,
      "grad_norm": 5.495711326599121,
      "learning_rate": 4.84623255689889e-05,
      "loss": 0.5512,
      "num_input_tokens_seen": 690176,
      "step": 201
    },
    {
      "epoch": 0.6622950819672131,
      "grad_norm": 5.177145004272461,
      "learning_rate": 4.843204973729729e-05,
      "loss": 0.454,
      "num_input_tokens_seen": 694272,
      "step": 202
    },
    {
      "epoch": 0.6655737704918033,
      "grad_norm": 4.999046325683594,
      "learning_rate": 4.840148839347434e-05,
      "loss": 0.4512,
      "num_input_tokens_seen": 697344,
      "step": 203
    },
    {
      "epoch": 0.6688524590163935,
      "grad_norm": 5.580689430236816,
      "learning_rate": 4.837064190990036e-05,
      "loss": 0.7203,
      "num_input_tokens_seen": 701440,
      "step": 204
    },
    {
      "epoch": 0.6721311475409836,
      "grad_norm": 4.8932318687438965,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 0.5131,
      "num_input_tokens_seen": 705024,
      "step": 205
    },
    {
      "epoch": 0.6754098360655738,
      "grad_norm": 4.806511402130127,
      "learning_rate": 4.830809503038781e-05,
      "loss": 0.6366,
      "num_input_tokens_seen": 708608,
      "step": 206
    },
    {
      "epoch": 0.6786885245901639,
      "grad_norm": 3.8475899696350098,
      "learning_rate": 4.827639539656321e-05,
      "loss": 0.3892,
      "num_input_tokens_seen": 712192,
      "step": 207
    },
    {
      "epoch": 0.6819672131147541,
      "grad_norm": 4.624356269836426,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 0.4248,
      "num_input_tokens_seen": 715776,
      "step": 208
    },
    {
      "epoch": 0.6852459016393443,
      "grad_norm": 4.879307746887207,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 0.5262,
      "num_input_tokens_seen": 719360,
      "step": 209
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 5.756197452545166,
      "learning_rate": 4.817959636416969e-05,
      "loss": 0.6681,
      "num_input_tokens_seen": 722944,
      "step": 210
    },
    {
      "epoch": 0.6918032786885245,
      "grad_norm": 4.946873188018799,
      "learning_rate": 4.814676462024988e-05,
      "loss": 0.4915,
      "num_input_tokens_seen": 726528,
      "step": 211
    },
    {
      "epoch": 0.6950819672131148,
      "grad_norm": 4.200891971588135,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 0.3812,
      "num_input_tokens_seen": 730112,
      "step": 212
    },
    {
      "epoch": 0.6983606557377049,
      "grad_norm": 5.223448276519775,
      "learning_rate": 4.808025542782453e-05,
      "loss": 0.4466,
      "num_input_tokens_seen": 733696,
      "step": 213
    },
    {
      "epoch": 0.7016393442622951,
      "grad_norm": 4.146871089935303,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 0.3519,
      "num_input_tokens_seen": 736768,
      "step": 214
    },
    {
      "epoch": 0.7049180327868853,
      "grad_norm": 5.559998512268066,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 0.5983,
      "num_input_tokens_seen": 740352,
      "step": 215
    },
    {
      "epoch": 0.7081967213114754,
      "grad_norm": 4.6619768142700195,
      "learning_rate": 4.797838348138086e-05,
      "loss": 0.2878,
      "num_input_tokens_seen": 743424,
      "step": 216
    },
    {
      "epoch": 0.7114754098360656,
      "grad_norm": 5.588301181793213,
      "learning_rate": 4.794386564209953e-05,
      "loss": 0.5062,
      "num_input_tokens_seen": 747008,
      "step": 217
    },
    {
      "epoch": 0.7147540983606557,
      "grad_norm": 7.368695259094238,
      "learning_rate": 4.790906823905599e-05,
      "loss": 0.4522,
      "num_input_tokens_seen": 750080,
      "step": 218
    },
    {
      "epoch": 0.7180327868852459,
      "grad_norm": 6.604324817657471,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 0.645,
      "num_input_tokens_seen": 753664,
      "step": 219
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 7.532996654510498,
      "learning_rate": 4.783863644106502e-05,
      "loss": 0.6299,
      "num_input_tokens_seen": 756736,
      "step": 220
    },
    {
      "epoch": 0.7245901639344262,
      "grad_norm": 6.795130252838135,
      "learning_rate": 4.780300290430682e-05,
      "loss": 0.5843,
      "num_input_tokens_seen": 760320,
      "step": 221
    },
    {
      "epoch": 0.7278688524590164,
      "grad_norm": 7.990849018096924,
      "learning_rate": 4.776709152015443e-05,
      "loss": 0.7542,
      "num_input_tokens_seen": 764416,
      "step": 222
    },
    {
      "epoch": 0.7311475409836066,
      "grad_norm": 7.819694519042969,
      "learning_rate": 4.773090272617672e-05,
      "loss": 0.6076,
      "num_input_tokens_seen": 768000,
      "step": 223
    },
    {
      "epoch": 0.7344262295081967,
      "grad_norm": 7.799463272094727,
      "learning_rate": 4.769443696332272e-05,
      "loss": 0.5438,
      "num_input_tokens_seen": 771072,
      "step": 224
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 5.238918781280518,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.3288,
      "num_input_tokens_seen": 774144,
      "step": 225
    },
    {
      "epoch": 0.740983606557377,
      "grad_norm": 8.815590858459473,
      "learning_rate": 4.762067631165049e-05,
      "loss": 0.5717,
      "num_input_tokens_seen": 777728,
      "step": 226
    },
    {
      "epoch": 0.7442622950819672,
      "grad_norm": 6.217066764831543,
      "learning_rate": 4.758338232158252e-05,
      "loss": 0.475,
      "num_input_tokens_seen": 780800,
      "step": 227
    },
    {
      "epoch": 0.7475409836065574,
      "grad_norm": 6.8338141441345215,
      "learning_rate": 4.754581316012785e-05,
      "loss": 0.4771,
      "num_input_tokens_seen": 784384,
      "step": 228
    },
    {
      "epoch": 0.7508196721311475,
      "grad_norm": 4.9941816329956055,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 0.3098,
      "num_input_tokens_seen": 788480,
      "step": 229
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 5.878753662109375,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 0.4611,
      "num_input_tokens_seen": 792064,
      "step": 230
    },
    {
      "epoch": 0.7573770491803279,
      "grad_norm": 5.49456262588501,
      "learning_rate": 4.743145924185821e-05,
      "loss": 0.3792,
      "num_input_tokens_seen": 795648,
      "step": 231
    },
    {
      "epoch": 0.760655737704918,
      "grad_norm": 7.6244940757751465,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 0.6354,
      "num_input_tokens_seen": 799232,
      "step": 232
    },
    {
      "epoch": 0.7639344262295082,
      "grad_norm": 8.307188987731934,
      "learning_rate": 4.73538559209842e-05,
      "loss": 0.7713,
      "num_input_tokens_seen": 802816,
      "step": 233
    },
    {
      "epoch": 0.7672131147540984,
      "grad_norm": 6.793440818786621,
      "learning_rate": 4.731464546130314e-05,
      "loss": 0.4405,
      "num_input_tokens_seen": 805888,
      "step": 234
    },
    {
      "epoch": 0.7704918032786885,
      "grad_norm": 6.803724765777588,
      "learning_rate": 4.72751631047092e-05,
      "loss": 0.6449,
      "num_input_tokens_seen": 809472,
      "step": 235
    },
    {
      "epoch": 0.7737704918032787,
      "grad_norm": 5.57187557220459,
      "learning_rate": 4.723540933228244e-05,
      "loss": 0.3577,
      "num_input_tokens_seen": 812544,
      "step": 236
    },
    {
      "epoch": 0.7770491803278688,
      "grad_norm": 6.255548000335693,
      "learning_rate": 4.719538462841003e-05,
      "loss": 0.5719,
      "num_input_tokens_seen": 816128,
      "step": 237
    },
    {
      "epoch": 0.780327868852459,
      "grad_norm": 5.007678985595703,
      "learning_rate": 4.715508948078037e-05,
      "loss": 0.3419,
      "num_input_tokens_seen": 819200,
      "step": 238
    },
    {
      "epoch": 0.7836065573770492,
      "grad_norm": 4.253977298736572,
      "learning_rate": 4.71145243803771e-05,
      "loss": 0.3813,
      "num_input_tokens_seen": 823296,
      "step": 239
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 6.7791829109191895,
      "learning_rate": 4.707368982147318e-05,
      "loss": 0.528,
      "num_input_tokens_seen": 826880,
      "step": 240
    },
    {
      "epoch": 0.7901639344262295,
      "grad_norm": 6.872074604034424,
      "learning_rate": 4.70325863016248e-05,
      "loss": 0.4893,
      "num_input_tokens_seen": 830464,
      "step": 241
    },
    {
      "epoch": 0.7934426229508197,
      "grad_norm": 6.479966163635254,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 0.5117,
      "num_input_tokens_seen": 834048,
      "step": 242
    },
    {
      "epoch": 0.7967213114754098,
      "grad_norm": 6.3179426193237305,
      "learning_rate": 4.694957438569951e-05,
      "loss": 0.5663,
      "num_input_tokens_seen": 837120,
      "step": 243
    },
    {
      "epoch": 0.8,
      "grad_norm": 7.068606376647949,
      "learning_rate": 4.690766700109659e-05,
      "loss": 0.4201,
      "num_input_tokens_seen": 840704,
      "step": 244
    },
    {
      "epoch": 0.8032786885245902,
      "grad_norm": 9.75481128692627,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 0.4548,
      "num_input_tokens_seen": 843776,
      "step": 245
    },
    {
      "epoch": 0.8065573770491803,
      "grad_norm": 5.3998212814331055,
      "learning_rate": 4.682305193174524e-05,
      "loss": 0.4733,
      "num_input_tokens_seen": 846848,
      "step": 246
    },
    {
      "epoch": 0.8098360655737705,
      "grad_norm": 8.692030906677246,
      "learning_rate": 4.678034527800474e-05,
      "loss": 0.7458,
      "num_input_tokens_seen": 850432,
      "step": 247
    },
    {
      "epoch": 0.8131147540983606,
      "grad_norm": 3.927462577819824,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 0.3157,
      "num_input_tokens_seen": 853504,
      "step": 248
    },
    {
      "epoch": 0.8163934426229508,
      "grad_norm": 7.093422889709473,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.5704,
      "num_input_tokens_seen": 857088,
      "step": 249
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 4.936685085296631,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.4624,
      "num_input_tokens_seen": 861184,
      "step": 250
    },
    {
      "epoch": 0.8229508196721311,
      "grad_norm": 4.682296276092529,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 0.4417,
      "num_input_tokens_seen": 864768,
      "step": 251
    },
    {
      "epoch": 0.8262295081967214,
      "grad_norm": 5.5670166015625,
      "learning_rate": 4.656284173018144e-05,
      "loss": 0.5241,
      "num_input_tokens_seen": 867840,
      "step": 252
    },
    {
      "epoch": 0.8295081967213115,
      "grad_norm": 8.014142036437988,
      "learning_rate": 4.65185506750986e-05,
      "loss": 0.6827,
      "num_input_tokens_seen": 870912,
      "step": 253
    },
    {
      "epoch": 0.8327868852459016,
      "grad_norm": 5.706748008728027,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 0.5627,
      "num_input_tokens_seen": 874496,
      "step": 254
    },
    {
      "epoch": 0.8360655737704918,
      "grad_norm": 7.072136402130127,
      "learning_rate": 4.642918251755281e-05,
      "loss": 0.5191,
      "num_input_tokens_seen": 878080,
      "step": 255
    },
    {
      "epoch": 0.839344262295082,
      "grad_norm": 6.102778434753418,
      "learning_rate": 4.638410650401267e-05,
      "loss": 0.6271,
      "num_input_tokens_seen": 881664,
      "step": 256
    },
    {
      "epoch": 0.8426229508196721,
      "grad_norm": 5.721380710601807,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 0.471,
      "num_input_tokens_seen": 885760,
      "step": 257
    },
    {
      "epoch": 0.8459016393442623,
      "grad_norm": 7.416042327880859,
      "learning_rate": 4.629317335357619e-05,
      "loss": 0.5457,
      "num_input_tokens_seen": 889344,
      "step": 258
    },
    {
      "epoch": 0.8491803278688524,
      "grad_norm": 3.954896926879883,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 0.2999,
      "num_input_tokens_seen": 892928,
      "step": 259
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 5.62251091003418,
      "learning_rate": 4.620120240391065e-05,
      "loss": 0.3097,
      "num_input_tokens_seen": 896512,
      "step": 260
    },
    {
      "epoch": 0.8557377049180328,
      "grad_norm": 7.64418363571167,
      "learning_rate": 4.615482915318911e-05,
      "loss": 0.5677,
      "num_input_tokens_seen": 900096,
      "step": 261
    },
    {
      "epoch": 0.8590163934426229,
      "grad_norm": 6.482415199279785,
      "learning_rate": 4.610819813755038e-05,
      "loss": 0.5897,
      "num_input_tokens_seen": 903680,
      "step": 262
    },
    {
      "epoch": 0.8622950819672132,
      "grad_norm": 7.398310661315918,
      "learning_rate": 4.606130992517869e-05,
      "loss": 0.6378,
      "num_input_tokens_seen": 906752,
      "step": 263
    },
    {
      "epoch": 0.8655737704918033,
      "grad_norm": 4.1240153312683105,
      "learning_rate": 4.601416508739211e-05,
      "loss": 0.281,
      "num_input_tokens_seen": 909824,
      "step": 264
    },
    {
      "epoch": 0.8688524590163934,
      "grad_norm": 4.301139831542969,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 0.3787,
      "num_input_tokens_seen": 913408,
      "step": 265
    },
    {
      "epoch": 0.8721311475409836,
      "grad_norm": 7.1203293800354,
      "learning_rate": 4.591910783647404e-05,
      "loss": 0.6874,
      "num_input_tokens_seen": 916992,
      "step": 266
    },
    {
      "epoch": 0.8754098360655738,
      "grad_norm": 6.1887311935424805,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 0.5389,
      "num_input_tokens_seen": 921088,
      "step": 267
    },
    {
      "epoch": 0.8786885245901639,
      "grad_norm": 7.213088035583496,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 0.6687,
      "num_input_tokens_seen": 924672,
      "step": 268
    },
    {
      "epoch": 0.8819672131147541,
      "grad_norm": 8.232933044433594,
      "learning_rate": 4.577461173185821e-05,
      "loss": 0.5264,
      "num_input_tokens_seen": 927744,
      "step": 269
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 8.329693794250488,
      "learning_rate": 4.572593931387604e-05,
      "loss": 0.6337,
      "num_input_tokens_seen": 931328,
      "step": 270
    },
    {
      "epoch": 0.8885245901639345,
      "grad_norm": 8.98690414428711,
      "learning_rate": 4.567701435686404e-05,
      "loss": 0.7031,
      "num_input_tokens_seen": 934912,
      "step": 271
    },
    {
      "epoch": 0.8918032786885246,
      "grad_norm": 6.584659099578857,
      "learning_rate": 4.562783745695738e-05,
      "loss": 0.3481,
      "num_input_tokens_seen": 937984,
      "step": 272
    },
    {
      "epoch": 0.8950819672131147,
      "grad_norm": 8.212920188903809,
      "learning_rate": 4.557840921336105e-05,
      "loss": 0.7593,
      "num_input_tokens_seen": 941568,
      "step": 273
    },
    {
      "epoch": 0.898360655737705,
      "grad_norm": 7.338629722595215,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 0.6929,
      "num_input_tokens_seen": 945152,
      "step": 274
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 8.23013973236084,
      "learning_rate": 4.54788011072248e-05,
      "loss": 0.4733,
      "num_input_tokens_seen": 948736,
      "step": 275
    },
    {
      "epoch": 0.9049180327868852,
      "grad_norm": 7.65561580657959,
      "learning_rate": 4.542862245837821e-05,
      "loss": 0.4978,
      "num_input_tokens_seen": 951808,
      "step": 276
    },
    {
      "epoch": 0.9081967213114754,
      "grad_norm": 6.682304859161377,
      "learning_rate": 4.537819489321386e-05,
      "loss": 0.4354,
      "num_input_tokens_seen": 954880,
      "step": 277
    },
    {
      "epoch": 0.9114754098360656,
      "grad_norm": 6.918341159820557,
      "learning_rate": 4.532751902617569e-05,
      "loss": 0.6015,
      "num_input_tokens_seen": 957952,
      "step": 278
    },
    {
      "epoch": 0.9147540983606557,
      "grad_norm": 9.259469985961914,
      "learning_rate": 4.527659547473317e-05,
      "loss": 0.8884,
      "num_input_tokens_seen": 961536,
      "step": 279
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 5.731265068054199,
      "learning_rate": 4.522542485937369e-05,
      "loss": 0.5067,
      "num_input_tokens_seen": 965120,
      "step": 280
    },
    {
      "epoch": 0.921311475409836,
      "grad_norm": 5.66297721862793,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 0.4905,
      "num_input_tokens_seen": 968192,
      "step": 281
    },
    {
      "epoch": 0.9245901639344263,
      "grad_norm": 4.855496406555176,
      "learning_rate": 4.512234493389785e-05,
      "loss": 0.6234,
      "num_input_tokens_seen": 971264,
      "step": 282
    },
    {
      "epoch": 0.9278688524590164,
      "grad_norm": 7.211270332336426,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 0.5639,
      "num_input_tokens_seen": 974336,
      "step": 283
    },
    {
      "epoch": 0.9311475409836065,
      "grad_norm": 4.295474529266357,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 0.3409,
      "num_input_tokens_seen": 977920,
      "step": 284
    },
    {
      "epoch": 0.9344262295081968,
      "grad_norm": 5.4483489990234375,
      "learning_rate": 4.496588775118232e-05,
      "loss": 0.379,
      "num_input_tokens_seen": 980992,
      "step": 285
    },
    {
      "epoch": 0.9377049180327869,
      "grad_norm": 4.338398456573486,
      "learning_rate": 4.491324795060491e-05,
      "loss": 0.4363,
      "num_input_tokens_seen": 985088,
      "step": 286
    },
    {
      "epoch": 0.940983606557377,
      "grad_norm": 3.7922959327697754,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.272,
      "num_input_tokens_seen": 988160,
      "step": 287
    },
    {
      "epoch": 0.9442622950819672,
      "grad_norm": 5.901621341705322,
      "learning_rate": 4.480724108387977e-05,
      "loss": 0.4576,
      "num_input_tokens_seen": 991232,
      "step": 288
    },
    {
      "epoch": 0.9475409836065574,
      "grad_norm": 7.9380669593811035,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 0.8087,
      "num_input_tokens_seen": 994816,
      "step": 289
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 6.885442733764648,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 0.542,
      "num_input_tokens_seen": 999424,
      "step": 290
    },
    {
      "epoch": 0.9540983606557377,
      "grad_norm": 8.581880569458008,
      "learning_rate": 4.464642232938505e-05,
      "loss": 0.6377,
      "num_input_tokens_seen": 1003008,
      "step": 291
    },
    {
      "epoch": 0.9573770491803278,
      "grad_norm": 7.35661506652832,
      "learning_rate": 4.4592336433146e-05,
      "loss": 0.708,
      "num_input_tokens_seen": 1006080,
      "step": 292
    },
    {
      "epoch": 0.9606557377049181,
      "grad_norm": 5.5333404541015625,
      "learning_rate": 4.453801181047047e-05,
      "loss": 0.5006,
      "num_input_tokens_seen": 1009664,
      "step": 293
    },
    {
      "epoch": 0.9639344262295082,
      "grad_norm": 7.86466646194458,
      "learning_rate": 4.448344912328686e-05,
      "loss": 0.5419,
      "num_input_tokens_seen": 1013248,
      "step": 294
    },
    {
      "epoch": 0.9672131147540983,
      "grad_norm": 6.54731559753418,
      "learning_rate": 4.442864903642428e-05,
      "loss": 0.5478,
      "num_input_tokens_seen": 1016832,
      "step": 295
    },
    {
      "epoch": 0.9704918032786886,
      "grad_norm": 4.457104206085205,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 0.3502,
      "num_input_tokens_seen": 1020928,
      "step": 296
    },
    {
      "epoch": 0.9737704918032787,
      "grad_norm": 7.870576858520508,
      "learning_rate": 4.431833933743378e-05,
      "loss": 0.7688,
      "num_input_tokens_seen": 1024512,
      "step": 297
    },
    {
      "epoch": 0.9770491803278688,
      "grad_norm": 7.675661563873291,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.6616,
      "num_input_tokens_seen": 1028096,
      "step": 298
    },
    {
      "epoch": 0.980327868852459,
      "grad_norm": 6.363917350769043,
      "learning_rate": 4.420708808983809e-05,
      "loss": 0.6827,
      "num_input_tokens_seen": 1031680,
      "step": 299
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 6.054831027984619,
      "learning_rate": 4.415111107797445e-05,
      "loss": 0.5384,
      "num_input_tokens_seen": 1034752,
      "step": 300
    },
    {
      "epoch": 0.9868852459016394,
      "grad_norm": 5.005954265594482,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.3901,
      "num_input_tokens_seen": 1038336,
      "step": 301
    },
    {
      "epoch": 0.9901639344262295,
      "grad_norm": 5.645168304443359,
      "learning_rate": 4.403845768841842e-05,
      "loss": 0.3544,
      "num_input_tokens_seen": 1042432,
      "step": 302
    },
    {
      "epoch": 0.9934426229508196,
      "grad_norm": 6.114139080047607,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 0.4511,
      "num_input_tokens_seen": 1047040,
      "step": 303
    },
    {
      "epoch": 0.9967213114754099,
      "grad_norm": 6.750384330749512,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 0.6477,
      "num_input_tokens_seen": 1050624,
      "step": 304
    },
    {
      "epoch": 1.0,
      "grad_norm": 12.524174690246582,
      "learning_rate": 4.386773950556931e-05,
      "loss": 0.9617,
      "num_input_tokens_seen": 1052080,
      "step": 305
    },
    {
      "epoch": 1.0032786885245901,
      "grad_norm": 6.744550704956055,
      "learning_rate": 4.381037272239311e-05,
      "loss": 0.526,
      "num_input_tokens_seen": 1055664,
      "step": 306
    },
    {
      "epoch": 1.0065573770491802,
      "grad_norm": 6.002376556396484,
      "learning_rate": 4.375277674076149e-05,
      "loss": 0.5794,
      "num_input_tokens_seen": 1058736,
      "step": 307
    },
    {
      "epoch": 1.0098360655737706,
      "grad_norm": 6.490222454071045,
      "learning_rate": 4.36949522624633e-05,
      "loss": 0.375,
      "num_input_tokens_seen": 1061808,
      "step": 308
    },
    {
      "epoch": 1.0131147540983607,
      "grad_norm": 7.731186389923096,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.72,
      "num_input_tokens_seen": 1065904,
      "step": 309
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 5.763629913330078,
      "learning_rate": 4.357862063693486e-05,
      "loss": 0.5214,
      "num_input_tokens_seen": 1068976,
      "step": 310
    },
    {
      "epoch": 1.019672131147541,
      "grad_norm": 7.358626365661621,
      "learning_rate": 4.352011490716875e-05,
      "loss": 0.5298,
      "num_input_tokens_seen": 1072560,
      "step": 311
    },
    {
      "epoch": 1.022950819672131,
      "grad_norm": 5.1954874992370605,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 0.3416,
      "num_input_tokens_seen": 1076144,
      "step": 312
    },
    {
      "epoch": 1.0262295081967212,
      "grad_norm": 8.140822410583496,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 0.5922,
      "num_input_tokens_seen": 1079728,
      "step": 313
    },
    {
      "epoch": 1.0295081967213116,
      "grad_norm": 6.57662296295166,
      "learning_rate": 4.334324661257191e-05,
      "loss": 0.4328,
      "num_input_tokens_seen": 1082800,
      "step": 314
    },
    {
      "epoch": 1.0327868852459017,
      "grad_norm": 7.5104570388793945,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 0.543,
      "num_input_tokens_seen": 1085872,
      "step": 315
    },
    {
      "epoch": 1.0360655737704918,
      "grad_norm": 6.426177024841309,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 0.4532,
      "num_input_tokens_seen": 1089456,
      "step": 316
    },
    {
      "epoch": 1.039344262295082,
      "grad_norm": 5.890456676483154,
      "learning_rate": 4.31643667742744e-05,
      "loss": 0.5805,
      "num_input_tokens_seen": 1093040,
      "step": 317
    },
    {
      "epoch": 1.042622950819672,
      "grad_norm": 6.165313243865967,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.5388,
      "num_input_tokens_seen": 1096112,
      "step": 318
    },
    {
      "epoch": 1.0459016393442624,
      "grad_norm": 7.596555709838867,
      "learning_rate": 4.304400570245906e-05,
      "loss": 0.4371,
      "num_input_tokens_seen": 1099184,
      "step": 319
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 5.359528064727783,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 0.4697,
      "num_input_tokens_seen": 1103280,
      "step": 320
    },
    {
      "epoch": 1.0524590163934426,
      "grad_norm": 6.971686363220215,
      "learning_rate": 4.292276519126207e-05,
      "loss": 0.5339,
      "num_input_tokens_seen": 1106864,
      "step": 321
    },
    {
      "epoch": 1.0557377049180328,
      "grad_norm": 6.700169563293457,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 0.6002,
      "num_input_tokens_seen": 1110448,
      "step": 322
    },
    {
      "epoch": 1.0590163934426229,
      "grad_norm": 5.216481685638428,
      "learning_rate": 4.280065114977492e-05,
      "loss": 0.3473,
      "num_input_tokens_seen": 1114032,
      "step": 323
    },
    {
      "epoch": 1.0622950819672132,
      "grad_norm": 3.64694881439209,
      "learning_rate": 4.273926841341302e-05,
      "loss": 0.2055,
      "num_input_tokens_seen": 1117104,
      "step": 324
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 5.90377140045166,
      "learning_rate": 4.267766952966369e-05,
      "loss": 0.3572,
      "num_input_tokens_seen": 1120688,
      "step": 325
    },
    {
      "epoch": 1.0688524590163935,
      "grad_norm": 6.163837432861328,
      "learning_rate": 4.261585524908987e-05,
      "loss": 0.4549,
      "num_input_tokens_seen": 1124272,
      "step": 326
    },
    {
      "epoch": 1.0721311475409836,
      "grad_norm": 8.961606979370117,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 0.3483,
      "num_input_tokens_seen": 1127856,
      "step": 327
    },
    {
      "epoch": 1.0754098360655737,
      "grad_norm": 10.547452926635742,
      "learning_rate": 4.249158351283414e-05,
      "loss": 0.771,
      "num_input_tokens_seen": 1131440,
      "step": 328
    },
    {
      "epoch": 1.0786885245901638,
      "grad_norm": 7.785688877105713,
      "learning_rate": 4.242912757136412e-05,
      "loss": 0.6921,
      "num_input_tokens_seen": 1135024,
      "step": 329
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 5.866882801055908,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 0.3799,
      "num_input_tokens_seen": 1138608,
      "step": 330
    },
    {
      "epoch": 1.0852459016393443,
      "grad_norm": 8.320405960083008,
      "learning_rate": 4.230357934676017e-05,
      "loss": 0.5962,
      "num_input_tokens_seen": 1142192,
      "step": 331
    },
    {
      "epoch": 1.0885245901639344,
      "grad_norm": 6.17679500579834,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.3529,
      "num_input_tokens_seen": 1145776,
      "step": 332
    },
    {
      "epoch": 1.0918032786885246,
      "grad_norm": 6.845378875732422,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.4047,
      "num_input_tokens_seen": 1149360,
      "step": 333
    },
    {
      "epoch": 1.0950819672131147,
      "grad_norm": 6.516812324523926,
      "learning_rate": 4.211367764821722e-05,
      "loss": 0.4699,
      "num_input_tokens_seen": 1153456,
      "step": 334
    },
    {
      "epoch": 1.098360655737705,
      "grad_norm": 4.418260097503662,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 0.2222,
      "num_input_tokens_seen": 1156528,
      "step": 335
    },
    {
      "epoch": 1.1016393442622952,
      "grad_norm": 8.901095390319824,
      "learning_rate": 4.198603260653792e-05,
      "loss": 0.5485,
      "num_input_tokens_seen": 1159600,
      "step": 336
    },
    {
      "epoch": 1.1049180327868853,
      "grad_norm": 6.0295820236206055,
      "learning_rate": 4.192189924206652e-05,
      "loss": 0.2744,
      "num_input_tokens_seen": 1162672,
      "step": 337
    },
    {
      "epoch": 1.1081967213114754,
      "grad_norm": 5.741112232208252,
      "learning_rate": 4.185755968959308e-05,
      "loss": 0.4154,
      "num_input_tokens_seen": 1165744,
      "step": 338
    },
    {
      "epoch": 1.1114754098360655,
      "grad_norm": 7.036557674407959,
      "learning_rate": 4.179301473307476e-05,
      "loss": 0.2905,
      "num_input_tokens_seen": 1169328,
      "step": 339
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 6.45889949798584,
      "learning_rate": 4.172826515897146e-05,
      "loss": 0.2673,
      "num_input_tokens_seen": 1173424,
      "step": 340
    },
    {
      "epoch": 1.118032786885246,
      "grad_norm": 7.091146469116211,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.4764,
      "num_input_tokens_seen": 1176496,
      "step": 341
    },
    {
      "epoch": 1.1213114754098361,
      "grad_norm": 6.57634162902832,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 0.3868,
      "num_input_tokens_seen": 1180080,
      "step": 342
    },
    {
      "epoch": 1.1245901639344262,
      "grad_norm": 8.394218444824219,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 0.5747,
      "num_input_tokens_seen": 1183152,
      "step": 343
    },
    {
      "epoch": 1.1278688524590164,
      "grad_norm": 7.7674031257629395,
      "learning_rate": 4.146723650296701e-05,
      "loss": 0.258,
      "num_input_tokens_seen": 1186736,
      "step": 344
    },
    {
      "epoch": 1.1311475409836065,
      "grad_norm": 9.776511192321777,
      "learning_rate": 4.140147572476268e-05,
      "loss": 0.5043,
      "num_input_tokens_seen": 1189808,
      "step": 345
    },
    {
      "epoch": 1.1344262295081968,
      "grad_norm": 12.136795997619629,
      "learning_rate": 4.133551509975264e-05,
      "loss": 0.5905,
      "num_input_tokens_seen": 1193904,
      "step": 346
    },
    {
      "epoch": 1.137704918032787,
      "grad_norm": 11.428803443908691,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 0.4991,
      "num_input_tokens_seen": 1197488,
      "step": 347
    },
    {
      "epoch": 1.140983606557377,
      "grad_norm": 13.36294937133789,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 0.7904,
      "num_input_tokens_seen": 1200560,
      "step": 348
    },
    {
      "epoch": 1.1442622950819672,
      "grad_norm": 7.959591388702393,
      "learning_rate": 4.113644219309877e-05,
      "loss": 0.5227,
      "num_input_tokens_seen": 1203632,
      "step": 349
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 12.930249214172363,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 1.0644,
      "num_input_tokens_seen": 1207216,
      "step": 350
    },
    {
      "epoch": 1.1508196721311474,
      "grad_norm": 6.257218837738037,
      "learning_rate": 4.100274248712389e-05,
      "loss": 0.2851,
      "num_input_tokens_seen": 1210800,
      "step": 351
    },
    {
      "epoch": 1.1540983606557378,
      "grad_norm": 7.663172245025635,
      "learning_rate": 4.093559974371725e-05,
      "loss": 0.5288,
      "num_input_tokens_seen": 1213872,
      "step": 352
    },
    {
      "epoch": 1.157377049180328,
      "grad_norm": 9.780983924865723,
      "learning_rate": 4.086826283005669e-05,
      "loss": 0.6556,
      "num_input_tokens_seen": 1216944,
      "step": 353
    },
    {
      "epoch": 1.160655737704918,
      "grad_norm": 8.317816734313965,
      "learning_rate": 4.080073256662127e-05,
      "loss": 0.5514,
      "num_input_tokens_seen": 1220528,
      "step": 354
    },
    {
      "epoch": 1.1639344262295082,
      "grad_norm": 5.918487071990967,
      "learning_rate": 4.073300977624594e-05,
      "loss": 0.4886,
      "num_input_tokens_seen": 1223600,
      "step": 355
    },
    {
      "epoch": 1.1672131147540983,
      "grad_norm": 7.318450450897217,
      "learning_rate": 4.066509528411152e-05,
      "loss": 0.519,
      "num_input_tokens_seen": 1227184,
      "step": 356
    },
    {
      "epoch": 1.1704918032786886,
      "grad_norm": 5.45626163482666,
      "learning_rate": 4.059698991773466e-05,
      "loss": 0.35,
      "num_input_tokens_seen": 1230768,
      "step": 357
    },
    {
      "epoch": 1.1737704918032787,
      "grad_norm": 5.587316036224365,
      "learning_rate": 4.052869450695776e-05,
      "loss": 0.3327,
      "num_input_tokens_seen": 1233840,
      "step": 358
    },
    {
      "epoch": 1.1770491803278689,
      "grad_norm": 6.602006435394287,
      "learning_rate": 4.046020988393885e-05,
      "loss": 0.4146,
      "num_input_tokens_seen": 1236912,
      "step": 359
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 7.37346887588501,
      "learning_rate": 4.039153688314145e-05,
      "loss": 0.3569,
      "num_input_tokens_seen": 1239984,
      "step": 360
    },
    {
      "epoch": 1.1836065573770491,
      "grad_norm": 5.545708179473877,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 0.2757,
      "num_input_tokens_seen": 1243568,
      "step": 361
    },
    {
      "epoch": 1.1868852459016392,
      "grad_norm": 7.264955520629883,
      "learning_rate": 4.02536290975317e-05,
      "loss": 0.5975,
      "num_input_tokens_seen": 1246640,
      "step": 362
    },
    {
      "epoch": 1.1901639344262296,
      "grad_norm": 5.624760627746582,
      "learning_rate": 4.018439599308217e-05,
      "loss": 0.2505,
      "num_input_tokens_seen": 1250224,
      "step": 363
    },
    {
      "epoch": 1.1934426229508197,
      "grad_norm": 8.251317024230957,
      "learning_rate": 4.011497787155938e-05,
      "loss": 0.5344,
      "num_input_tokens_seen": 1254320,
      "step": 364
    },
    {
      "epoch": 1.1967213114754098,
      "grad_norm": 8.2302885055542,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 0.5952,
      "num_input_tokens_seen": 1257904,
      "step": 365
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.857067108154297,
      "learning_rate": 3.997558996288965e-05,
      "loss": 0.62,
      "num_input_tokens_seen": 1261488,
      "step": 366
    },
    {
      "epoch": 1.20327868852459,
      "grad_norm": 7.086766719818115,
      "learning_rate": 3.99056218741404e-05,
      "loss": 0.3795,
      "num_input_tokens_seen": 1265072,
      "step": 367
    },
    {
      "epoch": 1.2065573770491804,
      "grad_norm": 8.546010971069336,
      "learning_rate": 3.983547216509254e-05,
      "loss": 0.7076,
      "num_input_tokens_seen": 1268144,
      "step": 368
    },
    {
      "epoch": 1.2098360655737705,
      "grad_norm": 8.468064308166504,
      "learning_rate": 3.976514169049814e-05,
      "loss": 0.4663,
      "num_input_tokens_seen": 1271216,
      "step": 369
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 7.518307685852051,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.586,
      "num_input_tokens_seen": 1274800,
      "step": 370
    },
    {
      "epoch": 1.2163934426229508,
      "grad_norm": 9.7147216796875,
      "learning_rate": 3.962394187468039e-05,
      "loss": 0.5993,
      "num_input_tokens_seen": 1278384,
      "step": 371
    },
    {
      "epoch": 1.219672131147541,
      "grad_norm": 7.033090114593506,
      "learning_rate": 3.955307425393224e-05,
      "loss": 0.43,
      "num_input_tokens_seen": 1281456,
      "step": 372
    },
    {
      "epoch": 1.222950819672131,
      "grad_norm": 5.73870325088501,
      "learning_rate": 3.948202930856697e-05,
      "loss": 0.3026,
      "num_input_tokens_seen": 1285040,
      "step": 373
    },
    {
      "epoch": 1.2262295081967214,
      "grad_norm": 8.868624687194824,
      "learning_rate": 3.941080790424484e-05,
      "loss": 0.6314,
      "num_input_tokens_seen": 1288624,
      "step": 374
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 10.57719898223877,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.663,
      "num_input_tokens_seen": 1292208,
      "step": 375
    },
    {
      "epoch": 1.2327868852459016,
      "grad_norm": 7.3481645584106445,
      "learning_rate": 3.92678391921108e-05,
      "loss": 0.3923,
      "num_input_tokens_seen": 1295792,
      "step": 376
    },
    {
      "epoch": 1.2360655737704918,
      "grad_norm": 6.577630519866943,
      "learning_rate": 3.919609362632753e-05,
      "loss": 0.3406,
      "num_input_tokens_seen": 1299376,
      "step": 377
    },
    {
      "epoch": 1.2393442622950819,
      "grad_norm": 5.16686487197876,
      "learning_rate": 3.912417508562345e-05,
      "loss": 0.1977,
      "num_input_tokens_seen": 1302448,
      "step": 378
    },
    {
      "epoch": 1.2426229508196722,
      "grad_norm": 7.3431901931762695,
      "learning_rate": 3.905208444630327e-05,
      "loss": 0.4495,
      "num_input_tokens_seen": 1306032,
      "step": 379
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 6.669623374938965,
      "learning_rate": 3.897982258676867e-05,
      "loss": 0.4952,
      "num_input_tokens_seen": 1309104,
      "step": 380
    },
    {
      "epoch": 1.2491803278688525,
      "grad_norm": 7.041914939880371,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 0.5536,
      "num_input_tokens_seen": 1312176,
      "step": 381
    },
    {
      "epoch": 1.2524590163934426,
      "grad_norm": 8.445089340209961,
      "learning_rate": 3.883478873108361e-05,
      "loss": 0.5479,
      "num_input_tokens_seen": 1315760,
      "step": 382
    },
    {
      "epoch": 1.2557377049180327,
      "grad_norm": 5.9584784507751465,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 0.3068,
      "num_input_tokens_seen": 1319344,
      "step": 383
    },
    {
      "epoch": 1.2590163934426228,
      "grad_norm": 6.019734859466553,
      "learning_rate": 3.868908058731376e-05,
      "loss": 0.3981,
      "num_input_tokens_seen": 1322928,
      "step": 384
    },
    {
      "epoch": 1.2622950819672132,
      "grad_norm": 10.519938468933105,
      "learning_rate": 3.861597587537568e-05,
      "loss": 0.7833,
      "num_input_tokens_seen": 1326000,
      "step": 385
    },
    {
      "epoch": 1.2655737704918033,
      "grad_norm": 10.833054542541504,
      "learning_rate": 3.85427052570685e-05,
      "loss": 0.6628,
      "num_input_tokens_seen": 1329584,
      "step": 386
    },
    {
      "epoch": 1.2688524590163934,
      "grad_norm": 6.552168846130371,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 0.3598,
      "num_input_tokens_seen": 1333168,
      "step": 387
    },
    {
      "epoch": 1.2721311475409836,
      "grad_norm": 11.577607154846191,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 0.6614,
      "num_input_tokens_seen": 1336240,
      "step": 388
    },
    {
      "epoch": 1.275409836065574,
      "grad_norm": 13.04915714263916,
      "learning_rate": 3.832190690176825e-05,
      "loss": 0.8151,
      "num_input_tokens_seen": 1340336,
      "step": 389
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 8.122928619384766,
      "learning_rate": 3.824798160583012e-05,
      "loss": 0.4552,
      "num_input_tokens_seen": 1343920,
      "step": 390
    },
    {
      "epoch": 1.2819672131147541,
      "grad_norm": 7.420164585113525,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 0.6387,
      "num_input_tokens_seen": 1347504,
      "step": 391
    },
    {
      "epoch": 1.2852459016393443,
      "grad_norm": 7.152034759521484,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 0.2913,
      "num_input_tokens_seen": 1350576,
      "step": 392
    },
    {
      "epoch": 1.2885245901639344,
      "grad_norm": 6.858789443969727,
      "learning_rate": 3.802524079601442e-05,
      "loss": 0.3679,
      "num_input_tokens_seen": 1354160,
      "step": 393
    },
    {
      "epoch": 1.2918032786885245,
      "grad_norm": 9.956066131591797,
      "learning_rate": 3.795067523432826e-05,
      "loss": 0.6955,
      "num_input_tokens_seen": 1357232,
      "step": 394
    },
    {
      "epoch": 1.2950819672131146,
      "grad_norm": 7.117641925811768,
      "learning_rate": 3.787595187275136e-05,
      "loss": 0.4077,
      "num_input_tokens_seen": 1360816,
      "step": 395
    },
    {
      "epoch": 1.298360655737705,
      "grad_norm": 4.983419418334961,
      "learning_rate": 3.780107162176429e-05,
      "loss": 0.2583,
      "num_input_tokens_seen": 1363888,
      "step": 396
    },
    {
      "epoch": 1.301639344262295,
      "grad_norm": 7.152803421020508,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 0.2279,
      "num_input_tokens_seen": 1367472,
      "step": 397
    },
    {
      "epoch": 1.3049180327868852,
      "grad_norm": 4.96547794342041,
      "learning_rate": 3.765084410302909e-05,
      "loss": 0.1771,
      "num_input_tokens_seen": 1371056,
      "step": 398
    },
    {
      "epoch": 1.3081967213114754,
      "grad_norm": 11.959607124328613,
      "learning_rate": 3.757549866575588e-05,
      "loss": 0.714,
      "num_input_tokens_seen": 1374640,
      "step": 399
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 9.234650611877441,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.5562,
      "num_input_tokens_seen": 1378224,
      "step": 400
    },
    {
      "epoch": 1.3147540983606558,
      "grad_norm": 8.570303916931152,
      "learning_rate": 3.742434902568889e-05,
      "loss": 0.5221,
      "num_input_tokens_seen": 1381296,
      "step": 401
    },
    {
      "epoch": 1.318032786885246,
      "grad_norm": 5.307389259338379,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 0.2622,
      "num_input_tokens_seen": 1384880,
      "step": 402
    },
    {
      "epoch": 1.321311475409836,
      "grad_norm": 7.065673351287842,
      "learning_rate": 3.727259384037852e-05,
      "loss": 0.4832,
      "num_input_tokens_seen": 1387952,
      "step": 403
    },
    {
      "epoch": 1.3245901639344262,
      "grad_norm": 6.939498424530029,
      "learning_rate": 3.719649147846832e-05,
      "loss": 0.4358,
      "num_input_tokens_seen": 1391536,
      "step": 404
    },
    {
      "epoch": 1.3278688524590163,
      "grad_norm": 12.44707202911377,
      "learning_rate": 3.712024050615843e-05,
      "loss": 0.6623,
      "num_input_tokens_seen": 1395632,
      "step": 405
    },
    {
      "epoch": 1.3311475409836064,
      "grad_norm": 15.471646308898926,
      "learning_rate": 3.704384185254288e-05,
      "loss": 0.669,
      "num_input_tokens_seen": 1399216,
      "step": 406
    },
    {
      "epoch": 1.3344262295081968,
      "grad_norm": 7.860199928283691,
      "learning_rate": 3.696729644851518e-05,
      "loss": 0.2874,
      "num_input_tokens_seen": 1402288,
      "step": 407
    },
    {
      "epoch": 1.337704918032787,
      "grad_norm": 8.810921669006348,
      "learning_rate": 3.689060522675689e-05,
      "loss": 0.393,
      "num_input_tokens_seen": 1406384,
      "step": 408
    },
    {
      "epoch": 1.340983606557377,
      "grad_norm": 10.878657341003418,
      "learning_rate": 3.681376912172636e-05,
      "loss": 0.6867,
      "num_input_tokens_seen": 1409968,
      "step": 409
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 9.144725799560547,
      "learning_rate": 3.673678906964727e-05,
      "loss": 0.5825,
      "num_input_tokens_seen": 1413552,
      "step": 410
    },
    {
      "epoch": 1.3475409836065575,
      "grad_norm": 7.571273326873779,
      "learning_rate": 3.665966600849728e-05,
      "loss": 0.4927,
      "num_input_tokens_seen": 1417136,
      "step": 411
    },
    {
      "epoch": 1.3508196721311476,
      "grad_norm": 7.283060550689697,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 0.4086,
      "num_input_tokens_seen": 1420208,
      "step": 412
    },
    {
      "epoch": 1.3540983606557377,
      "grad_norm": 8.300950050354004,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 0.5559,
      "num_input_tokens_seen": 1423792,
      "step": 413
    },
    {
      "epoch": 1.3573770491803279,
      "grad_norm": 10.868128776550293,
      "learning_rate": 3.642744817646736e-05,
      "loss": 0.3517,
      "num_input_tokens_seen": 1426864,
      "step": 414
    },
    {
      "epoch": 1.360655737704918,
      "grad_norm": 8.631510734558105,
      "learning_rate": 3.634976249348867e-05,
      "loss": 0.634,
      "num_input_tokens_seen": 1430448,
      "step": 415
    },
    {
      "epoch": 1.3639344262295081,
      "grad_norm": 7.767363548278809,
      "learning_rate": 3.627193851723577e-05,
      "loss": 0.4673,
      "num_input_tokens_seen": 1433520,
      "step": 416
    },
    {
      "epoch": 1.3672131147540982,
      "grad_norm": 11.58713436126709,
      "learning_rate": 3.619397719596924e-05,
      "loss": 0.4349,
      "num_input_tokens_seen": 1436592,
      "step": 417
    },
    {
      "epoch": 1.3704918032786886,
      "grad_norm": 11.135054588317871,
      "learning_rate": 3.611587947962319e-05,
      "loss": 0.7155,
      "num_input_tokens_seen": 1440176,
      "step": 418
    },
    {
      "epoch": 1.3737704918032787,
      "grad_norm": 8.022826194763184,
      "learning_rate": 3.603764631979363e-05,
      "loss": 0.5519,
      "num_input_tokens_seen": 1443760,
      "step": 419
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 8.608506202697754,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.5084,
      "num_input_tokens_seen": 1447344,
      "step": 420
    },
    {
      "epoch": 1.380327868852459,
      "grad_norm": 12.570616722106934,
      "learning_rate": 3.588077748430819e-05,
      "loss": 0.9258,
      "num_input_tokens_seen": 1450416,
      "step": 421
    },
    {
      "epoch": 1.3836065573770493,
      "grad_norm": 5.012407302856445,
      "learning_rate": 3.580214372004956e-05,
      "loss": 0.3053,
      "num_input_tokens_seen": 1454000,
      "step": 422
    },
    {
      "epoch": 1.3868852459016394,
      "grad_norm": 5.027945041656494,
      "learning_rate": 3.572337833507865e-05,
      "loss": 0.3655,
      "num_input_tokens_seen": 1457584,
      "step": 423
    },
    {
      "epoch": 1.3901639344262295,
      "grad_norm": 5.805233478546143,
      "learning_rate": 3.564448228912682e-05,
      "loss": 0.3086,
      "num_input_tokens_seen": 1461168,
      "step": 424
    },
    {
      "epoch": 1.3934426229508197,
      "grad_norm": 5.27237606048584,
      "learning_rate": 3.556545654351749e-05,
      "loss": 0.271,
      "num_input_tokens_seen": 1464240,
      "step": 425
    },
    {
      "epoch": 1.3967213114754098,
      "grad_norm": 6.467782020568848,
      "learning_rate": 3.548630206115443e-05,
      "loss": 0.3723,
      "num_input_tokens_seen": 1468336,
      "step": 426
    },
    {
      "epoch": 1.4,
      "grad_norm": 6.663521766662598,
      "learning_rate": 3.540701980651003e-05,
      "loss": 0.4501,
      "num_input_tokens_seen": 1471408,
      "step": 427
    },
    {
      "epoch": 1.40327868852459,
      "grad_norm": 7.83502721786499,
      "learning_rate": 3.532761074561355e-05,
      "loss": 0.5468,
      "num_input_tokens_seen": 1474992,
      "step": 428
    },
    {
      "epoch": 1.4065573770491804,
      "grad_norm": 10.746574401855469,
      "learning_rate": 3.524807584603932e-05,
      "loss": 0.5989,
      "num_input_tokens_seen": 1478576,
      "step": 429
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 7.179470539093018,
      "learning_rate": 3.516841607689501e-05,
      "loss": 0.4473,
      "num_input_tokens_seen": 1482160,
      "step": 430
    },
    {
      "epoch": 1.4131147540983606,
      "grad_norm": 5.0019049644470215,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 0.2482,
      "num_input_tokens_seen": 1486256,
      "step": 431
    },
    {
      "epoch": 1.4163934426229507,
      "grad_norm": 13.038597106933594,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.8024,
      "num_input_tokens_seen": 1489328,
      "step": 432
    },
    {
      "epoch": 1.419672131147541,
      "grad_norm": 7.853848934173584,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 0.6045,
      "num_input_tokens_seen": 1492912,
      "step": 433
    },
    {
      "epoch": 1.4229508196721312,
      "grad_norm": 8.042394638061523,
      "learning_rate": 3.484854773977378e-05,
      "loss": 0.5208,
      "num_input_tokens_seen": 1496496,
      "step": 434
    },
    {
      "epoch": 1.4262295081967213,
      "grad_norm": 8.84683609008789,
      "learning_rate": 3.476827821223184e-05,
      "loss": 0.6112,
      "num_input_tokens_seen": 1499568,
      "step": 435
    },
    {
      "epoch": 1.4295081967213115,
      "grad_norm": 6.673642635345459,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 0.3648,
      "num_input_tokens_seen": 1502640,
      "step": 436
    },
    {
      "epoch": 1.4327868852459016,
      "grad_norm": 6.926111221313477,
      "learning_rate": 3.460738306649509e-05,
      "loss": 0.4042,
      "num_input_tokens_seen": 1506224,
      "step": 437
    },
    {
      "epoch": 1.4360655737704917,
      "grad_norm": 8.971760749816895,
      "learning_rate": 3.452675940875686e-05,
      "loss": 0.6095,
      "num_input_tokens_seen": 1509296,
      "step": 438
    },
    {
      "epoch": 1.4393442622950818,
      "grad_norm": 8.977940559387207,
      "learning_rate": 3.444601967046168e-05,
      "loss": 0.6063,
      "num_input_tokens_seen": 1512368,
      "step": 439
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 5.465718746185303,
      "learning_rate": 3.436516483539781e-05,
      "loss": 0.3001,
      "num_input_tokens_seen": 1515440,
      "step": 440
    },
    {
      "epoch": 1.4459016393442623,
      "grad_norm": 5.788984298706055,
      "learning_rate": 3.428419588875588e-05,
      "loss": 0.3833,
      "num_input_tokens_seen": 1519024,
      "step": 441
    },
    {
      "epoch": 1.4491803278688524,
      "grad_norm": 10.344032287597656,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 0.8252,
      "num_input_tokens_seen": 1522096,
      "step": 442
    },
    {
      "epoch": 1.4524590163934425,
      "grad_norm": 7.264003753662109,
      "learning_rate": 3.412191960844049e-05,
      "loss": 0.5127,
      "num_input_tokens_seen": 1525680,
      "step": 443
    },
    {
      "epoch": 1.455737704918033,
      "grad_norm": 8.223572731018066,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 0.6091,
      "num_input_tokens_seen": 1528752,
      "step": 444
    },
    {
      "epoch": 1.459016393442623,
      "grad_norm": 7.003304481506348,
      "learning_rate": 3.39591987386325e-05,
      "loss": 0.5598,
      "num_input_tokens_seen": 1533360,
      "step": 445
    },
    {
      "epoch": 1.4622950819672131,
      "grad_norm": 6.338940143585205,
      "learning_rate": 3.387767406020343e-05,
      "loss": 0.4269,
      "num_input_tokens_seen": 1536944,
      "step": 446
    },
    {
      "epoch": 1.4655737704918033,
      "grad_norm": 4.5505266189575195,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 0.2937,
      "num_input_tokens_seen": 1540528,
      "step": 447
    },
    {
      "epoch": 1.4688524590163934,
      "grad_norm": 6.877295017242432,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 0.4029,
      "num_input_tokens_seen": 1543600,
      "step": 448
    },
    {
      "epoch": 1.4721311475409835,
      "grad_norm": 8.864465713500977,
      "learning_rate": 3.363245497496337e-05,
      "loss": 0.6055,
      "num_input_tokens_seen": 1547184,
      "step": 449
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 6.158749103546143,
      "learning_rate": 3.355050358314172e-05,
      "loss": 0.3409,
      "num_input_tokens_seen": 1550768,
      "step": 450
    },
    {
      "epoch": 1.478688524590164,
      "grad_norm": 6.837271690368652,
      "learning_rate": 3.346844800613229e-05,
      "loss": 0.3465,
      "num_input_tokens_seen": 1553840,
      "step": 451
    },
    {
      "epoch": 1.481967213114754,
      "grad_norm": 7.738142013549805,
      "learning_rate": 3.338628924375638e-05,
      "loss": 0.5213,
      "num_input_tokens_seen": 1557424,
      "step": 452
    },
    {
      "epoch": 1.4852459016393442,
      "grad_norm": 8.303750038146973,
      "learning_rate": 3.330402829709258e-05,
      "loss": 0.6567,
      "num_input_tokens_seen": 1561008,
      "step": 453
    },
    {
      "epoch": 1.4885245901639343,
      "grad_norm": 9.407169342041016,
      "learning_rate": 3.322166616846458e-05,
      "loss": 0.6445,
      "num_input_tokens_seen": 1564080,
      "step": 454
    },
    {
      "epoch": 1.4918032786885247,
      "grad_norm": 6.680990219116211,
      "learning_rate": 3.313920386142892e-05,
      "loss": 0.4263,
      "num_input_tokens_seen": 1567664,
      "step": 455
    },
    {
      "epoch": 1.4950819672131148,
      "grad_norm": 6.161906719207764,
      "learning_rate": 3.305664238076278e-05,
      "loss": 0.4936,
      "num_input_tokens_seen": 1571248,
      "step": 456
    },
    {
      "epoch": 1.498360655737705,
      "grad_norm": 6.921408176422119,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 0.3858,
      "num_input_tokens_seen": 1575344,
      "step": 457
    },
    {
      "epoch": 1.501639344262295,
      "grad_norm": 8.140783309936523,
      "learning_rate": 3.289122592367757e-05,
      "loss": 0.613,
      "num_input_tokens_seen": 1578928,
      "step": 458
    },
    {
      "epoch": 1.5049180327868852,
      "grad_norm": 7.02859354019165,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 0.5089,
      "num_input_tokens_seen": 1582512,
      "step": 459
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 5.848351955413818,
      "learning_rate": 3.272542485937369e-05,
      "loss": 0.3975,
      "num_input_tokens_seen": 1586096,
      "step": 460
    },
    {
      "epoch": 1.5114754098360654,
      "grad_norm": 7.557081699371338,
      "learning_rate": 3.264238262407764e-05,
      "loss": 0.4496,
      "num_input_tokens_seen": 1589168,
      "step": 461
    },
    {
      "epoch": 1.5147540983606558,
      "grad_norm": 6.869940757751465,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 0.2737,
      "num_input_tokens_seen": 1592240,
      "step": 462
    },
    {
      "epoch": 1.518032786885246,
      "grad_norm": 6.1348795890808105,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.3604,
      "num_input_tokens_seen": 1595824,
      "step": 463
    },
    {
      "epoch": 1.521311475409836,
      "grad_norm": 3.9511497020721436,
      "learning_rate": 3.239270125110117e-05,
      "loss": 0.1923,
      "num_input_tokens_seen": 1598896,
      "step": 464
    },
    {
      "epoch": 1.5245901639344264,
      "grad_norm": 6.8227458000183105,
      "learning_rate": 3.230929261806842e-05,
      "loss": 0.3593,
      "num_input_tokens_seen": 1601968,
      "step": 465
    },
    {
      "epoch": 1.5278688524590165,
      "grad_norm": 10.879429817199707,
      "learning_rate": 3.222579492361179e-05,
      "loss": 0.6606,
      "num_input_tokens_seen": 1605552,
      "step": 466
    },
    {
      "epoch": 1.5311475409836066,
      "grad_norm": 7.969915866851807,
      "learning_rate": 3.214220918512434e-05,
      "loss": 0.4162,
      "num_input_tokens_seen": 1609136,
      "step": 467
    },
    {
      "epoch": 1.5344262295081967,
      "grad_norm": 8.178327560424805,
      "learning_rate": 3.205853642107192e-05,
      "loss": 0.521,
      "num_input_tokens_seen": 1612208,
      "step": 468
    },
    {
      "epoch": 1.5377049180327869,
      "grad_norm": 11.09627628326416,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 0.4177,
      "num_input_tokens_seen": 1615792,
      "step": 469
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 10.511483192443848,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 0.4716,
      "num_input_tokens_seen": 1618864,
      "step": 470
    },
    {
      "epoch": 1.544262295081967,
      "grad_norm": 6.791950702667236,
      "learning_rate": 3.180700617601436e-05,
      "loss": 0.3909,
      "num_input_tokens_seen": 1622448,
      "step": 471
    },
    {
      "epoch": 1.5475409836065572,
      "grad_norm": 8.064687728881836,
      "learning_rate": 3.172299551538164e-05,
      "loss": 0.4165,
      "num_input_tokens_seen": 1625520,
      "step": 472
    },
    {
      "epoch": 1.5508196721311476,
      "grad_norm": 10.603822708129883,
      "learning_rate": 3.163890293717022e-05,
      "loss": 0.3467,
      "num_input_tokens_seen": 1629104,
      "step": 473
    },
    {
      "epoch": 1.5540983606557377,
      "grad_norm": 7.8259477615356445,
      "learning_rate": 3.155472946602162e-05,
      "loss": 0.3624,
      "num_input_tokens_seen": 1632688,
      "step": 474
    },
    {
      "epoch": 1.5573770491803278,
      "grad_norm": 10.731529235839844,
      "learning_rate": 3.147047612756302e-05,
      "loss": 0.7442,
      "num_input_tokens_seen": 1636272,
      "step": 475
    },
    {
      "epoch": 1.5606557377049182,
      "grad_norm": 10.034735679626465,
      "learning_rate": 3.138614394839476e-05,
      "loss": 0.5584,
      "num_input_tokens_seen": 1639856,
      "step": 476
    },
    {
      "epoch": 1.5639344262295083,
      "grad_norm": 7.719532012939453,
      "learning_rate": 3.130173395607785e-05,
      "loss": 0.3657,
      "num_input_tokens_seen": 1642928,
      "step": 477
    },
    {
      "epoch": 1.5672131147540984,
      "grad_norm": 6.454411029815674,
      "learning_rate": 3.121724717912138e-05,
      "loss": 0.3374,
      "num_input_tokens_seen": 1646000,
      "step": 478
    },
    {
      "epoch": 1.5704918032786885,
      "grad_norm": 7.941906929016113,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 0.3214,
      "num_input_tokens_seen": 1649584,
      "step": 479
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 12.423262596130371,
      "learning_rate": 3.104804738999169e-05,
      "loss": 0.6296,
      "num_input_tokens_seen": 1653168,
      "step": 480
    },
    {
      "epoch": 1.5770491803278688,
      "grad_norm": 10.266712188720703,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 0.3727,
      "num_input_tokens_seen": 1656240,
      "step": 481
    },
    {
      "epoch": 1.580327868852459,
      "grad_norm": 8.270668029785156,
      "learning_rate": 3.087855282756475e-05,
      "loss": 0.4891,
      "num_input_tokens_seen": 1659824,
      "step": 482
    },
    {
      "epoch": 1.583606557377049,
      "grad_norm": 10.72032642364502,
      "learning_rate": 3.079369758735393e-05,
      "loss": 0.4719,
      "num_input_tokens_seen": 1663408,
      "step": 483
    },
    {
      "epoch": 1.5868852459016394,
      "grad_norm": 11.804098129272461,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 0.9784,
      "num_input_tokens_seen": 1668016,
      "step": 484
    },
    {
      "epoch": 1.5901639344262295,
      "grad_norm": 9.901402473449707,
      "learning_rate": 3.062377635859663e-05,
      "loss": 0.3759,
      "num_input_tokens_seen": 1671600,
      "step": 485
    },
    {
      "epoch": 1.5934426229508196,
      "grad_norm": 9.760473251342773,
      "learning_rate": 3.053871244048669e-05,
      "loss": 0.4595,
      "num_input_tokens_seen": 1674672,
      "step": 486
    },
    {
      "epoch": 1.59672131147541,
      "grad_norm": 8.27658462524414,
      "learning_rate": 3.045358103491357e-05,
      "loss": 0.5634,
      "num_input_tokens_seen": 1678256,
      "step": 487
    },
    {
      "epoch": 1.6,
      "grad_norm": 7.681777477264404,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 0.5358,
      "num_input_tokens_seen": 1681840,
      "step": 488
    },
    {
      "epoch": 1.6032786885245902,
      "grad_norm": 7.506468772888184,
      "learning_rate": 3.028311991138472e-05,
      "loss": 0.5012,
      "num_input_tokens_seen": 1684912,
      "step": 489
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 9.384390830993652,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 0.4473,
      "num_input_tokens_seen": 1687984,
      "step": 490
    },
    {
      "epoch": 1.6098360655737705,
      "grad_norm": 6.699517726898193,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 0.3176,
      "num_input_tokens_seen": 1691568,
      "step": 491
    },
    {
      "epoch": 1.6131147540983606,
      "grad_norm": 8.466188430786133,
      "learning_rate": 3.002694802864912e-05,
      "loss": 0.3664,
      "num_input_tokens_seen": 1695664,
      "step": 492
    },
    {
      "epoch": 1.6163934426229507,
      "grad_norm": 7.359668254852295,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 0.4039,
      "num_input_tokens_seen": 1698736,
      "step": 493
    },
    {
      "epoch": 1.6196721311475408,
      "grad_norm": 5.648977279663086,
      "learning_rate": 2.98558587804993e-05,
      "loss": 0.2533,
      "num_input_tokens_seen": 1702832,
      "step": 494
    },
    {
      "epoch": 1.6229508196721312,
      "grad_norm": 11.06004810333252,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 0.4662,
      "num_input_tokens_seen": 1705904,
      "step": 495
    },
    {
      "epoch": 1.6262295081967213,
      "grad_norm": 8.846047401428223,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 0.5384,
      "num_input_tokens_seen": 1709488,
      "step": 496
    },
    {
      "epoch": 1.6295081967213116,
      "grad_norm": 6.281642436981201,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.423,
      "num_input_tokens_seen": 1712560,
      "step": 497
    },
    {
      "epoch": 1.6327868852459018,
      "grad_norm": 10.176566123962402,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 0.7052,
      "num_input_tokens_seen": 1716656,
      "step": 498
    },
    {
      "epoch": 1.6360655737704919,
      "grad_norm": 6.243122577667236,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.3238,
      "num_input_tokens_seen": 1720240,
      "step": 499
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 5.21003532409668,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 0.2914,
      "num_input_tokens_seen": 1723312,
      "step": 500
    },
    {
      "epoch": 1.639344262295082,
      "eval_loss": 0.4377073347568512,
      "eval_runtime": 12.0749,
      "eval_samples_per_second": 101.119,
      "eval_steps_per_second": 12.671,
      "num_input_tokens_seen": 1723312,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 1723312,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.763519743839437e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
