{
  "best_metric": 0.4453779458999634,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_16_bs_4096_seqlen/liger_tau_commonsense_qa/checkpoint-1000",
  "epoch": 1.6420361247947455,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016420361247947454,
      "grad_norm": 8.320833206176758,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9391,
      "num_input_tokens_seen": 1536,
      "step": 1
    },
    {
      "epoch": 0.003284072249589491,
      "grad_norm": 6.269535064697266,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.2827,
      "num_input_tokens_seen": 3328,
      "step": 2
    },
    {
      "epoch": 0.0049261083743842365,
      "grad_norm": 6.393245220184326,
      "learning_rate": 1.5e-06,
      "loss": 1.4587,
      "num_input_tokens_seen": 5120,
      "step": 3
    },
    {
      "epoch": 0.006568144499178982,
      "grad_norm": 6.764632225036621,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.2798,
      "num_input_tokens_seen": 6912,
      "step": 4
    },
    {
      "epoch": 0.008210180623973728,
      "grad_norm": 7.1447882652282715,
      "learning_rate": 2.5e-06,
      "loss": 1.0998,
      "num_input_tokens_seen": 8960,
      "step": 5
    },
    {
      "epoch": 0.009852216748768473,
      "grad_norm": 7.972263813018799,
      "learning_rate": 3e-06,
      "loss": 1.7439,
      "num_input_tokens_seen": 11008,
      "step": 6
    },
    {
      "epoch": 0.011494252873563218,
      "grad_norm": 6.503018856048584,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.262,
      "num_input_tokens_seen": 12544,
      "step": 7
    },
    {
      "epoch": 0.013136288998357963,
      "grad_norm": 6.256182670593262,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.4057,
      "num_input_tokens_seen": 14080,
      "step": 8
    },
    {
      "epoch": 0.014778325123152709,
      "grad_norm": 5.949939250946045,
      "learning_rate": 4.5e-06,
      "loss": 0.9391,
      "num_input_tokens_seen": 15872,
      "step": 9
    },
    {
      "epoch": 0.016420361247947456,
      "grad_norm": 7.6023077964782715,
      "learning_rate": 5e-06,
      "loss": 1.9809,
      "num_input_tokens_seen": 17664,
      "step": 10
    },
    {
      "epoch": 0.0180623973727422,
      "grad_norm": 8.001041412353516,
      "learning_rate": 5.500000000000001e-06,
      "loss": 2.2038,
      "num_input_tokens_seen": 19200,
      "step": 11
    },
    {
      "epoch": 0.019704433497536946,
      "grad_norm": 6.477428913116455,
      "learning_rate": 6e-06,
      "loss": 1.3819,
      "num_input_tokens_seen": 20736,
      "step": 12
    },
    {
      "epoch": 0.021346469622331693,
      "grad_norm": 7.136923789978027,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.5584,
      "num_input_tokens_seen": 22528,
      "step": 13
    },
    {
      "epoch": 0.022988505747126436,
      "grad_norm": 5.900494575500488,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.9681,
      "num_input_tokens_seen": 24064,
      "step": 14
    },
    {
      "epoch": 0.024630541871921183,
      "grad_norm": 7.061529159545898,
      "learning_rate": 7.5e-06,
      "loss": 1.626,
      "num_input_tokens_seen": 25856,
      "step": 15
    },
    {
      "epoch": 0.026272577996715927,
      "grad_norm": 9.52997875213623,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.4762,
      "num_input_tokens_seen": 27392,
      "step": 16
    },
    {
      "epoch": 0.027914614121510674,
      "grad_norm": 9.08239459991455,
      "learning_rate": 8.500000000000002e-06,
      "loss": 2.4705,
      "num_input_tokens_seen": 29440,
      "step": 17
    },
    {
      "epoch": 0.029556650246305417,
      "grad_norm": 6.0102434158325195,
      "learning_rate": 9e-06,
      "loss": 1.1944,
      "num_input_tokens_seen": 30976,
      "step": 18
    },
    {
      "epoch": 0.031198686371100164,
      "grad_norm": 8.550704002380371,
      "learning_rate": 9.5e-06,
      "loss": 2.0576,
      "num_input_tokens_seen": 32512,
      "step": 19
    },
    {
      "epoch": 0.03284072249589491,
      "grad_norm": 6.461069583892822,
      "learning_rate": 1e-05,
      "loss": 1.2292,
      "num_input_tokens_seen": 34560,
      "step": 20
    },
    {
      "epoch": 0.034482758620689655,
      "grad_norm": 7.4704694747924805,
      "learning_rate": 1.05e-05,
      "loss": 1.0791,
      "num_input_tokens_seen": 36096,
      "step": 21
    },
    {
      "epoch": 0.0361247947454844,
      "grad_norm": 6.593317031860352,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.1396,
      "num_input_tokens_seen": 37888,
      "step": 22
    },
    {
      "epoch": 0.03776683087027915,
      "grad_norm": 6.30527400970459,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.8806,
      "num_input_tokens_seen": 39424,
      "step": 23
    },
    {
      "epoch": 0.03940886699507389,
      "grad_norm": 5.498584747314453,
      "learning_rate": 1.2e-05,
      "loss": 0.5966,
      "num_input_tokens_seen": 40960,
      "step": 24
    },
    {
      "epoch": 0.041050903119868636,
      "grad_norm": 6.093471527099609,
      "learning_rate": 1.25e-05,
      "loss": 1.0538,
      "num_input_tokens_seen": 42752,
      "step": 25
    },
    {
      "epoch": 0.042692939244663386,
      "grad_norm": 7.4986748695373535,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.4794,
      "num_input_tokens_seen": 44288,
      "step": 26
    },
    {
      "epoch": 0.04433497536945813,
      "grad_norm": 7.779294967651367,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.2454,
      "num_input_tokens_seen": 45824,
      "step": 27
    },
    {
      "epoch": 0.04597701149425287,
      "grad_norm": 7.161386966705322,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.5327,
      "num_input_tokens_seen": 47360,
      "step": 28
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 5.805469512939453,
      "learning_rate": 1.45e-05,
      "loss": 1.1869,
      "num_input_tokens_seen": 48896,
      "step": 29
    },
    {
      "epoch": 0.04926108374384237,
      "grad_norm": 7.249826431274414,
      "learning_rate": 1.5e-05,
      "loss": 1.444,
      "num_input_tokens_seen": 50688,
      "step": 30
    },
    {
      "epoch": 0.05090311986863711,
      "grad_norm": 7.6984944343566895,
      "learning_rate": 1.55e-05,
      "loss": 0.9847,
      "num_input_tokens_seen": 52224,
      "step": 31
    },
    {
      "epoch": 0.052545155993431854,
      "grad_norm": 5.519339561462402,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9947,
      "num_input_tokens_seen": 54272,
      "step": 32
    },
    {
      "epoch": 0.054187192118226604,
      "grad_norm": 6.05331563949585,
      "learning_rate": 1.65e-05,
      "loss": 0.666,
      "num_input_tokens_seen": 55808,
      "step": 33
    },
    {
      "epoch": 0.05582922824302135,
      "grad_norm": 5.448238372802734,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.8655,
      "num_input_tokens_seen": 57344,
      "step": 34
    },
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 7.113207817077637,
      "learning_rate": 1.75e-05,
      "loss": 1.1348,
      "num_input_tokens_seen": 58880,
      "step": 35
    },
    {
      "epoch": 0.059113300492610835,
      "grad_norm": 4.97666072845459,
      "learning_rate": 1.8e-05,
      "loss": 0.7809,
      "num_input_tokens_seen": 60416,
      "step": 36
    },
    {
      "epoch": 0.060755336617405585,
      "grad_norm": 5.872999668121338,
      "learning_rate": 1.85e-05,
      "loss": 1.0052,
      "num_input_tokens_seen": 61952,
      "step": 37
    },
    {
      "epoch": 0.06239737274220033,
      "grad_norm": 6.976776123046875,
      "learning_rate": 1.9e-05,
      "loss": 1.1081,
      "num_input_tokens_seen": 63744,
      "step": 38
    },
    {
      "epoch": 0.06403940886699508,
      "grad_norm": 6.3175048828125,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.5637,
      "num_input_tokens_seen": 65280,
      "step": 39
    },
    {
      "epoch": 0.06568144499178982,
      "grad_norm": 4.835817337036133,
      "learning_rate": 2e-05,
      "loss": 1.0792,
      "num_input_tokens_seen": 66816,
      "step": 40
    },
    {
      "epoch": 0.06732348111658457,
      "grad_norm": 3.3580455780029297,
      "learning_rate": 2.05e-05,
      "loss": 0.3394,
      "num_input_tokens_seen": 68864,
      "step": 41
    },
    {
      "epoch": 0.06896551724137931,
      "grad_norm": 4.831844329833984,
      "learning_rate": 2.1e-05,
      "loss": 0.8548,
      "num_input_tokens_seen": 70400,
      "step": 42
    },
    {
      "epoch": 0.07060755336617405,
      "grad_norm": 5.52235746383667,
      "learning_rate": 2.15e-05,
      "loss": 0.9917,
      "num_input_tokens_seen": 72192,
      "step": 43
    },
    {
      "epoch": 0.0722495894909688,
      "grad_norm": 3.1891930103302,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4063,
      "num_input_tokens_seen": 73728,
      "step": 44
    },
    {
      "epoch": 0.07389162561576355,
      "grad_norm": 8.179353713989258,
      "learning_rate": 2.25e-05,
      "loss": 0.9893,
      "num_input_tokens_seen": 75520,
      "step": 45
    },
    {
      "epoch": 0.0755336617405583,
      "grad_norm": 5.792477607727051,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.6072,
      "num_input_tokens_seen": 77312,
      "step": 46
    },
    {
      "epoch": 0.07717569786535304,
      "grad_norm": 7.518441200256348,
      "learning_rate": 2.35e-05,
      "loss": 1.5395,
      "num_input_tokens_seen": 78848,
      "step": 47
    },
    {
      "epoch": 0.07881773399014778,
      "grad_norm": 8.124507904052734,
      "learning_rate": 2.4e-05,
      "loss": 1.7025,
      "num_input_tokens_seen": 80384,
      "step": 48
    },
    {
      "epoch": 0.08045977011494253,
      "grad_norm": 6.604042053222656,
      "learning_rate": 2.45e-05,
      "loss": 1.0349,
      "num_input_tokens_seen": 82176,
      "step": 49
    },
    {
      "epoch": 0.08210180623973727,
      "grad_norm": 4.812566757202148,
      "learning_rate": 2.5e-05,
      "loss": 0.8146,
      "num_input_tokens_seen": 83712,
      "step": 50
    },
    {
      "epoch": 0.08374384236453201,
      "grad_norm": 3.880526304244995,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.5331,
      "num_input_tokens_seen": 85504,
      "step": 51
    },
    {
      "epoch": 0.08538587848932677,
      "grad_norm": 3.1551995277404785,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.799,
      "num_input_tokens_seen": 87296,
      "step": 52
    },
    {
      "epoch": 0.08702791461412152,
      "grad_norm": 4.790073394775391,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.6104,
      "num_input_tokens_seen": 89088,
      "step": 53
    },
    {
      "epoch": 0.08866995073891626,
      "grad_norm": 3.3616628646850586,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.7139,
      "num_input_tokens_seen": 90624,
      "step": 54
    },
    {
      "epoch": 0.090311986863711,
      "grad_norm": 4.32742977142334,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.6742,
      "num_input_tokens_seen": 92416,
      "step": 55
    },
    {
      "epoch": 0.09195402298850575,
      "grad_norm": 3.6549298763275146,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.7276,
      "num_input_tokens_seen": 94208,
      "step": 56
    },
    {
      "epoch": 0.09359605911330049,
      "grad_norm": 3.556574821472168,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.7706,
      "num_input_tokens_seen": 96000,
      "step": 57
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 3.7287886142730713,
      "learning_rate": 2.9e-05,
      "loss": 0.5925,
      "num_input_tokens_seen": 97536,
      "step": 58
    },
    {
      "epoch": 0.09688013136288999,
      "grad_norm": 2.9129505157470703,
      "learning_rate": 2.95e-05,
      "loss": 0.5357,
      "num_input_tokens_seen": 99328,
      "step": 59
    },
    {
      "epoch": 0.09852216748768473,
      "grad_norm": 5.111996650695801,
      "learning_rate": 3e-05,
      "loss": 0.7578,
      "num_input_tokens_seen": 100864,
      "step": 60
    },
    {
      "epoch": 0.10016420361247948,
      "grad_norm": 4.129121780395508,
      "learning_rate": 3.05e-05,
      "loss": 0.753,
      "num_input_tokens_seen": 102656,
      "step": 61
    },
    {
      "epoch": 0.10180623973727422,
      "grad_norm": 4.299892902374268,
      "learning_rate": 3.1e-05,
      "loss": 0.8913,
      "num_input_tokens_seen": 104448,
      "step": 62
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 4.47353458404541,
      "learning_rate": 3.15e-05,
      "loss": 0.829,
      "num_input_tokens_seen": 105984,
      "step": 63
    },
    {
      "epoch": 0.10509031198686371,
      "grad_norm": 6.5916242599487305,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.546,
      "num_input_tokens_seen": 107520,
      "step": 64
    },
    {
      "epoch": 0.10673234811165845,
      "grad_norm": 3.9817049503326416,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.7181,
      "num_input_tokens_seen": 109056,
      "step": 65
    },
    {
      "epoch": 0.10837438423645321,
      "grad_norm": 4.9240946769714355,
      "learning_rate": 3.3e-05,
      "loss": 1.0685,
      "num_input_tokens_seen": 110592,
      "step": 66
    },
    {
      "epoch": 0.11001642036124795,
      "grad_norm": 4.263679027557373,
      "learning_rate": 3.35e-05,
      "loss": 0.5384,
      "num_input_tokens_seen": 112128,
      "step": 67
    },
    {
      "epoch": 0.1116584564860427,
      "grad_norm": 3.8759355545043945,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6983,
      "num_input_tokens_seen": 113664,
      "step": 68
    },
    {
      "epoch": 0.11330049261083744,
      "grad_norm": 5.363999366760254,
      "learning_rate": 3.45e-05,
      "loss": 1.3343,
      "num_input_tokens_seen": 115456,
      "step": 69
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 4.616272449493408,
      "learning_rate": 3.5e-05,
      "loss": 1.0989,
      "num_input_tokens_seen": 116992,
      "step": 70
    },
    {
      "epoch": 0.11658456486042693,
      "grad_norm": 2.8674333095550537,
      "learning_rate": 3.55e-05,
      "loss": 0.7004,
      "num_input_tokens_seen": 118528,
      "step": 71
    },
    {
      "epoch": 0.11822660098522167,
      "grad_norm": 3.6532180309295654,
      "learning_rate": 3.6e-05,
      "loss": 0.6881,
      "num_input_tokens_seen": 120064,
      "step": 72
    },
    {
      "epoch": 0.11986863711001643,
      "grad_norm": 4.52836799621582,
      "learning_rate": 3.65e-05,
      "loss": 0.9007,
      "num_input_tokens_seen": 121600,
      "step": 73
    },
    {
      "epoch": 0.12151067323481117,
      "grad_norm": 5.003735065460205,
      "learning_rate": 3.7e-05,
      "loss": 0.8005,
      "num_input_tokens_seen": 123136,
      "step": 74
    },
    {
      "epoch": 0.12315270935960591,
      "grad_norm": 3.01255202293396,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.9412,
      "num_input_tokens_seen": 124928,
      "step": 75
    },
    {
      "epoch": 0.12479474548440066,
      "grad_norm": 4.174691677093506,
      "learning_rate": 3.8e-05,
      "loss": 0.9269,
      "num_input_tokens_seen": 126976,
      "step": 76
    },
    {
      "epoch": 0.12643678160919541,
      "grad_norm": 3.2333016395568848,
      "learning_rate": 3.85e-05,
      "loss": 0.6985,
      "num_input_tokens_seen": 128512,
      "step": 77
    },
    {
      "epoch": 0.12807881773399016,
      "grad_norm": 4.815736293792725,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.3431,
      "num_input_tokens_seen": 130048,
      "step": 78
    },
    {
      "epoch": 0.1297208538587849,
      "grad_norm": 4.679624557495117,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.0147,
      "num_input_tokens_seen": 131584,
      "step": 79
    },
    {
      "epoch": 0.13136288998357964,
      "grad_norm": 4.470336437225342,
      "learning_rate": 4e-05,
      "loss": 0.5571,
      "num_input_tokens_seen": 133376,
      "step": 80
    },
    {
      "epoch": 0.1330049261083744,
      "grad_norm": 3.5956501960754395,
      "learning_rate": 4.05e-05,
      "loss": 0.9358,
      "num_input_tokens_seen": 135168,
      "step": 81
    },
    {
      "epoch": 0.13464696223316913,
      "grad_norm": 3.8383305072784424,
      "learning_rate": 4.1e-05,
      "loss": 0.7325,
      "num_input_tokens_seen": 136704,
      "step": 82
    },
    {
      "epoch": 0.13628899835796388,
      "grad_norm": 4.16798210144043,
      "learning_rate": 4.15e-05,
      "loss": 0.5901,
      "num_input_tokens_seen": 138240,
      "step": 83
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 3.477592945098877,
      "learning_rate": 4.2e-05,
      "loss": 0.7117,
      "num_input_tokens_seen": 140032,
      "step": 84
    },
    {
      "epoch": 0.13957307060755336,
      "grad_norm": 4.430858135223389,
      "learning_rate": 4.25e-05,
      "loss": 0.5712,
      "num_input_tokens_seen": 141568,
      "step": 85
    },
    {
      "epoch": 0.1412151067323481,
      "grad_norm": 5.069769859313965,
      "learning_rate": 4.3e-05,
      "loss": 0.7494,
      "num_input_tokens_seen": 143360,
      "step": 86
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 3.64965558052063,
      "learning_rate": 4.35e-05,
      "loss": 0.8355,
      "num_input_tokens_seen": 144896,
      "step": 87
    },
    {
      "epoch": 0.1444991789819376,
      "grad_norm": 4.50240421295166,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.419,
      "num_input_tokens_seen": 146688,
      "step": 88
    },
    {
      "epoch": 0.14614121510673234,
      "grad_norm": 4.539053440093994,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.8544,
      "num_input_tokens_seen": 148480,
      "step": 89
    },
    {
      "epoch": 0.1477832512315271,
      "grad_norm": 4.258608341217041,
      "learning_rate": 4.5e-05,
      "loss": 0.8761,
      "num_input_tokens_seen": 150272,
      "step": 90
    },
    {
      "epoch": 0.14942528735632185,
      "grad_norm": 5.120360374450684,
      "learning_rate": 4.55e-05,
      "loss": 0.8403,
      "num_input_tokens_seen": 151808,
      "step": 91
    },
    {
      "epoch": 0.1510673234811166,
      "grad_norm": 5.145753860473633,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.8984,
      "num_input_tokens_seen": 153344,
      "step": 92
    },
    {
      "epoch": 0.15270935960591134,
      "grad_norm": 4.243793487548828,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.5511,
      "num_input_tokens_seen": 155136,
      "step": 93
    },
    {
      "epoch": 0.15435139573070608,
      "grad_norm": 3.7668721675872803,
      "learning_rate": 4.7e-05,
      "loss": 0.3016,
      "num_input_tokens_seen": 156928,
      "step": 94
    },
    {
      "epoch": 0.15599343185550082,
      "grad_norm": 5.300553798675537,
      "learning_rate": 4.75e-05,
      "loss": 0.7943,
      "num_input_tokens_seen": 158720,
      "step": 95
    },
    {
      "epoch": 0.15763546798029557,
      "grad_norm": 5.735097408294678,
      "learning_rate": 4.8e-05,
      "loss": 0.7217,
      "num_input_tokens_seen": 160256,
      "step": 96
    },
    {
      "epoch": 0.1592775041050903,
      "grad_norm": 8.108965873718262,
      "learning_rate": 4.85e-05,
      "loss": 1.2114,
      "num_input_tokens_seen": 162304,
      "step": 97
    },
    {
      "epoch": 0.16091954022988506,
      "grad_norm": 5.278881072998047,
      "learning_rate": 4.9e-05,
      "loss": 0.5704,
      "num_input_tokens_seen": 163840,
      "step": 98
    },
    {
      "epoch": 0.1625615763546798,
      "grad_norm": 4.378927707672119,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.7284,
      "num_input_tokens_seen": 165376,
      "step": 99
    },
    {
      "epoch": 0.16420361247947454,
      "grad_norm": 5.004955768585205,
      "learning_rate": 5e-05,
      "loss": 0.6789,
      "num_input_tokens_seen": 166912,
      "step": 100
    },
    {
      "epoch": 0.16584564860426929,
      "grad_norm": 10.889677047729492,
      "learning_rate": 4.999984769144476e-05,
      "loss": 1.4495,
      "num_input_tokens_seen": 168448,
      "step": 101
    },
    {
      "epoch": 0.16748768472906403,
      "grad_norm": 5.778162956237793,
      "learning_rate": 4.999939076763487e-05,
      "loss": 0.6467,
      "num_input_tokens_seen": 170240,
      "step": 102
    },
    {
      "epoch": 0.16912972085385877,
      "grad_norm": 6.595263481140137,
      "learning_rate": 4.999862923413781e-05,
      "loss": 0.4734,
      "num_input_tokens_seen": 172032,
      "step": 103
    },
    {
      "epoch": 0.17077175697865354,
      "grad_norm": 4.195610046386719,
      "learning_rate": 4.999756310023261e-05,
      "loss": 0.6411,
      "num_input_tokens_seen": 173824,
      "step": 104
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 5.903526306152344,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 0.7587,
      "num_input_tokens_seen": 175360,
      "step": 105
    },
    {
      "epoch": 0.17405582922824303,
      "grad_norm": 6.096695899963379,
      "learning_rate": 4.999451708687114e-05,
      "loss": 0.8492,
      "num_input_tokens_seen": 177152,
      "step": 106
    },
    {
      "epoch": 0.17569786535303777,
      "grad_norm": 5.547875881195068,
      "learning_rate": 4.999253724452958e-05,
      "loss": 0.3822,
      "num_input_tokens_seen": 178688,
      "step": 107
    },
    {
      "epoch": 0.17733990147783252,
      "grad_norm": 4.14644193649292,
      "learning_rate": 4.999025287600886e-05,
      "loss": 0.7154,
      "num_input_tokens_seen": 180480,
      "step": 108
    },
    {
      "epoch": 0.17898193760262726,
      "grad_norm": 8.730639457702637,
      "learning_rate": 4.998766400914329e-05,
      "loss": 0.9473,
      "num_input_tokens_seen": 182272,
      "step": 109
    },
    {
      "epoch": 0.180623973727422,
      "grad_norm": 6.203502178192139,
      "learning_rate": 4.99847706754774e-05,
      "loss": 1.0065,
      "num_input_tokens_seen": 183808,
      "step": 110
    },
    {
      "epoch": 0.18226600985221675,
      "grad_norm": 3.5459494590759277,
      "learning_rate": 4.998157291026553e-05,
      "loss": 0.4083,
      "num_input_tokens_seen": 185600,
      "step": 111
    },
    {
      "epoch": 0.1839080459770115,
      "grad_norm": 5.144546031951904,
      "learning_rate": 4.997807075247146e-05,
      "loss": 0.689,
      "num_input_tokens_seen": 187392,
      "step": 112
    },
    {
      "epoch": 0.18555008210180624,
      "grad_norm": 5.910136699676514,
      "learning_rate": 4.997426424476787e-05,
      "loss": 0.9811,
      "num_input_tokens_seen": 188928,
      "step": 113
    },
    {
      "epoch": 0.18719211822660098,
      "grad_norm": 5.804332256317139,
      "learning_rate": 4.997015343353585e-05,
      "loss": 0.839,
      "num_input_tokens_seen": 190976,
      "step": 114
    },
    {
      "epoch": 0.18883415435139572,
      "grad_norm": 3.728027820587158,
      "learning_rate": 4.996573836886435e-05,
      "loss": 0.4681,
      "num_input_tokens_seen": 192512,
      "step": 115
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 5.448850154876709,
      "learning_rate": 4.996101910454953e-05,
      "loss": 0.4856,
      "num_input_tokens_seen": 194048,
      "step": 116
    },
    {
      "epoch": 0.1921182266009852,
      "grad_norm": 4.473478317260742,
      "learning_rate": 4.995599569809414e-05,
      "loss": 0.5914,
      "num_input_tokens_seen": 195584,
      "step": 117
    },
    {
      "epoch": 0.19376026272577998,
      "grad_norm": 5.902856826782227,
      "learning_rate": 4.995066821070679e-05,
      "loss": 0.6366,
      "num_input_tokens_seen": 197376,
      "step": 118
    },
    {
      "epoch": 0.19540229885057472,
      "grad_norm": 6.07679557800293,
      "learning_rate": 4.994503670730125e-05,
      "loss": 1.1988,
      "num_input_tokens_seen": 198912,
      "step": 119
    },
    {
      "epoch": 0.19704433497536947,
      "grad_norm": 4.684236526489258,
      "learning_rate": 4.993910125649561e-05,
      "loss": 0.5591,
      "num_input_tokens_seen": 200448,
      "step": 120
    },
    {
      "epoch": 0.1986863711001642,
      "grad_norm": 4.756756782531738,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 0.5438,
      "num_input_tokens_seen": 202240,
      "step": 121
    },
    {
      "epoch": 0.20032840722495895,
      "grad_norm": 3.9197113513946533,
      "learning_rate": 4.992631880567301e-05,
      "loss": 0.3852,
      "num_input_tokens_seen": 204032,
      "step": 122
    },
    {
      "epoch": 0.2019704433497537,
      "grad_norm": 5.195866584777832,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.777,
      "num_input_tokens_seen": 206080,
      "step": 123
    },
    {
      "epoch": 0.20361247947454844,
      "grad_norm": 6.205840587615967,
      "learning_rate": 4.991232148123761e-05,
      "loss": 0.7714,
      "num_input_tokens_seen": 207616,
      "step": 124
    },
    {
      "epoch": 0.20525451559934318,
      "grad_norm": 6.300273895263672,
      "learning_rate": 4.990486745229364e-05,
      "loss": 0.511,
      "num_input_tokens_seen": 209152,
      "step": 125
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 5.200537204742432,
      "learning_rate": 4.989710996539926e-05,
      "loss": 0.3203,
      "num_input_tokens_seen": 210688,
      "step": 126
    },
    {
      "epoch": 0.20853858784893267,
      "grad_norm": 6.4079437255859375,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 0.7184,
      "num_input_tokens_seen": 212480,
      "step": 127
    },
    {
      "epoch": 0.21018062397372742,
      "grad_norm": 7.748164653778076,
      "learning_rate": 4.988068499954578e-05,
      "loss": 0.867,
      "num_input_tokens_seen": 214016,
      "step": 128
    },
    {
      "epoch": 0.21182266009852216,
      "grad_norm": 5.7075018882751465,
      "learning_rate": 4.987201772071971e-05,
      "loss": 0.5366,
      "num_input_tokens_seen": 215808,
      "step": 129
    },
    {
      "epoch": 0.2134646962233169,
      "grad_norm": 6.479815483093262,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 0.7888,
      "num_input_tokens_seen": 217600,
      "step": 130
    },
    {
      "epoch": 0.21510673234811165,
      "grad_norm": 6.28126335144043,
      "learning_rate": 4.985377409930789e-05,
      "loss": 0.621,
      "num_input_tokens_seen": 219136,
      "step": 131
    },
    {
      "epoch": 0.21674876847290642,
      "grad_norm": 6.368687152862549,
      "learning_rate": 4.984419797901491e-05,
      "loss": 0.6888,
      "num_input_tokens_seen": 220672,
      "step": 132
    },
    {
      "epoch": 0.21839080459770116,
      "grad_norm": 5.123678207397461,
      "learning_rate": 4.983431914000991e-05,
      "loss": 0.5649,
      "num_input_tokens_seen": 222464,
      "step": 133
    },
    {
      "epoch": 0.2200328407224959,
      "grad_norm": 8.890868186950684,
      "learning_rate": 4.982413770266342e-05,
      "loss": 0.8991,
      "num_input_tokens_seen": 224000,
      "step": 134
    },
    {
      "epoch": 0.22167487684729065,
      "grad_norm": 7.147843360900879,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 0.6648,
      "num_input_tokens_seen": 225792,
      "step": 135
    },
    {
      "epoch": 0.2233169129720854,
      "grad_norm": 4.766005992889404,
      "learning_rate": 4.980286753286195e-05,
      "loss": 0.5941,
      "num_input_tokens_seen": 227328,
      "step": 136
    },
    {
      "epoch": 0.22495894909688013,
      "grad_norm": 4.824014663696289,
      "learning_rate": 4.979177905957726e-05,
      "loss": 0.5969,
      "num_input_tokens_seen": 228864,
      "step": 137
    },
    {
      "epoch": 0.22660098522167488,
      "grad_norm": 6.911489963531494,
      "learning_rate": 4.978038850628854e-05,
      "loss": 0.7607,
      "num_input_tokens_seen": 230400,
      "step": 138
    },
    {
      "epoch": 0.22824302134646962,
      "grad_norm": 6.338594913482666,
      "learning_rate": 4.976869601178609e-05,
      "loss": 0.7781,
      "num_input_tokens_seen": 231936,
      "step": 139
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 7.05186128616333,
      "learning_rate": 4.975670171853926e-05,
      "loss": 0.7888,
      "num_input_tokens_seen": 233728,
      "step": 140
    },
    {
      "epoch": 0.2315270935960591,
      "grad_norm": 4.822178840637207,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 0.5872,
      "num_input_tokens_seen": 235520,
      "step": 141
    },
    {
      "epoch": 0.23316912972085385,
      "grad_norm": 4.83343505859375,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 0.4103,
      "num_input_tokens_seen": 237056,
      "step": 142
    },
    {
      "epoch": 0.2348111658456486,
      "grad_norm": 8.16440200805664,
      "learning_rate": 4.971890952617515e-05,
      "loss": 0.8662,
      "num_input_tokens_seen": 238592,
      "step": 143
    },
    {
      "epoch": 0.23645320197044334,
      "grad_norm": 5.5400285720825195,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 0.6378,
      "num_input_tokens_seen": 240384,
      "step": 144
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 5.395066261291504,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.9786,
      "num_input_tokens_seen": 241920,
      "step": 145
    },
    {
      "epoch": 0.23973727422003285,
      "grad_norm": 6.112288475036621,
      "learning_rate": 4.96784066268247e-05,
      "loss": 0.3963,
      "num_input_tokens_seen": 243712,
      "step": 146
    },
    {
      "epoch": 0.2413793103448276,
      "grad_norm": 6.629218101501465,
      "learning_rate": 4.966430404017424e-05,
      "loss": 0.5869,
      "num_input_tokens_seen": 245504,
      "step": 147
    },
    {
      "epoch": 0.24302134646962234,
      "grad_norm": 6.196705341339111,
      "learning_rate": 4.964990092676263e-05,
      "loss": 0.6797,
      "num_input_tokens_seen": 247040,
      "step": 148
    },
    {
      "epoch": 0.24466338259441708,
      "grad_norm": 8.085169792175293,
      "learning_rate": 4.963519746208726e-05,
      "loss": 1.0737,
      "num_input_tokens_seen": 248832,
      "step": 149
    },
    {
      "epoch": 0.24630541871921183,
      "grad_norm": 3.9619879722595215,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.5164,
      "num_input_tokens_seen": 250624,
      "step": 150
    },
    {
      "epoch": 0.24794745484400657,
      "grad_norm": 4.127017498016357,
      "learning_rate": 4.960489019923105e-05,
      "loss": 0.49,
      "num_input_tokens_seen": 252160,
      "step": 151
    },
    {
      "epoch": 0.24958949096880131,
      "grad_norm": 3.681698799133301,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 0.2575,
      "num_input_tokens_seen": 253696,
      "step": 152
    },
    {
      "epoch": 0.2512315270935961,
      "grad_norm": 5.877799987792969,
      "learning_rate": 4.957338372873886e-05,
      "loss": 0.6174,
      "num_input_tokens_seen": 255232,
      "step": 153
    },
    {
      "epoch": 0.25287356321839083,
      "grad_norm": 4.83979606628418,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.287,
      "num_input_tokens_seen": 256768,
      "step": 154
    },
    {
      "epoch": 0.2545155993431856,
      "grad_norm": 5.5699968338012695,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 0.3856,
      "num_input_tokens_seen": 258304,
      "step": 155
    },
    {
      "epoch": 0.2561576354679803,
      "grad_norm": 5.739400386810303,
      "learning_rate": 4.952387888372979e-05,
      "loss": 0.3841,
      "num_input_tokens_seen": 259840,
      "step": 156
    },
    {
      "epoch": 0.25779967159277506,
      "grad_norm": 11.482378959655762,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 1.1275,
      "num_input_tokens_seen": 261632,
      "step": 157
    },
    {
      "epoch": 0.2594417077175698,
      "grad_norm": 7.271402359008789,
      "learning_rate": 4.94893812399836e-05,
      "loss": 0.7045,
      "num_input_tokens_seen": 263168,
      "step": 158
    },
    {
      "epoch": 0.26108374384236455,
      "grad_norm": 4.612240791320801,
      "learning_rate": 4.947168471904213e-05,
      "loss": 0.2536,
      "num_input_tokens_seen": 264704,
      "step": 159
    },
    {
      "epoch": 0.2627257799671593,
      "grad_norm": 6.820746898651123,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 0.4742,
      "num_input_tokens_seen": 266496,
      "step": 160
    },
    {
      "epoch": 0.26436781609195403,
      "grad_norm": 11.282927513122559,
      "learning_rate": 4.94353973571524e-05,
      "loss": 1.1953,
      "num_input_tokens_seen": 268288,
      "step": 161
    },
    {
      "epoch": 0.2660098522167488,
      "grad_norm": 10.525944709777832,
      "learning_rate": 4.94168069583542e-05,
      "loss": 1.0177,
      "num_input_tokens_seen": 270080,
      "step": 162
    },
    {
      "epoch": 0.2676518883415435,
      "grad_norm": 7.930927753448486,
      "learning_rate": 4.939791904846869e-05,
      "loss": 0.3286,
      "num_input_tokens_seen": 271616,
      "step": 163
    },
    {
      "epoch": 0.26929392446633826,
      "grad_norm": 10.081080436706543,
      "learning_rate": 4.937873385763908e-05,
      "loss": 0.9038,
      "num_input_tokens_seen": 273152,
      "step": 164
    },
    {
      "epoch": 0.270935960591133,
      "grad_norm": 8.651973724365234,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 0.5608,
      "num_input_tokens_seen": 274688,
      "step": 165
    },
    {
      "epoch": 0.27257799671592775,
      "grad_norm": 5.2735371589660645,
      "learning_rate": 4.933947257182901e-05,
      "loss": 0.4796,
      "num_input_tokens_seen": 276224,
      "step": 166
    },
    {
      "epoch": 0.2742200328407225,
      "grad_norm": 3.485928773880005,
      "learning_rate": 4.931939695523492e-05,
      "loss": 0.2565,
      "num_input_tokens_seen": 278016,
      "step": 167
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 10.139420509338379,
      "learning_rate": 4.929902501446366e-05,
      "loss": 0.8295,
      "num_input_tokens_seen": 279808,
      "step": 168
    },
    {
      "epoch": 0.277504105090312,
      "grad_norm": 8.773930549621582,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 0.6103,
      "num_input_tokens_seen": 281600,
      "step": 169
    },
    {
      "epoch": 0.2791461412151067,
      "grad_norm": 8.77799129486084,
      "learning_rate": 4.925739315689991e-05,
      "loss": 0.5262,
      "num_input_tokens_seen": 283136,
      "step": 170
    },
    {
      "epoch": 0.28078817733990147,
      "grad_norm": 5.452849388122559,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 0.3223,
      "num_input_tokens_seen": 284928,
      "step": 171
    },
    {
      "epoch": 0.2824302134646962,
      "grad_norm": 7.654482364654541,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 0.6334,
      "num_input_tokens_seen": 286464,
      "step": 172
    },
    {
      "epoch": 0.28407224958949095,
      "grad_norm": 9.411149024963379,
      "learning_rate": 4.919272926204929e-05,
      "loss": 0.6493,
      "num_input_tokens_seen": 288256,
      "step": 173
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.965322017669678,
      "learning_rate": 4.917058471511149e-05,
      "loss": 0.261,
      "num_input_tokens_seen": 290048,
      "step": 174
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 8.894392967224121,
      "learning_rate": 4.914814565722671e-05,
      "loss": 0.7506,
      "num_input_tokens_seen": 291840,
      "step": 175
    },
    {
      "epoch": 0.2889983579638752,
      "grad_norm": 9.464948654174805,
      "learning_rate": 4.912541236180779e-05,
      "loss": 0.9073,
      "num_input_tokens_seen": 293632,
      "step": 176
    },
    {
      "epoch": 0.29064039408866993,
      "grad_norm": 7.189809799194336,
      "learning_rate": 4.910238510585276e-05,
      "loss": 0.7155,
      "num_input_tokens_seen": 295168,
      "step": 177
    },
    {
      "epoch": 0.2922824302134647,
      "grad_norm": 18.09961700439453,
      "learning_rate": 4.907906416994146e-05,
      "loss": 0.79,
      "num_input_tokens_seen": 296960,
      "step": 178
    },
    {
      "epoch": 0.2939244663382594,
      "grad_norm": 9.613940238952637,
      "learning_rate": 4.905544983823214e-05,
      "loss": 0.7921,
      "num_input_tokens_seen": 298496,
      "step": 179
    },
    {
      "epoch": 0.2955665024630542,
      "grad_norm": 11.098461151123047,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 0.8319,
      "num_input_tokens_seen": 300032,
      "step": 180
    },
    {
      "epoch": 0.29720853858784896,
      "grad_norm": 8.774308204650879,
      "learning_rate": 4.900734214192358e-05,
      "loss": 1.0741,
      "num_input_tokens_seen": 301568,
      "step": 181
    },
    {
      "epoch": 0.2988505747126437,
      "grad_norm": 5.736942768096924,
      "learning_rate": 4.898284936350144e-05,
      "loss": 0.5576,
      "num_input_tokens_seen": 303872,
      "step": 182
    },
    {
      "epoch": 0.30049261083743845,
      "grad_norm": 10.109865188598633,
      "learning_rate": 4.895806436162833e-05,
      "loss": 0.7486,
      "num_input_tokens_seen": 305408,
      "step": 183
    },
    {
      "epoch": 0.3021346469622332,
      "grad_norm": 8.383735656738281,
      "learning_rate": 4.893298743830168e-05,
      "loss": 0.7667,
      "num_input_tokens_seen": 306944,
      "step": 184
    },
    {
      "epoch": 0.30377668308702793,
      "grad_norm": 4.919384479522705,
      "learning_rate": 4.890761889907589e-05,
      "loss": 0.4417,
      "num_input_tokens_seen": 308480,
      "step": 185
    },
    {
      "epoch": 0.3054187192118227,
      "grad_norm": 5.116147518157959,
      "learning_rate": 4.888195905305859e-05,
      "loss": 0.3321,
      "num_input_tokens_seen": 310016,
      "step": 186
    },
    {
      "epoch": 0.3070607553366174,
      "grad_norm": 14.888818740844727,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 0.892,
      "num_input_tokens_seen": 311552,
      "step": 187
    },
    {
      "epoch": 0.30870279146141216,
      "grad_norm": 5.373311519622803,
      "learning_rate": 4.882976669482367e-05,
      "loss": 0.3146,
      "num_input_tokens_seen": 313088,
      "step": 188
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 5.25577449798584,
      "learning_rate": 4.880323481855347e-05,
      "loss": 0.6716,
      "num_input_tokens_seen": 314624,
      "step": 189
    },
    {
      "epoch": 0.31198686371100165,
      "grad_norm": 9.621116638183594,
      "learning_rate": 4.877641290737884e-05,
      "loss": 0.7534,
      "num_input_tokens_seen": 316160,
      "step": 190
    },
    {
      "epoch": 0.3136288998357964,
      "grad_norm": 7.744553089141846,
      "learning_rate": 4.874930128811631e-05,
      "loss": 0.7093,
      "num_input_tokens_seen": 317952,
      "step": 191
    },
    {
      "epoch": 0.31527093596059114,
      "grad_norm": 6.020045757293701,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 0.4968,
      "num_input_tokens_seen": 319488,
      "step": 192
    },
    {
      "epoch": 0.3169129720853859,
      "grad_norm": 6.895077228546143,
      "learning_rate": 4.869421025023965e-05,
      "loss": 0.6582,
      "num_input_tokens_seen": 321024,
      "step": 193
    },
    {
      "epoch": 0.3185550082101806,
      "grad_norm": 5.620556831359863,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 0.4301,
      "num_input_tokens_seen": 322816,
      "step": 194
    },
    {
      "epoch": 0.32019704433497537,
      "grad_norm": 3.6273419857025146,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 0.2241,
      "num_input_tokens_seen": 324352,
      "step": 195
    },
    {
      "epoch": 0.3218390804597701,
      "grad_norm": 4.411894798278809,
      "learning_rate": 4.860940925593703e-05,
      "loss": 0.2496,
      "num_input_tokens_seen": 325888,
      "step": 196
    },
    {
      "epoch": 0.32348111658456485,
      "grad_norm": 8.9426851272583,
      "learning_rate": 4.858056644869002e-05,
      "loss": 0.4444,
      "num_input_tokens_seen": 327424,
      "step": 197
    },
    {
      "epoch": 0.3251231527093596,
      "grad_norm": 9.3160982131958,
      "learning_rate": 4.855143631968242e-05,
      "loss": 0.4257,
      "num_input_tokens_seen": 328960,
      "step": 198
    },
    {
      "epoch": 0.32676518883415434,
      "grad_norm": 5.117934226989746,
      "learning_rate": 4.852201922385564e-05,
      "loss": 0.3735,
      "num_input_tokens_seen": 330496,
      "step": 199
    },
    {
      "epoch": 0.3284072249589491,
      "grad_norm": 8.116324424743652,
      "learning_rate": 4.849231551964771e-05,
      "loss": 0.6199,
      "num_input_tokens_seen": 332032,
      "step": 200
    },
    {
      "epoch": 0.33004926108374383,
      "grad_norm": 8.197232246398926,
      "learning_rate": 4.84623255689889e-05,
      "loss": 0.5121,
      "num_input_tokens_seen": 333824,
      "step": 201
    },
    {
      "epoch": 0.33169129720853857,
      "grad_norm": 8.005407333374023,
      "learning_rate": 4.843204973729729e-05,
      "loss": 0.5985,
      "num_input_tokens_seen": 335360,
      "step": 202
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 9.11996841430664,
      "learning_rate": 4.840148839347434e-05,
      "loss": 0.6579,
      "num_input_tokens_seen": 336896,
      "step": 203
    },
    {
      "epoch": 0.33497536945812806,
      "grad_norm": 9.134719848632812,
      "learning_rate": 4.837064190990036e-05,
      "loss": 1.048,
      "num_input_tokens_seen": 338432,
      "step": 204
    },
    {
      "epoch": 0.3366174055829228,
      "grad_norm": 7.3028154373168945,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 0.4028,
      "num_input_tokens_seen": 340224,
      "step": 205
    },
    {
      "epoch": 0.33825944170771755,
      "grad_norm": 8.062519073486328,
      "learning_rate": 4.830809503038781e-05,
      "loss": 0.3449,
      "num_input_tokens_seen": 342016,
      "step": 206
    },
    {
      "epoch": 0.3399014778325123,
      "grad_norm": 5.896022796630859,
      "learning_rate": 4.827639539656321e-05,
      "loss": 0.217,
      "num_input_tokens_seen": 343552,
      "step": 207
    },
    {
      "epoch": 0.3415435139573071,
      "grad_norm": 14.784514427185059,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 1.2352,
      "num_input_tokens_seen": 345088,
      "step": 208
    },
    {
      "epoch": 0.34318555008210183,
      "grad_norm": 5.363550186157227,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 0.2556,
      "num_input_tokens_seen": 346624,
      "step": 209
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 5.362055778503418,
      "learning_rate": 4.817959636416969e-05,
      "loss": 0.2763,
      "num_input_tokens_seen": 348416,
      "step": 210
    },
    {
      "epoch": 0.3464696223316913,
      "grad_norm": 7.040210247039795,
      "learning_rate": 4.814676462024988e-05,
      "loss": 0.6715,
      "num_input_tokens_seen": 349952,
      "step": 211
    },
    {
      "epoch": 0.34811165845648606,
      "grad_norm": 8.830702781677246,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 0.6328,
      "num_input_tokens_seen": 351488,
      "step": 212
    },
    {
      "epoch": 0.3497536945812808,
      "grad_norm": 7.476521968841553,
      "learning_rate": 4.808025542782453e-05,
      "loss": 0.4954,
      "num_input_tokens_seen": 353280,
      "step": 213
    },
    {
      "epoch": 0.35139573070607555,
      "grad_norm": 11.548465728759766,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 1.1447,
      "num_input_tokens_seen": 354816,
      "step": 214
    },
    {
      "epoch": 0.3530377668308703,
      "grad_norm": 8.969029426574707,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 0.4592,
      "num_input_tokens_seen": 356352,
      "step": 215
    },
    {
      "epoch": 0.35467980295566504,
      "grad_norm": 11.590046882629395,
      "learning_rate": 4.797838348138086e-05,
      "loss": 0.7061,
      "num_input_tokens_seen": 357888,
      "step": 216
    },
    {
      "epoch": 0.3563218390804598,
      "grad_norm": 12.351375579833984,
      "learning_rate": 4.794386564209953e-05,
      "loss": 0.4495,
      "num_input_tokens_seen": 359680,
      "step": 217
    },
    {
      "epoch": 0.3579638752052545,
      "grad_norm": 6.207775115966797,
      "learning_rate": 4.790906823905599e-05,
      "loss": 0.4519,
      "num_input_tokens_seen": 361216,
      "step": 218
    },
    {
      "epoch": 0.35960591133004927,
      "grad_norm": 12.254194259643555,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 0.5821,
      "num_input_tokens_seen": 362752,
      "step": 219
    },
    {
      "epoch": 0.361247947454844,
      "grad_norm": 6.665447235107422,
      "learning_rate": 4.783863644106502e-05,
      "loss": 0.27,
      "num_input_tokens_seen": 364544,
      "step": 220
    },
    {
      "epoch": 0.36288998357963875,
      "grad_norm": 6.792314529418945,
      "learning_rate": 4.780300290430682e-05,
      "loss": 0.4157,
      "num_input_tokens_seen": 366080,
      "step": 221
    },
    {
      "epoch": 0.3645320197044335,
      "grad_norm": 7.3075642585754395,
      "learning_rate": 4.776709152015443e-05,
      "loss": 0.5384,
      "num_input_tokens_seen": 367872,
      "step": 222
    },
    {
      "epoch": 0.36617405582922824,
      "grad_norm": 9.783679962158203,
      "learning_rate": 4.773090272617672e-05,
      "loss": 0.9993,
      "num_input_tokens_seen": 369664,
      "step": 223
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 8.831459999084473,
      "learning_rate": 4.769443696332272e-05,
      "loss": 0.6653,
      "num_input_tokens_seen": 371200,
      "step": 224
    },
    {
      "epoch": 0.3694581280788177,
      "grad_norm": 4.797425746917725,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.1891,
      "num_input_tokens_seen": 372736,
      "step": 225
    },
    {
      "epoch": 0.37110016420361247,
      "grad_norm": 5.235203742980957,
      "learning_rate": 4.762067631165049e-05,
      "loss": 0.291,
      "num_input_tokens_seen": 374272,
      "step": 226
    },
    {
      "epoch": 0.3727422003284072,
      "grad_norm": 6.089522838592529,
      "learning_rate": 4.758338232158252e-05,
      "loss": 0.3326,
      "num_input_tokens_seen": 375808,
      "step": 227
    },
    {
      "epoch": 0.37438423645320196,
      "grad_norm": 11.329794883728027,
      "learning_rate": 4.754581316012785e-05,
      "loss": 0.9279,
      "num_input_tokens_seen": 377344,
      "step": 228
    },
    {
      "epoch": 0.3760262725779967,
      "grad_norm": 10.32996940612793,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 0.8834,
      "num_input_tokens_seen": 379136,
      "step": 229
    },
    {
      "epoch": 0.37766830870279144,
      "grad_norm": 5.560516834259033,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 0.313,
      "num_input_tokens_seen": 380928,
      "step": 230
    },
    {
      "epoch": 0.3793103448275862,
      "grad_norm": 10.690991401672363,
      "learning_rate": 4.743145924185821e-05,
      "loss": 0.9392,
      "num_input_tokens_seen": 382464,
      "step": 231
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 11.450631141662598,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 0.9084,
      "num_input_tokens_seen": 384256,
      "step": 232
    },
    {
      "epoch": 0.3825944170771757,
      "grad_norm": 5.470330238342285,
      "learning_rate": 4.73538559209842e-05,
      "loss": 0.4848,
      "num_input_tokens_seen": 385792,
      "step": 233
    },
    {
      "epoch": 0.3842364532019704,
      "grad_norm": 11.82645320892334,
      "learning_rate": 4.731464546130314e-05,
      "loss": 1.144,
      "num_input_tokens_seen": 387328,
      "step": 234
    },
    {
      "epoch": 0.38587848932676516,
      "grad_norm": 5.429906845092773,
      "learning_rate": 4.72751631047092e-05,
      "loss": 0.724,
      "num_input_tokens_seen": 388864,
      "step": 235
    },
    {
      "epoch": 0.38752052545155996,
      "grad_norm": 10.220016479492188,
      "learning_rate": 4.723540933228244e-05,
      "loss": 0.8372,
      "num_input_tokens_seen": 390400,
      "step": 236
    },
    {
      "epoch": 0.3891625615763547,
      "grad_norm": 5.846182346343994,
      "learning_rate": 4.719538462841003e-05,
      "loss": 0.288,
      "num_input_tokens_seen": 391936,
      "step": 237
    },
    {
      "epoch": 0.39080459770114945,
      "grad_norm": 8.17985725402832,
      "learning_rate": 4.715508948078037e-05,
      "loss": 0.8271,
      "num_input_tokens_seen": 393472,
      "step": 238
    },
    {
      "epoch": 0.3924466338259442,
      "grad_norm": 5.207830905914307,
      "learning_rate": 4.71145243803771e-05,
      "loss": 0.3338,
      "num_input_tokens_seen": 395008,
      "step": 239
    },
    {
      "epoch": 0.39408866995073893,
      "grad_norm": 5.8644022941589355,
      "learning_rate": 4.707368982147318e-05,
      "loss": 0.5939,
      "num_input_tokens_seen": 396544,
      "step": 240
    },
    {
      "epoch": 0.3957307060755337,
      "grad_norm": 5.182755947113037,
      "learning_rate": 4.70325863016248e-05,
      "loss": 0.322,
      "num_input_tokens_seen": 398080,
      "step": 241
    },
    {
      "epoch": 0.3973727422003284,
      "grad_norm": 10.94226360321045,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 0.9732,
      "num_input_tokens_seen": 399616,
      "step": 242
    },
    {
      "epoch": 0.39901477832512317,
      "grad_norm": 4.124612808227539,
      "learning_rate": 4.694957438569951e-05,
      "loss": 0.1592,
      "num_input_tokens_seen": 401152,
      "step": 243
    },
    {
      "epoch": 0.4006568144499179,
      "grad_norm": 7.094418525695801,
      "learning_rate": 4.690766700109659e-05,
      "loss": 0.662,
      "num_input_tokens_seen": 402688,
      "step": 244
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 9.423198699951172,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 0.6079,
      "num_input_tokens_seen": 404224,
      "step": 245
    },
    {
      "epoch": 0.4039408866995074,
      "grad_norm": 7.481207847595215,
      "learning_rate": 4.682305193174524e-05,
      "loss": 0.8115,
      "num_input_tokens_seen": 406016,
      "step": 246
    },
    {
      "epoch": 0.40558292282430214,
      "grad_norm": 7.793207168579102,
      "learning_rate": 4.678034527800474e-05,
      "loss": 0.5965,
      "num_input_tokens_seen": 407552,
      "step": 247
    },
    {
      "epoch": 0.4072249589490969,
      "grad_norm": 6.054611682891846,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 0.5136,
      "num_input_tokens_seen": 409344,
      "step": 248
    },
    {
      "epoch": 0.4088669950738916,
      "grad_norm": 6.066230773925781,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.5586,
      "num_input_tokens_seen": 411136,
      "step": 249
    },
    {
      "epoch": 0.41050903119868637,
      "grad_norm": 5.9701666831970215,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.3964,
      "num_input_tokens_seen": 412672,
      "step": 250
    },
    {
      "epoch": 0.4121510673234811,
      "grad_norm": 9.263823509216309,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 0.5426,
      "num_input_tokens_seen": 414208,
      "step": 251
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 5.974584579467773,
      "learning_rate": 4.656284173018144e-05,
      "loss": 0.5369,
      "num_input_tokens_seen": 416000,
      "step": 252
    },
    {
      "epoch": 0.4154351395730706,
      "grad_norm": 8.87160587310791,
      "learning_rate": 4.65185506750986e-05,
      "loss": 0.8629,
      "num_input_tokens_seen": 417792,
      "step": 253
    },
    {
      "epoch": 0.41707717569786534,
      "grad_norm": 10.169363975524902,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 1.0637,
      "num_input_tokens_seen": 419584,
      "step": 254
    },
    {
      "epoch": 0.4187192118226601,
      "grad_norm": 7.296846866607666,
      "learning_rate": 4.642918251755281e-05,
      "loss": 0.7353,
      "num_input_tokens_seen": 421376,
      "step": 255
    },
    {
      "epoch": 0.42036124794745483,
      "grad_norm": 7.573177337646484,
      "learning_rate": 4.638410650401267e-05,
      "loss": 0.3533,
      "num_input_tokens_seen": 422912,
      "step": 256
    },
    {
      "epoch": 0.4220032840722496,
      "grad_norm": 8.316699028015137,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 0.4441,
      "num_input_tokens_seen": 424448,
      "step": 257
    },
    {
      "epoch": 0.4236453201970443,
      "grad_norm": 4.946277141571045,
      "learning_rate": 4.629317335357619e-05,
      "loss": 0.4446,
      "num_input_tokens_seen": 425984,
      "step": 258
    },
    {
      "epoch": 0.42528735632183906,
      "grad_norm": 6.533777713775635,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 0.3939,
      "num_input_tokens_seen": 427776,
      "step": 259
    },
    {
      "epoch": 0.4269293924466338,
      "grad_norm": 8.929642677307129,
      "learning_rate": 4.620120240391065e-05,
      "loss": 0.5735,
      "num_input_tokens_seen": 429568,
      "step": 260
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 8.184842109680176,
      "learning_rate": 4.615482915318911e-05,
      "loss": 0.6729,
      "num_input_tokens_seen": 431360,
      "step": 261
    },
    {
      "epoch": 0.4302134646962233,
      "grad_norm": 7.9617600440979,
      "learning_rate": 4.610819813755038e-05,
      "loss": 0.6501,
      "num_input_tokens_seen": 433152,
      "step": 262
    },
    {
      "epoch": 0.4318555008210181,
      "grad_norm": 5.435404300689697,
      "learning_rate": 4.606130992517869e-05,
      "loss": 0.4912,
      "num_input_tokens_seen": 434688,
      "step": 263
    },
    {
      "epoch": 0.43349753694581283,
      "grad_norm": 6.357053279876709,
      "learning_rate": 4.601416508739211e-05,
      "loss": 0.327,
      "num_input_tokens_seen": 436480,
      "step": 264
    },
    {
      "epoch": 0.4351395730706076,
      "grad_norm": 10.981689453125,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 0.8558,
      "num_input_tokens_seen": 438016,
      "step": 265
    },
    {
      "epoch": 0.4367816091954023,
      "grad_norm": 7.729053020477295,
      "learning_rate": 4.591910783647404e-05,
      "loss": 0.4291,
      "num_input_tokens_seen": 439808,
      "step": 266
    },
    {
      "epoch": 0.43842364532019706,
      "grad_norm": 11.338898658752441,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 0.8255,
      "num_input_tokens_seen": 441600,
      "step": 267
    },
    {
      "epoch": 0.4400656814449918,
      "grad_norm": 5.7034592628479,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 0.4626,
      "num_input_tokens_seen": 443392,
      "step": 268
    },
    {
      "epoch": 0.44170771756978655,
      "grad_norm": 9.174738883972168,
      "learning_rate": 4.577461173185821e-05,
      "loss": 0.6928,
      "num_input_tokens_seen": 444928,
      "step": 269
    },
    {
      "epoch": 0.4433497536945813,
      "grad_norm": 6.874179363250732,
      "learning_rate": 4.572593931387604e-05,
      "loss": 0.6744,
      "num_input_tokens_seen": 446976,
      "step": 270
    },
    {
      "epoch": 0.44499178981937604,
      "grad_norm": 11.046128273010254,
      "learning_rate": 4.567701435686404e-05,
      "loss": 1.0287,
      "num_input_tokens_seen": 448512,
      "step": 271
    },
    {
      "epoch": 0.4466338259441708,
      "grad_norm": 7.224323749542236,
      "learning_rate": 4.562783745695738e-05,
      "loss": 0.3943,
      "num_input_tokens_seen": 450048,
      "step": 272
    },
    {
      "epoch": 0.4482758620689655,
      "grad_norm": 7.4045305252075195,
      "learning_rate": 4.557840921336105e-05,
      "loss": 0.5188,
      "num_input_tokens_seen": 451584,
      "step": 273
    },
    {
      "epoch": 0.44991789819376027,
      "grad_norm": 10.621689796447754,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 0.5761,
      "num_input_tokens_seen": 453120,
      "step": 274
    },
    {
      "epoch": 0.451559934318555,
      "grad_norm": 6.444818496704102,
      "learning_rate": 4.54788011072248e-05,
      "loss": 0.6234,
      "num_input_tokens_seen": 454656,
      "step": 275
    },
    {
      "epoch": 0.45320197044334976,
      "grad_norm": 8.07247543334961,
      "learning_rate": 4.542862245837821e-05,
      "loss": 0.6851,
      "num_input_tokens_seen": 456448,
      "step": 276
    },
    {
      "epoch": 0.4548440065681445,
      "grad_norm": 5.066216468811035,
      "learning_rate": 4.537819489321386e-05,
      "loss": 0.3594,
      "num_input_tokens_seen": 457984,
      "step": 277
    },
    {
      "epoch": 0.45648604269293924,
      "grad_norm": 6.92727518081665,
      "learning_rate": 4.532751902617569e-05,
      "loss": 0.5227,
      "num_input_tokens_seen": 459520,
      "step": 278
    },
    {
      "epoch": 0.458128078817734,
      "grad_norm": 7.529803276062012,
      "learning_rate": 4.527659547473317e-05,
      "loss": 0.6923,
      "num_input_tokens_seen": 461056,
      "step": 279
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 7.730706214904785,
      "learning_rate": 4.522542485937369e-05,
      "loss": 0.4724,
      "num_input_tokens_seen": 462848,
      "step": 280
    },
    {
      "epoch": 0.4614121510673235,
      "grad_norm": 5.035782337188721,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 0.25,
      "num_input_tokens_seen": 464384,
      "step": 281
    },
    {
      "epoch": 0.4630541871921182,
      "grad_norm": 11.213071823120117,
      "learning_rate": 4.512234493389785e-05,
      "loss": 0.4161,
      "num_input_tokens_seen": 466176,
      "step": 282
    },
    {
      "epoch": 0.46469622331691296,
      "grad_norm": 6.815774440765381,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 0.687,
      "num_input_tokens_seen": 467712,
      "step": 283
    },
    {
      "epoch": 0.4663382594417077,
      "grad_norm": 10.597060203552246,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 0.7619,
      "num_input_tokens_seen": 469248,
      "step": 284
    },
    {
      "epoch": 0.46798029556650245,
      "grad_norm": 9.268364906311035,
      "learning_rate": 4.496588775118232e-05,
      "loss": 0.4138,
      "num_input_tokens_seen": 470784,
      "step": 285
    },
    {
      "epoch": 0.4696223316912972,
      "grad_norm": 8.141228675842285,
      "learning_rate": 4.491324795060491e-05,
      "loss": 0.3137,
      "num_input_tokens_seen": 472320,
      "step": 286
    },
    {
      "epoch": 0.47126436781609193,
      "grad_norm": 12.250035285949707,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.7594,
      "num_input_tokens_seen": 473856,
      "step": 287
    },
    {
      "epoch": 0.4729064039408867,
      "grad_norm": 6.662179470062256,
      "learning_rate": 4.480724108387977e-05,
      "loss": 0.5193,
      "num_input_tokens_seen": 475648,
      "step": 288
    },
    {
      "epoch": 0.4745484400656814,
      "grad_norm": 4.160407066345215,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 0.1367,
      "num_input_tokens_seen": 477184,
      "step": 289
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 8.444494247436523,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 0.8683,
      "num_input_tokens_seen": 479232,
      "step": 290
    },
    {
      "epoch": 0.47783251231527096,
      "grad_norm": 5.151856899261475,
      "learning_rate": 4.464642232938505e-05,
      "loss": 0.2112,
      "num_input_tokens_seen": 480768,
      "step": 291
    },
    {
      "epoch": 0.4794745484400657,
      "grad_norm": 9.496416091918945,
      "learning_rate": 4.4592336433146e-05,
      "loss": 0.7098,
      "num_input_tokens_seen": 482304,
      "step": 292
    },
    {
      "epoch": 0.48111658456486045,
      "grad_norm": 6.5953264236450195,
      "learning_rate": 4.453801181047047e-05,
      "loss": 0.3947,
      "num_input_tokens_seen": 484096,
      "step": 293
    },
    {
      "epoch": 0.4827586206896552,
      "grad_norm": 7.282223224639893,
      "learning_rate": 4.448344912328686e-05,
      "loss": 0.4537,
      "num_input_tokens_seen": 485632,
      "step": 294
    },
    {
      "epoch": 0.48440065681444994,
      "grad_norm": 11.287287712097168,
      "learning_rate": 4.442864903642428e-05,
      "loss": 0.6478,
      "num_input_tokens_seen": 487168,
      "step": 295
    },
    {
      "epoch": 0.4860426929392447,
      "grad_norm": 9.074029922485352,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 0.7643,
      "num_input_tokens_seen": 488960,
      "step": 296
    },
    {
      "epoch": 0.4876847290640394,
      "grad_norm": 10.99294662475586,
      "learning_rate": 4.431833933743378e-05,
      "loss": 0.8154,
      "num_input_tokens_seen": 490496,
      "step": 297
    },
    {
      "epoch": 0.48932676518883417,
      "grad_norm": 11.005977630615234,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.9316,
      "num_input_tokens_seen": 492288,
      "step": 298
    },
    {
      "epoch": 0.4909688013136289,
      "grad_norm": 16.365930557250977,
      "learning_rate": 4.420708808983809e-05,
      "loss": 1.1274,
      "num_input_tokens_seen": 494080,
      "step": 299
    },
    {
      "epoch": 0.49261083743842365,
      "grad_norm": 11.526331901550293,
      "learning_rate": 4.415111107797445e-05,
      "loss": 0.5565,
      "num_input_tokens_seen": 495616,
      "step": 300
    },
    {
      "epoch": 0.4942528735632184,
      "grad_norm": 6.1497111320495605,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.2549,
      "num_input_tokens_seen": 497152,
      "step": 301
    },
    {
      "epoch": 0.49589490968801314,
      "grad_norm": 8.304544448852539,
      "learning_rate": 4.403845768841842e-05,
      "loss": 0.6928,
      "num_input_tokens_seen": 498688,
      "step": 302
    },
    {
      "epoch": 0.4975369458128079,
      "grad_norm": 7.461394786834717,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 0.5733,
      "num_input_tokens_seen": 500224,
      "step": 303
    },
    {
      "epoch": 0.49917898193760263,
      "grad_norm": 7.708468437194824,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 0.5663,
      "num_input_tokens_seen": 501760,
      "step": 304
    },
    {
      "epoch": 0.5008210180623974,
      "grad_norm": 15.420251846313477,
      "learning_rate": 4.386773950556931e-05,
      "loss": 0.8308,
      "num_input_tokens_seen": 503296,
      "step": 305
    },
    {
      "epoch": 0.5024630541871922,
      "grad_norm": 11.832877159118652,
      "learning_rate": 4.381037272239311e-05,
      "loss": 0.8169,
      "num_input_tokens_seen": 504832,
      "step": 306
    },
    {
      "epoch": 0.5041050903119869,
      "grad_norm": 15.054519653320312,
      "learning_rate": 4.375277674076149e-05,
      "loss": 1.2923,
      "num_input_tokens_seen": 506368,
      "step": 307
    },
    {
      "epoch": 0.5057471264367817,
      "grad_norm": 5.475646018981934,
      "learning_rate": 4.36949522624633e-05,
      "loss": 0.374,
      "num_input_tokens_seen": 507904,
      "step": 308
    },
    {
      "epoch": 0.5073891625615764,
      "grad_norm": 12.47098159790039,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.9973,
      "num_input_tokens_seen": 509440,
      "step": 309
    },
    {
      "epoch": 0.5090311986863711,
      "grad_norm": 9.229969024658203,
      "learning_rate": 4.357862063693486e-05,
      "loss": 0.5657,
      "num_input_tokens_seen": 510976,
      "step": 310
    },
    {
      "epoch": 0.5106732348111659,
      "grad_norm": 7.348416328430176,
      "learning_rate": 4.352011490716875e-05,
      "loss": 0.8103,
      "num_input_tokens_seen": 513024,
      "step": 311
    },
    {
      "epoch": 0.5123152709359606,
      "grad_norm": 8.14151668548584,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 0.519,
      "num_input_tokens_seen": 514816,
      "step": 312
    },
    {
      "epoch": 0.5139573070607554,
      "grad_norm": 14.39990234375,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 0.8189,
      "num_input_tokens_seen": 516352,
      "step": 313
    },
    {
      "epoch": 0.5155993431855501,
      "grad_norm": 8.64537525177002,
      "learning_rate": 4.334324661257191e-05,
      "loss": 0.9661,
      "num_input_tokens_seen": 517888,
      "step": 314
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 9.834150314331055,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 0.7851,
      "num_input_tokens_seen": 519424,
      "step": 315
    },
    {
      "epoch": 0.5188834154351396,
      "grad_norm": 5.117928981781006,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 0.2109,
      "num_input_tokens_seen": 520960,
      "step": 316
    },
    {
      "epoch": 0.5205254515599343,
      "grad_norm": 7.914673328399658,
      "learning_rate": 4.31643667742744e-05,
      "loss": 0.5944,
      "num_input_tokens_seen": 522496,
      "step": 317
    },
    {
      "epoch": 0.5221674876847291,
      "grad_norm": 8.603588104248047,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.7546,
      "num_input_tokens_seen": 524032,
      "step": 318
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 6.9124603271484375,
      "learning_rate": 4.304400570245906e-05,
      "loss": 0.5072,
      "num_input_tokens_seen": 525824,
      "step": 319
    },
    {
      "epoch": 0.5254515599343186,
      "grad_norm": 7.453680515289307,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 0.6108,
      "num_input_tokens_seen": 527872,
      "step": 320
    },
    {
      "epoch": 0.5270935960591133,
      "grad_norm": 5.711050987243652,
      "learning_rate": 4.292276519126207e-05,
      "loss": 0.3856,
      "num_input_tokens_seen": 529408,
      "step": 321
    },
    {
      "epoch": 0.5287356321839081,
      "grad_norm": 5.571980953216553,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 0.5975,
      "num_input_tokens_seen": 530944,
      "step": 322
    },
    {
      "epoch": 0.5303776683087028,
      "grad_norm": 8.337955474853516,
      "learning_rate": 4.280065114977492e-05,
      "loss": 0.6949,
      "num_input_tokens_seen": 532736,
      "step": 323
    },
    {
      "epoch": 0.5320197044334976,
      "grad_norm": 5.555288791656494,
      "learning_rate": 4.273926841341302e-05,
      "loss": 0.538,
      "num_input_tokens_seen": 534272,
      "step": 324
    },
    {
      "epoch": 0.5336617405582923,
      "grad_norm": 9.518166542053223,
      "learning_rate": 4.267766952966369e-05,
      "loss": 0.6468,
      "num_input_tokens_seen": 535808,
      "step": 325
    },
    {
      "epoch": 0.535303776683087,
      "grad_norm": 5.918449401855469,
      "learning_rate": 4.261585524908987e-05,
      "loss": 0.4285,
      "num_input_tokens_seen": 537344,
      "step": 326
    },
    {
      "epoch": 0.5369458128078818,
      "grad_norm": 6.141743183135986,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 0.7337,
      "num_input_tokens_seen": 539136,
      "step": 327
    },
    {
      "epoch": 0.5385878489326765,
      "grad_norm": 7.562007427215576,
      "learning_rate": 4.249158351283414e-05,
      "loss": 0.6874,
      "num_input_tokens_seen": 540672,
      "step": 328
    },
    {
      "epoch": 0.5402298850574713,
      "grad_norm": 11.945138931274414,
      "learning_rate": 4.242912757136412e-05,
      "loss": 0.9272,
      "num_input_tokens_seen": 542464,
      "step": 329
    },
    {
      "epoch": 0.541871921182266,
      "grad_norm": 6.224806785583496,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 0.3413,
      "num_input_tokens_seen": 544256,
      "step": 330
    },
    {
      "epoch": 0.5435139573070608,
      "grad_norm": 8.209141731262207,
      "learning_rate": 4.230357934676017e-05,
      "loss": 0.4432,
      "num_input_tokens_seen": 546048,
      "step": 331
    },
    {
      "epoch": 0.5451559934318555,
      "grad_norm": 8.523113250732422,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.6503,
      "num_input_tokens_seen": 547584,
      "step": 332
    },
    {
      "epoch": 0.5467980295566502,
      "grad_norm": 10.2213773727417,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.8835,
      "num_input_tokens_seen": 549120,
      "step": 333
    },
    {
      "epoch": 0.548440065681445,
      "grad_norm": 11.220792770385742,
      "learning_rate": 4.211367764821722e-05,
      "loss": 0.5031,
      "num_input_tokens_seen": 550656,
      "step": 334
    },
    {
      "epoch": 0.5500821018062397,
      "grad_norm": 5.758815288543701,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 0.3986,
      "num_input_tokens_seen": 552192,
      "step": 335
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 10.222811698913574,
      "learning_rate": 4.198603260653792e-05,
      "loss": 0.5924,
      "num_input_tokens_seen": 553984,
      "step": 336
    },
    {
      "epoch": 0.5533661740558292,
      "grad_norm": 7.638319492340088,
      "learning_rate": 4.192189924206652e-05,
      "loss": 0.5436,
      "num_input_tokens_seen": 555776,
      "step": 337
    },
    {
      "epoch": 0.555008210180624,
      "grad_norm": 7.641714096069336,
      "learning_rate": 4.185755968959308e-05,
      "loss": 0.5593,
      "num_input_tokens_seen": 557568,
      "step": 338
    },
    {
      "epoch": 0.5566502463054187,
      "grad_norm": 7.554540157318115,
      "learning_rate": 4.179301473307476e-05,
      "loss": 0.5042,
      "num_input_tokens_seen": 559360,
      "step": 339
    },
    {
      "epoch": 0.5582922824302134,
      "grad_norm": 4.247395992279053,
      "learning_rate": 4.172826515897146e-05,
      "loss": 0.2187,
      "num_input_tokens_seen": 560896,
      "step": 340
    },
    {
      "epoch": 0.5599343185550082,
      "grad_norm": 7.6747331619262695,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.407,
      "num_input_tokens_seen": 562688,
      "step": 341
    },
    {
      "epoch": 0.5615763546798029,
      "grad_norm": 6.3686113357543945,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 0.2772,
      "num_input_tokens_seen": 564224,
      "step": 342
    },
    {
      "epoch": 0.5632183908045977,
      "grad_norm": 7.340240001678467,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 0.3034,
      "num_input_tokens_seen": 565760,
      "step": 343
    },
    {
      "epoch": 0.5648604269293924,
      "grad_norm": 7.050150394439697,
      "learning_rate": 4.146723650296701e-05,
      "loss": 0.3666,
      "num_input_tokens_seen": 567552,
      "step": 344
    },
    {
      "epoch": 0.5665024630541872,
      "grad_norm": 5.664145469665527,
      "learning_rate": 4.140147572476268e-05,
      "loss": 0.1913,
      "num_input_tokens_seen": 569088,
      "step": 345
    },
    {
      "epoch": 0.5681444991789819,
      "grad_norm": 12.010294914245605,
      "learning_rate": 4.133551509975264e-05,
      "loss": 0.7046,
      "num_input_tokens_seen": 570880,
      "step": 346
    },
    {
      "epoch": 0.5697865353037767,
      "grad_norm": 17.65962791442871,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 1.0985,
      "num_input_tokens_seen": 572928,
      "step": 347
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 6.02955436706543,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 0.2277,
      "num_input_tokens_seen": 574720,
      "step": 348
    },
    {
      "epoch": 0.5730706075533661,
      "grad_norm": 9.550541877746582,
      "learning_rate": 4.113644219309877e-05,
      "loss": 0.5301,
      "num_input_tokens_seen": 576512,
      "step": 349
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 12.213729858398438,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 0.7352,
      "num_input_tokens_seen": 578304,
      "step": 350
    },
    {
      "epoch": 0.5763546798029556,
      "grad_norm": 13.424047470092773,
      "learning_rate": 4.100274248712389e-05,
      "loss": 0.726,
      "num_input_tokens_seen": 580096,
      "step": 351
    },
    {
      "epoch": 0.5779967159277504,
      "grad_norm": 11.189955711364746,
      "learning_rate": 4.093559974371725e-05,
      "loss": 0.2986,
      "num_input_tokens_seen": 581888,
      "step": 352
    },
    {
      "epoch": 0.5796387520525451,
      "grad_norm": 10.021078109741211,
      "learning_rate": 4.086826283005669e-05,
      "loss": 0.4814,
      "num_input_tokens_seen": 583680,
      "step": 353
    },
    {
      "epoch": 0.5812807881773399,
      "grad_norm": 15.267791748046875,
      "learning_rate": 4.080073256662127e-05,
      "loss": 1.0386,
      "num_input_tokens_seen": 585216,
      "step": 354
    },
    {
      "epoch": 0.5829228243021346,
      "grad_norm": 10.135416984558105,
      "learning_rate": 4.073300977624594e-05,
      "loss": 0.5196,
      "num_input_tokens_seen": 586752,
      "step": 355
    },
    {
      "epoch": 0.5845648604269293,
      "grad_norm": 15.590773582458496,
      "learning_rate": 4.066509528411152e-05,
      "loss": 0.9003,
      "num_input_tokens_seen": 588544,
      "step": 356
    },
    {
      "epoch": 0.5862068965517241,
      "grad_norm": 12.373132705688477,
      "learning_rate": 4.059698991773466e-05,
      "loss": 0.684,
      "num_input_tokens_seen": 590080,
      "step": 357
    },
    {
      "epoch": 0.5878489326765188,
      "grad_norm": 4.592932224273682,
      "learning_rate": 4.052869450695776e-05,
      "loss": 0.1687,
      "num_input_tokens_seen": 591616,
      "step": 358
    },
    {
      "epoch": 0.5894909688013136,
      "grad_norm": 7.813549518585205,
      "learning_rate": 4.046020988393885e-05,
      "loss": 0.6817,
      "num_input_tokens_seen": 593408,
      "step": 359
    },
    {
      "epoch": 0.5911330049261084,
      "grad_norm": 8.121283531188965,
      "learning_rate": 4.039153688314145e-05,
      "loss": 0.3926,
      "num_input_tokens_seen": 595200,
      "step": 360
    },
    {
      "epoch": 0.5927750410509032,
      "grad_norm": 7.027106285095215,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 0.3217,
      "num_input_tokens_seen": 596736,
      "step": 361
    },
    {
      "epoch": 0.5944170771756979,
      "grad_norm": 10.528098106384277,
      "learning_rate": 4.02536290975317e-05,
      "loss": 0.7804,
      "num_input_tokens_seen": 598272,
      "step": 362
    },
    {
      "epoch": 0.5960591133004927,
      "grad_norm": 14.893736839294434,
      "learning_rate": 4.018439599308217e-05,
      "loss": 1.0873,
      "num_input_tokens_seen": 599808,
      "step": 363
    },
    {
      "epoch": 0.5977011494252874,
      "grad_norm": 4.096742153167725,
      "learning_rate": 4.011497787155938e-05,
      "loss": 0.1826,
      "num_input_tokens_seen": 601344,
      "step": 364
    },
    {
      "epoch": 0.5993431855500821,
      "grad_norm": 10.939790725708008,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 0.5777,
      "num_input_tokens_seen": 602880,
      "step": 365
    },
    {
      "epoch": 0.6009852216748769,
      "grad_norm": 7.159605026245117,
      "learning_rate": 3.997558996288965e-05,
      "loss": 0.5238,
      "num_input_tokens_seen": 604928,
      "step": 366
    },
    {
      "epoch": 0.6026272577996716,
      "grad_norm": 8.995986938476562,
      "learning_rate": 3.99056218741404e-05,
      "loss": 0.3818,
      "num_input_tokens_seen": 606464,
      "step": 367
    },
    {
      "epoch": 0.6042692939244664,
      "grad_norm": 8.514795303344727,
      "learning_rate": 3.983547216509254e-05,
      "loss": 0.6315,
      "num_input_tokens_seen": 608000,
      "step": 368
    },
    {
      "epoch": 0.6059113300492611,
      "grad_norm": 8.627570152282715,
      "learning_rate": 3.976514169049814e-05,
      "loss": 0.5161,
      "num_input_tokens_seen": 609536,
      "step": 369
    },
    {
      "epoch": 0.6075533661740559,
      "grad_norm": 6.815672397613525,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.5134,
      "num_input_tokens_seen": 611072,
      "step": 370
    },
    {
      "epoch": 0.6091954022988506,
      "grad_norm": 9.537765502929688,
      "learning_rate": 3.962394187468039e-05,
      "loss": 0.8909,
      "num_input_tokens_seen": 612864,
      "step": 371
    },
    {
      "epoch": 0.6108374384236454,
      "grad_norm": 9.953758239746094,
      "learning_rate": 3.955307425393224e-05,
      "loss": 0.4538,
      "num_input_tokens_seen": 614400,
      "step": 372
    },
    {
      "epoch": 0.6124794745484401,
      "grad_norm": 9.1524076461792,
      "learning_rate": 3.948202930856697e-05,
      "loss": 0.7691,
      "num_input_tokens_seen": 615936,
      "step": 373
    },
    {
      "epoch": 0.6141215106732348,
      "grad_norm": 6.4512152671813965,
      "learning_rate": 3.941080790424484e-05,
      "loss": 0.3851,
      "num_input_tokens_seen": 617472,
      "step": 374
    },
    {
      "epoch": 0.6157635467980296,
      "grad_norm": 5.048415184020996,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.2716,
      "num_input_tokens_seen": 619008,
      "step": 375
    },
    {
      "epoch": 0.6174055829228243,
      "grad_norm": 7.152865886688232,
      "learning_rate": 3.92678391921108e-05,
      "loss": 0.4495,
      "num_input_tokens_seen": 620544,
      "step": 376
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 6.7574615478515625,
      "learning_rate": 3.919609362632753e-05,
      "loss": 0.3657,
      "num_input_tokens_seen": 622336,
      "step": 377
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 8.402036666870117,
      "learning_rate": 3.912417508562345e-05,
      "loss": 0.4836,
      "num_input_tokens_seen": 624128,
      "step": 378
    },
    {
      "epoch": 0.6223316912972086,
      "grad_norm": 13.503610610961914,
      "learning_rate": 3.905208444630327e-05,
      "loss": 1.0728,
      "num_input_tokens_seen": 625920,
      "step": 379
    },
    {
      "epoch": 0.6239737274220033,
      "grad_norm": 5.721933364868164,
      "learning_rate": 3.897982258676867e-05,
      "loss": 0.2705,
      "num_input_tokens_seen": 627456,
      "step": 380
    },
    {
      "epoch": 0.625615763546798,
      "grad_norm": 8.535919189453125,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 0.4883,
      "num_input_tokens_seen": 628992,
      "step": 381
    },
    {
      "epoch": 0.6272577996715928,
      "grad_norm": 12.788334846496582,
      "learning_rate": 3.883478873108361e-05,
      "loss": 0.3417,
      "num_input_tokens_seen": 630784,
      "step": 382
    },
    {
      "epoch": 0.6288998357963875,
      "grad_norm": 10.150952339172363,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 0.7751,
      "num_input_tokens_seen": 632320,
      "step": 383
    },
    {
      "epoch": 0.6305418719211823,
      "grad_norm": 8.912384986877441,
      "learning_rate": 3.868908058731376e-05,
      "loss": 0.5947,
      "num_input_tokens_seen": 633856,
      "step": 384
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 9.618309020996094,
      "learning_rate": 3.861597587537568e-05,
      "loss": 0.6634,
      "num_input_tokens_seen": 635392,
      "step": 385
    },
    {
      "epoch": 0.6338259441707718,
      "grad_norm": 12.22801399230957,
      "learning_rate": 3.85427052570685e-05,
      "loss": 0.9599,
      "num_input_tokens_seen": 636928,
      "step": 386
    },
    {
      "epoch": 0.6354679802955665,
      "grad_norm": 10.52514934539795,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 0.8461,
      "num_input_tokens_seen": 638464,
      "step": 387
    },
    {
      "epoch": 0.6371100164203612,
      "grad_norm": 14.018866539001465,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 1.1369,
      "num_input_tokens_seen": 640512,
      "step": 388
    },
    {
      "epoch": 0.638752052545156,
      "grad_norm": 6.370203971862793,
      "learning_rate": 3.832190690176825e-05,
      "loss": 0.3934,
      "num_input_tokens_seen": 642304,
      "step": 389
    },
    {
      "epoch": 0.6403940886699507,
      "grad_norm": 10.761367797851562,
      "learning_rate": 3.824798160583012e-05,
      "loss": 0.8057,
      "num_input_tokens_seen": 643840,
      "step": 390
    },
    {
      "epoch": 0.6420361247947455,
      "grad_norm": 8.128589630126953,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 0.3477,
      "num_input_tokens_seen": 645376,
      "step": 391
    },
    {
      "epoch": 0.6436781609195402,
      "grad_norm": 9.019478797912598,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 0.6728,
      "num_input_tokens_seen": 646912,
      "step": 392
    },
    {
      "epoch": 0.645320197044335,
      "grad_norm": 9.18168830871582,
      "learning_rate": 3.802524079601442e-05,
      "loss": 0.7021,
      "num_input_tokens_seen": 648448,
      "step": 393
    },
    {
      "epoch": 0.6469622331691297,
      "grad_norm": 15.80788516998291,
      "learning_rate": 3.795067523432826e-05,
      "loss": 1.3545,
      "num_input_tokens_seen": 649984,
      "step": 394
    },
    {
      "epoch": 0.6486042692939245,
      "grad_norm": 6.332064151763916,
      "learning_rate": 3.787595187275136e-05,
      "loss": 0.5579,
      "num_input_tokens_seen": 651520,
      "step": 395
    },
    {
      "epoch": 0.6502463054187192,
      "grad_norm": 9.771950721740723,
      "learning_rate": 3.780107162176429e-05,
      "loss": 0.7129,
      "num_input_tokens_seen": 653056,
      "step": 396
    },
    {
      "epoch": 0.6518883415435139,
      "grad_norm": 10.560647964477539,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 0.7082,
      "num_input_tokens_seen": 654848,
      "step": 397
    },
    {
      "epoch": 0.6535303776683087,
      "grad_norm": 7.1063032150268555,
      "learning_rate": 3.765084410302909e-05,
      "loss": 0.5006,
      "num_input_tokens_seen": 656640,
      "step": 398
    },
    {
      "epoch": 0.6551724137931034,
      "grad_norm": 6.134965419769287,
      "learning_rate": 3.757549866575588e-05,
      "loss": 0.4768,
      "num_input_tokens_seen": 658176,
      "step": 399
    },
    {
      "epoch": 0.6568144499178982,
      "grad_norm": 8.72840404510498,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.6184,
      "num_input_tokens_seen": 659712,
      "step": 400
    },
    {
      "epoch": 0.6584564860426929,
      "grad_norm": 9.925165176391602,
      "learning_rate": 3.742434902568889e-05,
      "loss": 0.6176,
      "num_input_tokens_seen": 661504,
      "step": 401
    },
    {
      "epoch": 0.6600985221674877,
      "grad_norm": 6.130084991455078,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 0.4508,
      "num_input_tokens_seen": 663040,
      "step": 402
    },
    {
      "epoch": 0.6617405582922824,
      "grad_norm": 6.460803985595703,
      "learning_rate": 3.727259384037852e-05,
      "loss": 0.3483,
      "num_input_tokens_seen": 665088,
      "step": 403
    },
    {
      "epoch": 0.6633825944170771,
      "grad_norm": 8.505308151245117,
      "learning_rate": 3.719649147846832e-05,
      "loss": 0.5372,
      "num_input_tokens_seen": 666624,
      "step": 404
    },
    {
      "epoch": 0.6650246305418719,
      "grad_norm": 7.1459574699401855,
      "learning_rate": 3.712024050615843e-05,
      "loss": 0.3365,
      "num_input_tokens_seen": 668160,
      "step": 405
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.611334323883057,
      "learning_rate": 3.704384185254288e-05,
      "loss": 0.4661,
      "num_input_tokens_seen": 669696,
      "step": 406
    },
    {
      "epoch": 0.6683087027914614,
      "grad_norm": 7.959935188293457,
      "learning_rate": 3.696729644851518e-05,
      "loss": 0.5011,
      "num_input_tokens_seen": 671744,
      "step": 407
    },
    {
      "epoch": 0.6699507389162561,
      "grad_norm": 9.476557731628418,
      "learning_rate": 3.689060522675689e-05,
      "loss": 0.867,
      "num_input_tokens_seen": 673280,
      "step": 408
    },
    {
      "epoch": 0.6715927750410509,
      "grad_norm": 4.877057075500488,
      "learning_rate": 3.681376912172636e-05,
      "loss": 0.2969,
      "num_input_tokens_seen": 675072,
      "step": 409
    },
    {
      "epoch": 0.6732348111658456,
      "grad_norm": 9.812087059020996,
      "learning_rate": 3.673678906964727e-05,
      "loss": 0.6871,
      "num_input_tokens_seen": 676864,
      "step": 410
    },
    {
      "epoch": 0.6748768472906403,
      "grad_norm": 11.55356502532959,
      "learning_rate": 3.665966600849728e-05,
      "loss": 1.085,
      "num_input_tokens_seen": 678400,
      "step": 411
    },
    {
      "epoch": 0.6765188834154351,
      "grad_norm": 4.991828918457031,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 0.1658,
      "num_input_tokens_seen": 680192,
      "step": 412
    },
    {
      "epoch": 0.6781609195402298,
      "grad_norm": 8.444284439086914,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 0.7038,
      "num_input_tokens_seen": 681984,
      "step": 413
    },
    {
      "epoch": 0.6798029556650246,
      "grad_norm": 3.2424802780151367,
      "learning_rate": 3.642744817646736e-05,
      "loss": 0.1205,
      "num_input_tokens_seen": 683520,
      "step": 414
    },
    {
      "epoch": 0.6814449917898193,
      "grad_norm": 5.724653244018555,
      "learning_rate": 3.634976249348867e-05,
      "loss": 0.2674,
      "num_input_tokens_seen": 685056,
      "step": 415
    },
    {
      "epoch": 0.6830870279146142,
      "grad_norm": 9.178004264831543,
      "learning_rate": 3.627193851723577e-05,
      "loss": 0.5236,
      "num_input_tokens_seen": 686848,
      "step": 416
    },
    {
      "epoch": 0.6847290640394089,
      "grad_norm": 5.289801120758057,
      "learning_rate": 3.619397719596924e-05,
      "loss": 0.1598,
      "num_input_tokens_seen": 688640,
      "step": 417
    },
    {
      "epoch": 0.6863711001642037,
      "grad_norm": 11.09640121459961,
      "learning_rate": 3.611587947962319e-05,
      "loss": 0.7695,
      "num_input_tokens_seen": 690432,
      "step": 418
    },
    {
      "epoch": 0.6880131362889984,
      "grad_norm": 11.337730407714844,
      "learning_rate": 3.603764631979363e-05,
      "loss": 0.859,
      "num_input_tokens_seen": 691968,
      "step": 419
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 9.920872688293457,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.4134,
      "num_input_tokens_seen": 693760,
      "step": 420
    },
    {
      "epoch": 0.6912972085385879,
      "grad_norm": 5.2697834968566895,
      "learning_rate": 3.588077748430819e-05,
      "loss": 0.1684,
      "num_input_tokens_seen": 695552,
      "step": 421
    },
    {
      "epoch": 0.6929392446633826,
      "grad_norm": 14.435799598693848,
      "learning_rate": 3.580214372004956e-05,
      "loss": 0.719,
      "num_input_tokens_seen": 697088,
      "step": 422
    },
    {
      "epoch": 0.6945812807881774,
      "grad_norm": 6.636081218719482,
      "learning_rate": 3.572337833507865e-05,
      "loss": 0.2539,
      "num_input_tokens_seen": 698880,
      "step": 423
    },
    {
      "epoch": 0.6962233169129721,
      "grad_norm": 11.768461227416992,
      "learning_rate": 3.564448228912682e-05,
      "loss": 0.4808,
      "num_input_tokens_seen": 700416,
      "step": 424
    },
    {
      "epoch": 0.6978653530377669,
      "grad_norm": 13.946436882019043,
      "learning_rate": 3.556545654351749e-05,
      "loss": 0.4373,
      "num_input_tokens_seen": 702208,
      "step": 425
    },
    {
      "epoch": 0.6995073891625616,
      "grad_norm": 8.752349853515625,
      "learning_rate": 3.548630206115443e-05,
      "loss": 0.3704,
      "num_input_tokens_seen": 704000,
      "step": 426
    },
    {
      "epoch": 0.7011494252873564,
      "grad_norm": 9.565587043762207,
      "learning_rate": 3.540701980651003e-05,
      "loss": 0.6099,
      "num_input_tokens_seen": 705536,
      "step": 427
    },
    {
      "epoch": 0.7027914614121511,
      "grad_norm": 3.6129209995269775,
      "learning_rate": 3.532761074561355e-05,
      "loss": 0.0932,
      "num_input_tokens_seen": 707072,
      "step": 428
    },
    {
      "epoch": 0.7044334975369458,
      "grad_norm": 11.053170204162598,
      "learning_rate": 3.524807584603932e-05,
      "loss": 0.4933,
      "num_input_tokens_seen": 708608,
      "step": 429
    },
    {
      "epoch": 0.7060755336617406,
      "grad_norm": 10.028136253356934,
      "learning_rate": 3.516841607689501e-05,
      "loss": 0.7287,
      "num_input_tokens_seen": 710400,
      "step": 430
    },
    {
      "epoch": 0.7077175697865353,
      "grad_norm": 7.39561653137207,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 0.2519,
      "num_input_tokens_seen": 711936,
      "step": 431
    },
    {
      "epoch": 0.7093596059113301,
      "grad_norm": 7.948756694793701,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.2494,
      "num_input_tokens_seen": 713472,
      "step": 432
    },
    {
      "epoch": 0.7110016420361248,
      "grad_norm": 6.775885581970215,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 0.3847,
      "num_input_tokens_seen": 715264,
      "step": 433
    },
    {
      "epoch": 0.7126436781609196,
      "grad_norm": 13.239886283874512,
      "learning_rate": 3.484854773977378e-05,
      "loss": 0.6951,
      "num_input_tokens_seen": 717056,
      "step": 434
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 6.7908806800842285,
      "learning_rate": 3.476827821223184e-05,
      "loss": 0.2173,
      "num_input_tokens_seen": 718592,
      "step": 435
    },
    {
      "epoch": 0.715927750410509,
      "grad_norm": 15.507123947143555,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 0.6322,
      "num_input_tokens_seen": 720128,
      "step": 436
    },
    {
      "epoch": 0.7175697865353038,
      "grad_norm": 11.965864181518555,
      "learning_rate": 3.460738306649509e-05,
      "loss": 0.3874,
      "num_input_tokens_seen": 721664,
      "step": 437
    },
    {
      "epoch": 0.7192118226600985,
      "grad_norm": 11.37467098236084,
      "learning_rate": 3.452675940875686e-05,
      "loss": 0.8714,
      "num_input_tokens_seen": 723456,
      "step": 438
    },
    {
      "epoch": 0.7208538587848933,
      "grad_norm": 11.411519050598145,
      "learning_rate": 3.444601967046168e-05,
      "loss": 0.3835,
      "num_input_tokens_seen": 724992,
      "step": 439
    },
    {
      "epoch": 0.722495894909688,
      "grad_norm": 14.423687934875488,
      "learning_rate": 3.436516483539781e-05,
      "loss": 0.793,
      "num_input_tokens_seen": 726528,
      "step": 440
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 8.273104667663574,
      "learning_rate": 3.428419588875588e-05,
      "loss": 0.4121,
      "num_input_tokens_seen": 728320,
      "step": 441
    },
    {
      "epoch": 0.7257799671592775,
      "grad_norm": 11.097722053527832,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 0.6845,
      "num_input_tokens_seen": 729856,
      "step": 442
    },
    {
      "epoch": 0.7274220032840722,
      "grad_norm": 12.507640838623047,
      "learning_rate": 3.412191960844049e-05,
      "loss": 0.804,
      "num_input_tokens_seen": 731648,
      "step": 443
    },
    {
      "epoch": 0.729064039408867,
      "grad_norm": 9.239700317382812,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 0.7169,
      "num_input_tokens_seen": 733696,
      "step": 444
    },
    {
      "epoch": 0.7307060755336617,
      "grad_norm": 11.999727249145508,
      "learning_rate": 3.39591987386325e-05,
      "loss": 0.719,
      "num_input_tokens_seen": 735232,
      "step": 445
    },
    {
      "epoch": 0.7323481116584565,
      "grad_norm": 11.150725364685059,
      "learning_rate": 3.387767406020343e-05,
      "loss": 0.4936,
      "num_input_tokens_seen": 737024,
      "step": 446
    },
    {
      "epoch": 0.7339901477832512,
      "grad_norm": 13.229561805725098,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 0.7349,
      "num_input_tokens_seen": 738560,
      "step": 447
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 9.518925666809082,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 0.3494,
      "num_input_tokens_seen": 740096,
      "step": 448
    },
    {
      "epoch": 0.7372742200328407,
      "grad_norm": 9.94247817993164,
      "learning_rate": 3.363245497496337e-05,
      "loss": 0.5674,
      "num_input_tokens_seen": 741632,
      "step": 449
    },
    {
      "epoch": 0.7389162561576355,
      "grad_norm": 2.36515736579895,
      "learning_rate": 3.355050358314172e-05,
      "loss": 0.0765,
      "num_input_tokens_seen": 743168,
      "step": 450
    },
    {
      "epoch": 0.7405582922824302,
      "grad_norm": 14.938685417175293,
      "learning_rate": 3.346844800613229e-05,
      "loss": 0.7643,
      "num_input_tokens_seen": 744704,
      "step": 451
    },
    {
      "epoch": 0.7422003284072249,
      "grad_norm": 5.726465702056885,
      "learning_rate": 3.338628924375638e-05,
      "loss": 0.1937,
      "num_input_tokens_seen": 746496,
      "step": 452
    },
    {
      "epoch": 0.7438423645320197,
      "grad_norm": 9.16743278503418,
      "learning_rate": 3.330402829709258e-05,
      "loss": 0.4583,
      "num_input_tokens_seen": 748032,
      "step": 453
    },
    {
      "epoch": 0.7454844006568144,
      "grad_norm": 10.41439151763916,
      "learning_rate": 3.322166616846458e-05,
      "loss": 0.497,
      "num_input_tokens_seen": 749568,
      "step": 454
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 10.051876068115234,
      "learning_rate": 3.313920386142892e-05,
      "loss": 0.6452,
      "num_input_tokens_seen": 751104,
      "step": 455
    },
    {
      "epoch": 0.7487684729064039,
      "grad_norm": 9.721705436706543,
      "learning_rate": 3.305664238076278e-05,
      "loss": 0.3865,
      "num_input_tokens_seen": 752896,
      "step": 456
    },
    {
      "epoch": 0.7504105090311987,
      "grad_norm": 6.806125164031982,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 0.3381,
      "num_input_tokens_seen": 754688,
      "step": 457
    },
    {
      "epoch": 0.7520525451559934,
      "grad_norm": 6.9068121910095215,
      "learning_rate": 3.289122592367757e-05,
      "loss": 0.2625,
      "num_input_tokens_seen": 756736,
      "step": 458
    },
    {
      "epoch": 0.7536945812807881,
      "grad_norm": 8.528579711914062,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 0.2486,
      "num_input_tokens_seen": 758272,
      "step": 459
    },
    {
      "epoch": 0.7553366174055829,
      "grad_norm": 9.312043190002441,
      "learning_rate": 3.272542485937369e-05,
      "loss": 0.6571,
      "num_input_tokens_seen": 760064,
      "step": 460
    },
    {
      "epoch": 0.7569786535303776,
      "grad_norm": 4.873279571533203,
      "learning_rate": 3.264238262407764e-05,
      "loss": 0.189,
      "num_input_tokens_seen": 761600,
      "step": 461
    },
    {
      "epoch": 0.7586206896551724,
      "grad_norm": 9.944074630737305,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 0.5459,
      "num_input_tokens_seen": 763392,
      "step": 462
    },
    {
      "epoch": 0.7602627257799671,
      "grad_norm": 9.151817321777344,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.631,
      "num_input_tokens_seen": 765184,
      "step": 463
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 14.269225120544434,
      "learning_rate": 3.239270125110117e-05,
      "loss": 0.6226,
      "num_input_tokens_seen": 766720,
      "step": 464
    },
    {
      "epoch": 0.7635467980295566,
      "grad_norm": 15.691020011901855,
      "learning_rate": 3.230929261806842e-05,
      "loss": 1.0814,
      "num_input_tokens_seen": 768256,
      "step": 465
    },
    {
      "epoch": 0.7651888341543513,
      "grad_norm": 11.293437004089355,
      "learning_rate": 3.222579492361179e-05,
      "loss": 0.5326,
      "num_input_tokens_seen": 770048,
      "step": 466
    },
    {
      "epoch": 0.7668308702791461,
      "grad_norm": 9.560182571411133,
      "learning_rate": 3.214220918512434e-05,
      "loss": 0.4826,
      "num_input_tokens_seen": 771584,
      "step": 467
    },
    {
      "epoch": 0.7684729064039408,
      "grad_norm": 12.817855834960938,
      "learning_rate": 3.205853642107192e-05,
      "loss": 0.4285,
      "num_input_tokens_seen": 773120,
      "step": 468
    },
    {
      "epoch": 0.7701149425287356,
      "grad_norm": 10.304119110107422,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 0.6263,
      "num_input_tokens_seen": 774656,
      "step": 469
    },
    {
      "epoch": 0.7717569786535303,
      "grad_norm": 8.751985549926758,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 0.5701,
      "num_input_tokens_seen": 776448,
      "step": 470
    },
    {
      "epoch": 0.7733990147783252,
      "grad_norm": 8.457816123962402,
      "learning_rate": 3.180700617601436e-05,
      "loss": 0.3851,
      "num_input_tokens_seen": 777984,
      "step": 471
    },
    {
      "epoch": 0.7750410509031199,
      "grad_norm": 6.686663627624512,
      "learning_rate": 3.172299551538164e-05,
      "loss": 0.2669,
      "num_input_tokens_seen": 779520,
      "step": 472
    },
    {
      "epoch": 0.7766830870279147,
      "grad_norm": 8.111963272094727,
      "learning_rate": 3.163890293717022e-05,
      "loss": 0.5836,
      "num_input_tokens_seen": 781056,
      "step": 473
    },
    {
      "epoch": 0.7783251231527094,
      "grad_norm": 11.525074005126953,
      "learning_rate": 3.155472946602162e-05,
      "loss": 0.5172,
      "num_input_tokens_seen": 782848,
      "step": 474
    },
    {
      "epoch": 0.7799671592775042,
      "grad_norm": 5.17915153503418,
      "learning_rate": 3.147047612756302e-05,
      "loss": 0.2782,
      "num_input_tokens_seen": 784384,
      "step": 475
    },
    {
      "epoch": 0.7816091954022989,
      "grad_norm": 7.321991920471191,
      "learning_rate": 3.138614394839476e-05,
      "loss": 0.3218,
      "num_input_tokens_seen": 785920,
      "step": 476
    },
    {
      "epoch": 0.7832512315270936,
      "grad_norm": 4.655022144317627,
      "learning_rate": 3.130173395607785e-05,
      "loss": 0.2476,
      "num_input_tokens_seen": 787712,
      "step": 477
    },
    {
      "epoch": 0.7848932676518884,
      "grad_norm": 7.761220932006836,
      "learning_rate": 3.121724717912138e-05,
      "loss": 0.4828,
      "num_input_tokens_seen": 789760,
      "step": 478
    },
    {
      "epoch": 0.7865353037766831,
      "grad_norm": 14.45020580291748,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 0.806,
      "num_input_tokens_seen": 791552,
      "step": 479
    },
    {
      "epoch": 0.7881773399014779,
      "grad_norm": 6.615232467651367,
      "learning_rate": 3.104804738999169e-05,
      "loss": 0.2425,
      "num_input_tokens_seen": 793088,
      "step": 480
    },
    {
      "epoch": 0.7898193760262726,
      "grad_norm": 10.012640953063965,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 0.4492,
      "num_input_tokens_seen": 794624,
      "step": 481
    },
    {
      "epoch": 0.7914614121510674,
      "grad_norm": 11.378480911254883,
      "learning_rate": 3.087855282756475e-05,
      "loss": 0.5365,
      "num_input_tokens_seen": 796416,
      "step": 482
    },
    {
      "epoch": 0.7931034482758621,
      "grad_norm": 9.372695922851562,
      "learning_rate": 3.079369758735393e-05,
      "loss": 0.548,
      "num_input_tokens_seen": 798208,
      "step": 483
    },
    {
      "epoch": 0.7947454844006568,
      "grad_norm": 10.641199111938477,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 0.3554,
      "num_input_tokens_seen": 799744,
      "step": 484
    },
    {
      "epoch": 0.7963875205254516,
      "grad_norm": 10.591800689697266,
      "learning_rate": 3.062377635859663e-05,
      "loss": 0.7338,
      "num_input_tokens_seen": 801280,
      "step": 485
    },
    {
      "epoch": 0.7980295566502463,
      "grad_norm": 8.147485733032227,
      "learning_rate": 3.053871244048669e-05,
      "loss": 0.4331,
      "num_input_tokens_seen": 802816,
      "step": 486
    },
    {
      "epoch": 0.7996715927750411,
      "grad_norm": 12.917737007141113,
      "learning_rate": 3.045358103491357e-05,
      "loss": 0.5513,
      "num_input_tokens_seen": 804608,
      "step": 487
    },
    {
      "epoch": 0.8013136288998358,
      "grad_norm": 10.26484489440918,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 0.314,
      "num_input_tokens_seen": 806144,
      "step": 488
    },
    {
      "epoch": 0.8029556650246306,
      "grad_norm": 7.888310432434082,
      "learning_rate": 3.028311991138472e-05,
      "loss": 0.3269,
      "num_input_tokens_seen": 807680,
      "step": 489
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 22.377246856689453,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 0.4488,
      "num_input_tokens_seen": 809216,
      "step": 490
    },
    {
      "epoch": 0.80623973727422,
      "grad_norm": 9.17805004119873,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 0.5412,
      "num_input_tokens_seen": 810752,
      "step": 491
    },
    {
      "epoch": 0.8078817733990148,
      "grad_norm": 7.36535120010376,
      "learning_rate": 3.002694802864912e-05,
      "loss": 0.3371,
      "num_input_tokens_seen": 812288,
      "step": 492
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 7.599347114562988,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 0.4997,
      "num_input_tokens_seen": 814080,
      "step": 493
    },
    {
      "epoch": 0.8111658456486043,
      "grad_norm": 18.0394229888916,
      "learning_rate": 2.98558587804993e-05,
      "loss": 0.9292,
      "num_input_tokens_seen": 815872,
      "step": 494
    },
    {
      "epoch": 0.812807881773399,
      "grad_norm": 7.6125993728637695,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 0.3202,
      "num_input_tokens_seen": 817408,
      "step": 495
    },
    {
      "epoch": 0.8144499178981938,
      "grad_norm": 5.453746318817139,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 0.2919,
      "num_input_tokens_seen": 818944,
      "step": 496
    },
    {
      "epoch": 0.8160919540229885,
      "grad_norm": 9.743996620178223,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.4172,
      "num_input_tokens_seen": 820736,
      "step": 497
    },
    {
      "epoch": 0.8177339901477833,
      "grad_norm": 11.177108764648438,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 0.6963,
      "num_input_tokens_seen": 822272,
      "step": 498
    },
    {
      "epoch": 0.819376026272578,
      "grad_norm": 12.060616493225098,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.6615,
      "num_input_tokens_seen": 823808,
      "step": 499
    },
    {
      "epoch": 0.8210180623973727,
      "grad_norm": 4.365041732788086,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 0.2351,
      "num_input_tokens_seen": 825856,
      "step": 500
    },
    {
      "epoch": 0.8210180623973727,
      "eval_loss": 0.46229347586631775,
      "eval_runtime": 10.7237,
      "eval_samples_per_second": 113.86,
      "eval_steps_per_second": 14.268,
      "num_input_tokens_seen": 825856,
      "step": 500
    },
    {
      "epoch": 0.8226600985221675,
      "grad_norm": 10.05671215057373,
      "learning_rate": 2.9255237479150816e-05,
      "loss": 0.4698,
      "num_input_tokens_seen": 827392,
      "step": 501
    },
    {
      "epoch": 0.8243021346469622,
      "grad_norm": 5.972174167633057,
      "learning_rate": 2.916921866790256e-05,
      "loss": 0.3976,
      "num_input_tokens_seen": 829184,
      "step": 502
    },
    {
      "epoch": 0.825944170771757,
      "grad_norm": 11.746431350708008,
      "learning_rate": 2.908314905604056e-05,
      "loss": 0.7339,
      "num_input_tokens_seen": 830720,
      "step": 503
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 7.052107334136963,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 0.2813,
      "num_input_tokens_seen": 832256,
      "step": 504
    },
    {
      "epoch": 0.8292282430213465,
      "grad_norm": 11.363142013549805,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 0.6609,
      "num_input_tokens_seen": 833792,
      "step": 505
    },
    {
      "epoch": 0.8308702791461412,
      "grad_norm": 11.164517402648926,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 0.5634,
      "num_input_tokens_seen": 835328,
      "step": 506
    },
    {
      "epoch": 0.8325123152709359,
      "grad_norm": 10.891583442687988,
      "learning_rate": 2.8738383586092745e-05,
      "loss": 0.6721,
      "num_input_tokens_seen": 836864,
      "step": 507
    },
    {
      "epoch": 0.8341543513957307,
      "grad_norm": 6.435769557952881,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 0.3834,
      "num_input_tokens_seen": 838656,
      "step": 508
    },
    {
      "epoch": 0.8357963875205254,
      "grad_norm": 8.314001083374023,
      "learning_rate": 2.8565723342637796e-05,
      "loss": 0.3614,
      "num_input_tokens_seen": 840448,
      "step": 509
    },
    {
      "epoch": 0.8374384236453202,
      "grad_norm": 12.544979095458984,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 0.7081,
      "num_input_tokens_seen": 841984,
      "step": 510
    },
    {
      "epoch": 0.8390804597701149,
      "grad_norm": 7.068687438964844,
      "learning_rate": 2.8392889310857612e-05,
      "loss": 0.4477,
      "num_input_tokens_seen": 843520,
      "step": 511
    },
    {
      "epoch": 0.8407224958949097,
      "grad_norm": 13.097939491271973,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 0.7354,
      "num_input_tokens_seen": 845312,
      "step": 512
    },
    {
      "epoch": 0.8423645320197044,
      "grad_norm": 7.463913917541504,
      "learning_rate": 2.8219889914439074e-05,
      "loss": 0.4832,
      "num_input_tokens_seen": 846848,
      "step": 513
    },
    {
      "epoch": 0.8440065681444991,
      "grad_norm": 11.028315544128418,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 0.4564,
      "num_input_tokens_seen": 848896,
      "step": 514
    },
    {
      "epoch": 0.8456486042692939,
      "grad_norm": 7.6387939453125,
      "learning_rate": 2.8046733585128687e-05,
      "loss": 0.6251,
      "num_input_tokens_seen": 850432,
      "step": 515
    },
    {
      "epoch": 0.8472906403940886,
      "grad_norm": 13.609099388122559,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 0.4302,
      "num_input_tokens_seen": 852224,
      "step": 516
    },
    {
      "epoch": 0.8489326765188834,
      "grad_norm": 5.024510383605957,
      "learning_rate": 2.787342876232167e-05,
      "loss": 0.1688,
      "num_input_tokens_seen": 854016,
      "step": 517
    },
    {
      "epoch": 0.8505747126436781,
      "grad_norm": 6.364668846130371,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 0.3676,
      "num_input_tokens_seen": 855552,
      "step": 518
    },
    {
      "epoch": 0.8522167487684729,
      "grad_norm": 6.283124923706055,
      "learning_rate": 2.7699983892650573e-05,
      "loss": 0.2092,
      "num_input_tokens_seen": 857088,
      "step": 519
    },
    {
      "epoch": 0.8538587848932676,
      "grad_norm": 7.626494884490967,
      "learning_rate": 2.761321158169134e-05,
      "loss": 0.3534,
      "num_input_tokens_seen": 858880,
      "step": 520
    },
    {
      "epoch": 0.8555008210180624,
      "grad_norm": 7.384657382965088,
      "learning_rate": 2.7526407429573657e-05,
      "loss": 0.3117,
      "num_input_tokens_seen": 860416,
      "step": 521
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 13.94422721862793,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 0.7077,
      "num_input_tokens_seen": 862208,
      "step": 522
    },
    {
      "epoch": 0.8587848932676518,
      "grad_norm": 7.076534271240234,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 0.2314,
      "num_input_tokens_seen": 864000,
      "step": 523
    },
    {
      "epoch": 0.8604269293924466,
      "grad_norm": 13.117365837097168,
      "learning_rate": 2.726581450494451e-05,
      "loss": 0.8831,
      "num_input_tokens_seen": 865536,
      "step": 524
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 11.027436256408691,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.6819,
      "num_input_tokens_seen": 867072,
      "step": 525
    },
    {
      "epoch": 0.8637110016420362,
      "grad_norm": 13.396268844604492,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 0.5495,
      "num_input_tokens_seen": 868608,
      "step": 526
    },
    {
      "epoch": 0.8653530377668309,
      "grad_norm": 6.512442111968994,
      "learning_rate": 2.7004973108221472e-05,
      "loss": 0.2127,
      "num_input_tokens_seen": 870144,
      "step": 527
    },
    {
      "epoch": 0.8669950738916257,
      "grad_norm": 6.440733432769775,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 0.3311,
      "num_input_tokens_seen": 871680,
      "step": 528
    },
    {
      "epoch": 0.8686371100164204,
      "grad_norm": 5.119698524475098,
      "learning_rate": 2.6830954928190794e-05,
      "loss": 0.1881,
      "num_input_tokens_seen": 873216,
      "step": 529
    },
    {
      "epoch": 0.8702791461412152,
      "grad_norm": 7.7045769691467285,
      "learning_rate": 2.674391184360313e-05,
      "loss": 0.5251,
      "num_input_tokens_seen": 875008,
      "step": 530
    },
    {
      "epoch": 0.8719211822660099,
      "grad_norm": 11.976244926452637,
      "learning_rate": 2.6656847510000012e-05,
      "loss": 0.8272,
      "num_input_tokens_seen": 876544,
      "step": 531
    },
    {
      "epoch": 0.8735632183908046,
      "grad_norm": 10.696399688720703,
      "learning_rate": 2.656976298823284e-05,
      "loss": 0.6728,
      "num_input_tokens_seen": 878336,
      "step": 532
    },
    {
      "epoch": 0.8752052545155994,
      "grad_norm": 9.512816429138184,
      "learning_rate": 2.6482659339399045e-05,
      "loss": 0.6017,
      "num_input_tokens_seen": 879872,
      "step": 533
    },
    {
      "epoch": 0.8768472906403941,
      "grad_norm": 9.834319114685059,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 0.3947,
      "num_input_tokens_seen": 881920,
      "step": 534
    },
    {
      "epoch": 0.8784893267651889,
      "grad_norm": 13.181539535522461,
      "learning_rate": 2.63083989060736e-05,
      "loss": 0.7479,
      "num_input_tokens_seen": 883712,
      "step": 535
    },
    {
      "epoch": 0.8801313628899836,
      "grad_norm": 8.24552059173584,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 0.5915,
      "num_input_tokens_seen": 885248,
      "step": 536
    },
    {
      "epoch": 0.8817733990147784,
      "grad_norm": 9.224813461303711,
      "learning_rate": 2.6134074703231344e-05,
      "loss": 0.4018,
      "num_input_tokens_seen": 886784,
      "step": 537
    },
    {
      "epoch": 0.8834154351395731,
      "grad_norm": 16.71966552734375,
      "learning_rate": 2.604689134322999e-05,
      "loss": 0.6,
      "num_input_tokens_seen": 888320,
      "step": 538
    },
    {
      "epoch": 0.8850574712643678,
      "grad_norm": 10.81427001953125,
      "learning_rate": 2.5959695227188004e-05,
      "loss": 0.3617,
      "num_input_tokens_seen": 889856,
      "step": 539
    },
    {
      "epoch": 0.8866995073891626,
      "grad_norm": 14.1209135055542,
      "learning_rate": 2.587248741756253e-05,
      "loss": 0.9071,
      "num_input_tokens_seen": 891648,
      "step": 540
    },
    {
      "epoch": 0.8883415435139573,
      "grad_norm": 12.136558532714844,
      "learning_rate": 2.578526897695321e-05,
      "loss": 0.7434,
      "num_input_tokens_seen": 893184,
      "step": 541
    },
    {
      "epoch": 0.8899835796387521,
      "grad_norm": 11.088306427001953,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 0.5946,
      "num_input_tokens_seen": 894976,
      "step": 542
    },
    {
      "epoch": 0.8916256157635468,
      "grad_norm": 10.302495956420898,
      "learning_rate": 2.5610804453816333e-05,
      "loss": 0.467,
      "num_input_tokens_seen": 896512,
      "step": 543
    },
    {
      "epoch": 0.8932676518883416,
      "grad_norm": 6.7054595947265625,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 0.2261,
      "num_input_tokens_seen": 898048,
      "step": 544
    },
    {
      "epoch": 0.8949096880131363,
      "grad_norm": 10.811493873596191,
      "learning_rate": 2.5436310160932092e-05,
      "loss": 0.6158,
      "num_input_tokens_seen": 899584,
      "step": 545
    },
    {
      "epoch": 0.896551724137931,
      "grad_norm": 11.773672103881836,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 0.795,
      "num_input_tokens_seen": 901376,
      "step": 546
    },
    {
      "epoch": 0.8981937602627258,
      "grad_norm": 10.923112869262695,
      "learning_rate": 2.5261794602906145e-05,
      "loss": 0.7706,
      "num_input_tokens_seen": 903168,
      "step": 547
    },
    {
      "epoch": 0.8998357963875205,
      "grad_norm": 11.201061248779297,
      "learning_rate": 2.517453150744904e-05,
      "loss": 0.5873,
      "num_input_tokens_seen": 904704,
      "step": 548
    },
    {
      "epoch": 0.9014778325123153,
      "grad_norm": 10.201632499694824,
      "learning_rate": 2.5087266285380596e-05,
      "loss": 0.4015,
      "num_input_tokens_seen": 906496,
      "step": 549
    },
    {
      "epoch": 0.90311986863711,
      "grad_norm": 12.01679515838623,
      "learning_rate": 2.5e-05,
      "loss": 0.4401,
      "num_input_tokens_seen": 908032,
      "step": 550
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 11.552082061767578,
      "learning_rate": 2.4912733714619417e-05,
      "loss": 0.563,
      "num_input_tokens_seen": 909568,
      "step": 551
    },
    {
      "epoch": 0.9064039408866995,
      "grad_norm": 10.978461265563965,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 0.4623,
      "num_input_tokens_seen": 911104,
      "step": 552
    },
    {
      "epoch": 0.9080459770114943,
      "grad_norm": 11.35382080078125,
      "learning_rate": 2.4738205397093864e-05,
      "loss": 0.5665,
      "num_input_tokens_seen": 912640,
      "step": 553
    },
    {
      "epoch": 0.909688013136289,
      "grad_norm": 9.205479621887207,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 0.3617,
      "num_input_tokens_seen": 914176,
      "step": 554
    },
    {
      "epoch": 0.9113300492610837,
      "grad_norm": 12.579888343811035,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.5729,
      "num_input_tokens_seen": 915712,
      "step": 555
    },
    {
      "epoch": 0.9129720853858785,
      "grad_norm": 8.45704174041748,
      "learning_rate": 2.447643950291608e-05,
      "loss": 0.5823,
      "num_input_tokens_seen": 917248,
      "step": 556
    },
    {
      "epoch": 0.9146141215106732,
      "grad_norm": 13.80907154083252,
      "learning_rate": 2.4389195546183673e-05,
      "loss": 0.9179,
      "num_input_tokens_seen": 919040,
      "step": 557
    },
    {
      "epoch": 0.916256157635468,
      "grad_norm": 9.249164581298828,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 0.7525,
      "num_input_tokens_seen": 920576,
      "step": 558
    },
    {
      "epoch": 0.9178981937602627,
      "grad_norm": 6.357658863067627,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 0.3909,
      "num_input_tokens_seen": 922368,
      "step": 559
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 9.787382125854492,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 0.6404,
      "num_input_tokens_seen": 923904,
      "step": 560
    },
    {
      "epoch": 0.9211822660098522,
      "grad_norm": 7.880619525909424,
      "learning_rate": 2.4040304772812002e-05,
      "loss": 0.3756,
      "num_input_tokens_seen": 925440,
      "step": 561
    },
    {
      "epoch": 0.922824302134647,
      "grad_norm": 8.133321762084961,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 0.5846,
      "num_input_tokens_seen": 926976,
      "step": 562
    },
    {
      "epoch": 0.9244663382594417,
      "grad_norm": 8.065176963806152,
      "learning_rate": 2.386592529676866e-05,
      "loss": 0.6157,
      "num_input_tokens_seen": 928512,
      "step": 563
    },
    {
      "epoch": 0.9261083743842364,
      "grad_norm": 7.180366039276123,
      "learning_rate": 2.377875575510967e-05,
      "loss": 0.6765,
      "num_input_tokens_seen": 930048,
      "step": 564
    },
    {
      "epoch": 0.9277504105090312,
      "grad_norm": 10.998181343078613,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 0.5252,
      "num_input_tokens_seen": 931584,
      "step": 565
    },
    {
      "epoch": 0.9293924466338259,
      "grad_norm": 10.070841789245605,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 0.547,
      "num_input_tokens_seen": 933120,
      "step": 566
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 6.069126129150391,
      "learning_rate": 2.3517340660600964e-05,
      "loss": 0.2883,
      "num_input_tokens_seen": 934912,
      "step": 567
    },
    {
      "epoch": 0.9326765188834154,
      "grad_norm": 5.6393609046936035,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 0.3318,
      "num_input_tokens_seen": 936448,
      "step": 568
    },
    {
      "epoch": 0.9343185550082101,
      "grad_norm": 10.091519355773926,
      "learning_rate": 2.3343152490000004e-05,
      "loss": 0.4154,
      "num_input_tokens_seen": 937984,
      "step": 569
    },
    {
      "epoch": 0.9359605911330049,
      "grad_norm": 6.167525291442871,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 0.3144,
      "num_input_tokens_seen": 939520,
      "step": 570
    },
    {
      "epoch": 0.9376026272577996,
      "grad_norm": 5.988082408905029,
      "learning_rate": 2.3169045071809215e-05,
      "loss": 0.4068,
      "num_input_tokens_seen": 941312,
      "step": 571
    },
    {
      "epoch": 0.9392446633825944,
      "grad_norm": 6.457799434661865,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 0.445,
      "num_input_tokens_seen": 943360,
      "step": 572
    },
    {
      "epoch": 0.9408866995073891,
      "grad_norm": 6.988043308258057,
      "learning_rate": 2.299502689177853e-05,
      "loss": 0.4012,
      "num_input_tokens_seen": 944896,
      "step": 573
    },
    {
      "epoch": 0.9425287356321839,
      "grad_norm": 3.31038761138916,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 0.1063,
      "num_input_tokens_seen": 946432,
      "step": 574
    },
    {
      "epoch": 0.9441707717569786,
      "grad_norm": 9.058358192443848,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 0.5031,
      "num_input_tokens_seen": 947968,
      "step": 575
    },
    {
      "epoch": 0.9458128078817734,
      "grad_norm": 8.161986351013184,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 0.4067,
      "num_input_tokens_seen": 949504,
      "step": 576
    },
    {
      "epoch": 0.9474548440065681,
      "grad_norm": 11.10789680480957,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 0.7155,
      "num_input_tokens_seen": 951040,
      "step": 577
    },
    {
      "epoch": 0.9490968801313628,
      "grad_norm": 11.818195343017578,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 0.9613,
      "num_input_tokens_seen": 952832,
      "step": 578
    },
    {
      "epoch": 0.9507389162561576,
      "grad_norm": 5.235378265380859,
      "learning_rate": 2.247359257042634e-05,
      "loss": 0.2972,
      "num_input_tokens_seen": 954624,
      "step": 579
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 13.563277244567871,
      "learning_rate": 2.238678841830867e-05,
      "loss": 0.7886,
      "num_input_tokens_seen": 956928,
      "step": 580
    },
    {
      "epoch": 0.9540229885057471,
      "grad_norm": 9.574675559997559,
      "learning_rate": 2.230001610734943e-05,
      "loss": 0.555,
      "num_input_tokens_seen": 958464,
      "step": 581
    },
    {
      "epoch": 0.9556650246305419,
      "grad_norm": 14.986449241638184,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 0.7777,
      "num_input_tokens_seen": 960256,
      "step": 582
    },
    {
      "epoch": 0.9573070607553367,
      "grad_norm": 10.136667251586914,
      "learning_rate": 2.212657123767834e-05,
      "loss": 0.661,
      "num_input_tokens_seen": 961792,
      "step": 583
    },
    {
      "epoch": 0.9589490968801314,
      "grad_norm": 11.859803199768066,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 0.75,
      "num_input_tokens_seen": 963328,
      "step": 584
    },
    {
      "epoch": 0.9605911330049262,
      "grad_norm": 8.86447525024414,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.3859,
      "num_input_tokens_seen": 965120,
      "step": 585
    },
    {
      "epoch": 0.9622331691297209,
      "grad_norm": 9.179612159729004,
      "learning_rate": 2.186666916089239e-05,
      "loss": 0.6129,
      "num_input_tokens_seen": 966656,
      "step": 586
    },
    {
      "epoch": 0.9638752052545156,
      "grad_norm": 9.966468811035156,
      "learning_rate": 2.1780110085560935e-05,
      "loss": 0.4362,
      "num_input_tokens_seen": 968192,
      "step": 587
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 11.16082763671875,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 0.6146,
      "num_input_tokens_seen": 969984,
      "step": 588
    },
    {
      "epoch": 0.9671592775041051,
      "grad_norm": 7.612130641937256,
      "learning_rate": 2.1607110689142393e-05,
      "loss": 0.4172,
      "num_input_tokens_seen": 971776,
      "step": 589
    },
    {
      "epoch": 0.9688013136288999,
      "grad_norm": 10.436551094055176,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 0.6196,
      "num_input_tokens_seen": 973312,
      "step": 590
    },
    {
      "epoch": 0.9704433497536946,
      "grad_norm": 6.249948024749756,
      "learning_rate": 2.1434276657362213e-05,
      "loss": 0.3468,
      "num_input_tokens_seen": 975104,
      "step": 591
    },
    {
      "epoch": 0.9720853858784894,
      "grad_norm": 7.721785068511963,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 0.334,
      "num_input_tokens_seen": 977152,
      "step": 592
    },
    {
      "epoch": 0.9737274220032841,
      "grad_norm": 13.591642379760742,
      "learning_rate": 2.1261616413907265e-05,
      "loss": 0.8916,
      "num_input_tokens_seen": 978944,
      "step": 593
    },
    {
      "epoch": 0.9753694581280788,
      "grad_norm": 8.180648803710938,
      "learning_rate": 2.117535409289905e-05,
      "loss": 0.5387,
      "num_input_tokens_seen": 980480,
      "step": 594
    },
    {
      "epoch": 0.9770114942528736,
      "grad_norm": 9.093683242797852,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 0.5025,
      "num_input_tokens_seen": 982272,
      "step": 595
    },
    {
      "epoch": 0.9786535303776683,
      "grad_norm": 12.774280548095703,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 0.7178,
      "num_input_tokens_seen": 983808,
      "step": 596
    },
    {
      "epoch": 0.9802955665024631,
      "grad_norm": 7.071635723114014,
      "learning_rate": 2.0916850943959452e-05,
      "loss": 0.5568,
      "num_input_tokens_seen": 985344,
      "step": 597
    },
    {
      "epoch": 0.9819376026272578,
      "grad_norm": 10.338504791259766,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 0.7227,
      "num_input_tokens_seen": 987136,
      "step": 598
    },
    {
      "epoch": 0.9835796387520526,
      "grad_norm": 6.2459940910339355,
      "learning_rate": 2.0744762520849193e-05,
      "loss": 0.3384,
      "num_input_tokens_seen": 988672,
      "step": 599
    },
    {
      "epoch": 0.9852216748768473,
      "grad_norm": 10.024309158325195,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 0.6529,
      "num_input_tokens_seen": 990208,
      "step": 600
    },
    {
      "epoch": 0.986863711001642,
      "grad_norm": 7.073548793792725,
      "learning_rate": 2.057288149201042e-05,
      "loss": 0.4898,
      "num_input_tokens_seen": 991744,
      "step": 601
    },
    {
      "epoch": 0.9885057471264368,
      "grad_norm": 7.569375514984131,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 0.3108,
      "num_input_tokens_seen": 993536,
      "step": 602
    },
    {
      "epoch": 0.9901477832512315,
      "grad_norm": 9.78426456451416,
      "learning_rate": 2.0401216234681995e-05,
      "loss": 0.4064,
      "num_input_tokens_seen": 995584,
      "step": 603
    },
    {
      "epoch": 0.9917898193760263,
      "grad_norm": 6.5752105712890625,
      "learning_rate": 2.031546713535688e-05,
      "loss": 0.281,
      "num_input_tokens_seen": 997376,
      "step": 604
    },
    {
      "epoch": 0.993431855500821,
      "grad_norm": 9.56201171875,
      "learning_rate": 2.022977511558638e-05,
      "loss": 0.467,
      "num_input_tokens_seen": 999680,
      "step": 605
    },
    {
      "epoch": 0.9950738916256158,
      "grad_norm": 7.6992902755737305,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 0.3534,
      "num_input_tokens_seen": 1001472,
      "step": 606
    },
    {
      "epoch": 0.9967159277504105,
      "grad_norm": 9.1334810256958,
      "learning_rate": 2.0058566490521847e-05,
      "loss": 0.5171,
      "num_input_tokens_seen": 1003008,
      "step": 607
    },
    {
      "epoch": 0.9983579638752053,
      "grad_norm": 10.123245239257812,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 0.7396,
      "num_input_tokens_seen": 1004800,
      "step": 608
    },
    {
      "epoch": 1.0,
      "grad_norm": 14.070388793945312,
      "learning_rate": 1.9887598703955242e-05,
      "loss": 0.9834,
      "num_input_tokens_seen": 1006256,
      "step": 609
    },
    {
      "epoch": 1.0016420361247949,
      "grad_norm": 11.557026863098145,
      "learning_rate": 1.980220772955602e-05,
      "loss": 0.6965,
      "num_input_tokens_seen": 1008048,
      "step": 610
    },
    {
      "epoch": 1.0032840722495895,
      "grad_norm": 6.611094951629639,
      "learning_rate": 1.9716880088615285e-05,
      "loss": 0.3057,
      "num_input_tokens_seen": 1009584,
      "step": 611
    },
    {
      "epoch": 1.0049261083743843,
      "grad_norm": 8.120351791381836,
      "learning_rate": 1.963161682082342e-05,
      "loss": 0.5893,
      "num_input_tokens_seen": 1011120,
      "step": 612
    },
    {
      "epoch": 1.006568144499179,
      "grad_norm": 10.714030265808105,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 0.5437,
      "num_input_tokens_seen": 1012656,
      "step": 613
    },
    {
      "epoch": 1.0082101806239738,
      "grad_norm": 8.649077415466309,
      "learning_rate": 1.946128755951332e-05,
      "loss": 0.3357,
      "num_input_tokens_seen": 1014192,
      "step": 614
    },
    {
      "epoch": 1.0098522167487685,
      "grad_norm": 7.641050815582275,
      "learning_rate": 1.937622364140338e-05,
      "loss": 0.2854,
      "num_input_tokens_seen": 1015728,
      "step": 615
    },
    {
      "epoch": 1.0114942528735633,
      "grad_norm": 15.188261985778809,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 0.9276,
      "num_input_tokens_seen": 1017776,
      "step": 616
    },
    {
      "epoch": 1.013136288998358,
      "grad_norm": 7.4135847091674805,
      "learning_rate": 1.920630241264607e-05,
      "loss": 0.4008,
      "num_input_tokens_seen": 1019568,
      "step": 617
    },
    {
      "epoch": 1.0147783251231528,
      "grad_norm": 8.185888290405273,
      "learning_rate": 1.912144717243525e-05,
      "loss": 0.4242,
      "num_input_tokens_seen": 1021104,
      "step": 618
    },
    {
      "epoch": 1.0164203612479474,
      "grad_norm": 8.437142372131348,
      "learning_rate": 1.9036663560535483e-05,
      "loss": 0.5317,
      "num_input_tokens_seen": 1022640,
      "step": 619
    },
    {
      "epoch": 1.0180623973727423,
      "grad_norm": 8.029325485229492,
      "learning_rate": 1.895195261000831e-05,
      "loss": 0.2341,
      "num_input_tokens_seen": 1024176,
      "step": 620
    },
    {
      "epoch": 1.019704433497537,
      "grad_norm": 14.404728889465332,
      "learning_rate": 1.8867315353029935e-05,
      "loss": 0.7531,
      "num_input_tokens_seen": 1025968,
      "step": 621
    },
    {
      "epoch": 1.0213464696223318,
      "grad_norm": 6.810080528259277,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 0.4492,
      "num_input_tokens_seen": 1027760,
      "step": 622
    },
    {
      "epoch": 1.0229885057471264,
      "grad_norm": 6.486141204833984,
      "learning_rate": 1.869826604392216e-05,
      "loss": 0.1569,
      "num_input_tokens_seen": 1029296,
      "step": 623
    },
    {
      "epoch": 1.0246305418719213,
      "grad_norm": 7.858831405639648,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 0.335,
      "num_input_tokens_seen": 1031088,
      "step": 624
    },
    {
      "epoch": 1.026272577996716,
      "grad_norm": 15.105812072753906,
      "learning_rate": 1.852952387243698e-05,
      "loss": 0.8189,
      "num_input_tokens_seen": 1032624,
      "step": 625
    },
    {
      "epoch": 1.0279146141215108,
      "grad_norm": 10.473692893981934,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 0.4169,
      "num_input_tokens_seen": 1034160,
      "step": 626
    },
    {
      "epoch": 1.0295566502463054,
      "grad_norm": 8.014642715454102,
      "learning_rate": 1.8361097062829778e-05,
      "loss": 0.3356,
      "num_input_tokens_seen": 1035696,
      "step": 627
    },
    {
      "epoch": 1.0311986863711002,
      "grad_norm": 10.925652503967285,
      "learning_rate": 1.827700448461836e-05,
      "loss": 0.4314,
      "num_input_tokens_seen": 1037232,
      "step": 628
    },
    {
      "epoch": 1.0328407224958949,
      "grad_norm": 10.91152572631836,
      "learning_rate": 1.8192993823985643e-05,
      "loss": 0.5987,
      "num_input_tokens_seen": 1038768,
      "step": 629
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 9.947729110717773,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 0.3236,
      "num_input_tokens_seen": 1040304,
      "step": 630
    },
    {
      "epoch": 1.0361247947454844,
      "grad_norm": 9.302042007446289,
      "learning_rate": 1.802522234901927e-05,
      "loss": 0.5509,
      "num_input_tokens_seen": 1042096,
      "step": 631
    },
    {
      "epoch": 1.0377668308702792,
      "grad_norm": 6.830209732055664,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 0.4275,
      "num_input_tokens_seen": 1043888,
      "step": 632
    },
    {
      "epoch": 1.0394088669950738,
      "grad_norm": 11.9580717086792,
      "learning_rate": 1.7857790814875663e-05,
      "loss": 0.6506,
      "num_input_tokens_seen": 1045424,
      "step": 633
    },
    {
      "epoch": 1.0410509031198687,
      "grad_norm": 5.002927780151367,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 0.1917,
      "num_input_tokens_seen": 1046960,
      "step": 634
    },
    {
      "epoch": 1.0426929392446633,
      "grad_norm": 12.42183780670166,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 0.7846,
      "num_input_tokens_seen": 1048496,
      "step": 635
    },
    {
      "epoch": 1.0443349753694582,
      "grad_norm": 11.095224380493164,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 0.4458,
      "num_input_tokens_seen": 1050032,
      "step": 636
    },
    {
      "epoch": 1.0459770114942528,
      "grad_norm": 7.083676815032959,
      "learning_rate": 1.7523980193597836e-05,
      "loss": 0.3357,
      "num_input_tokens_seen": 1051568,
      "step": 637
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 6.149731159210205,
      "learning_rate": 1.744075273123889e-05,
      "loss": 0.2158,
      "num_input_tokens_seen": 1053616,
      "step": 638
    },
    {
      "epoch": 1.0492610837438423,
      "grad_norm": 10.587393760681152,
      "learning_rate": 1.735761737592236e-05,
      "loss": 0.7571,
      "num_input_tokens_seen": 1055408,
      "step": 639
    },
    {
      "epoch": 1.0509031198686372,
      "grad_norm": 11.807936668395996,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 0.5596,
      "num_input_tokens_seen": 1057200,
      "step": 640
    },
    {
      "epoch": 1.0525451559934318,
      "grad_norm": 10.575610160827637,
      "learning_rate": 1.7191627037194186e-05,
      "loss": 0.4748,
      "num_input_tokens_seen": 1058736,
      "step": 641
    },
    {
      "epoch": 1.0541871921182266,
      "grad_norm": 21.1263484954834,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 0.6749,
      "num_input_tokens_seen": 1060272,
      "step": 642
    },
    {
      "epoch": 1.0558292282430213,
      "grad_norm": 9.643438339233398,
      "learning_rate": 1.702601726754825e-05,
      "loss": 0.4296,
      "num_input_tokens_seen": 1062064,
      "step": 643
    },
    {
      "epoch": 1.0574712643678161,
      "grad_norm": 10.229148864746094,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 0.4844,
      "num_input_tokens_seen": 1063856,
      "step": 644
    },
    {
      "epoch": 1.0591133004926108,
      "grad_norm": 4.005950927734375,
      "learning_rate": 1.686079613857109e-05,
      "loss": 0.1681,
      "num_input_tokens_seen": 1065392,
      "step": 645
    },
    {
      "epoch": 1.0607553366174056,
      "grad_norm": 5.15407657623291,
      "learning_rate": 1.677833383153542e-05,
      "loss": 0.1628,
      "num_input_tokens_seen": 1066928,
      "step": 646
    },
    {
      "epoch": 1.0623973727422003,
      "grad_norm": 6.03054141998291,
      "learning_rate": 1.6695971702907426e-05,
      "loss": 0.2415,
      "num_input_tokens_seen": 1068464,
      "step": 647
    },
    {
      "epoch": 1.064039408866995,
      "grad_norm": 5.2551164627075195,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 0.1813,
      "num_input_tokens_seen": 1070256,
      "step": 648
    },
    {
      "epoch": 1.0656814449917897,
      "grad_norm": 11.073025703430176,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 0.4527,
      "num_input_tokens_seen": 1071792,
      "step": 649
    },
    {
      "epoch": 1.0673234811165846,
      "grad_norm": 7.564355373382568,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 0.1935,
      "num_input_tokens_seen": 1073328,
      "step": 650
    },
    {
      "epoch": 1.0689655172413792,
      "grad_norm": 10.782079696655273,
      "learning_rate": 1.6367545025036636e-05,
      "loss": 0.6443,
      "num_input_tokens_seen": 1075120,
      "step": 651
    },
    {
      "epoch": 1.070607553366174,
      "grad_norm": 12.162782669067383,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 0.3348,
      "num_input_tokens_seen": 1076912,
      "step": 652
    },
    {
      "epoch": 1.0722495894909687,
      "grad_norm": 9.642202377319336,
      "learning_rate": 1.6203958789882456e-05,
      "loss": 0.2897,
      "num_input_tokens_seen": 1078704,
      "step": 653
    },
    {
      "epoch": 1.0738916256157636,
      "grad_norm": 17.00408363342285,
      "learning_rate": 1.612232593979658e-05,
      "loss": 0.6401,
      "num_input_tokens_seen": 1080496,
      "step": 654
    },
    {
      "epoch": 1.0755336617405582,
      "grad_norm": 13.505940437316895,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 0.8744,
      "num_input_tokens_seen": 1082032,
      "step": 655
    },
    {
      "epoch": 1.077175697865353,
      "grad_norm": 14.553035736083984,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 1.0131,
      "num_input_tokens_seen": 1083824,
      "step": 656
    },
    {
      "epoch": 1.0788177339901477,
      "grad_norm": 7.472227096557617,
      "learning_rate": 1.5878080391559508e-05,
      "loss": 0.3359,
      "num_input_tokens_seen": 1085360,
      "step": 657
    },
    {
      "epoch": 1.0804597701149425,
      "grad_norm": 9.70743179321289,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 0.5195,
      "num_input_tokens_seen": 1087152,
      "step": 658
    },
    {
      "epoch": 1.0821018062397372,
      "grad_norm": 6.417266368865967,
      "learning_rate": 1.5715804111244137e-05,
      "loss": 0.1679,
      "num_input_tokens_seen": 1088688,
      "step": 659
    },
    {
      "epoch": 1.083743842364532,
      "grad_norm": 14.876625061035156,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.5398,
      "num_input_tokens_seen": 1090480,
      "step": 660
    },
    {
      "epoch": 1.0853858784893267,
      "grad_norm": 12.731746673583984,
      "learning_rate": 1.5553980329538326e-05,
      "loss": 0.6476,
      "num_input_tokens_seen": 1092272,
      "step": 661
    },
    {
      "epoch": 1.0870279146141215,
      "grad_norm": 10.512730598449707,
      "learning_rate": 1.547324059124315e-05,
      "loss": 0.5199,
      "num_input_tokens_seen": 1094064,
      "step": 662
    },
    {
      "epoch": 1.0886699507389164,
      "grad_norm": 4.405415058135986,
      "learning_rate": 1.539261693350491e-05,
      "loss": 0.1591,
      "num_input_tokens_seen": 1095600,
      "step": 663
    },
    {
      "epoch": 1.090311986863711,
      "grad_norm": 10.222028732299805,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 0.4938,
      "num_input_tokens_seen": 1097136,
      "step": 664
    },
    {
      "epoch": 1.0919540229885056,
      "grad_norm": 8.940844535827637,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.2356,
      "num_input_tokens_seen": 1098928,
      "step": 665
    },
    {
      "epoch": 1.0935960591133005,
      "grad_norm": 11.056028366088867,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 0.6083,
      "num_input_tokens_seen": 1100976,
      "step": 666
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 7.800994873046875,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 0.3095,
      "num_input_tokens_seen": 1102768,
      "step": 667
    },
    {
      "epoch": 1.09688013136289,
      "grad_norm": 7.197283744812012,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 0.276,
      "num_input_tokens_seen": 1104304,
      "step": 668
    },
    {
      "epoch": 1.0985221674876848,
      "grad_norm": 5.619527816772461,
      "learning_rate": 1.4911367591190248e-05,
      "loss": 0.1723,
      "num_input_tokens_seen": 1105840,
      "step": 669
    },
    {
      "epoch": 1.1001642036124795,
      "grad_norm": 12.585229873657227,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 0.5555,
      "num_input_tokens_seen": 1107376,
      "step": 670
    },
    {
      "epoch": 1.1018062397372743,
      "grad_norm": 13.035242080688477,
      "learning_rate": 1.475192415396068e-05,
      "loss": 0.4959,
      "num_input_tokens_seen": 1108912,
      "step": 671
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 9.179338455200195,
      "learning_rate": 1.467238925438646e-05,
      "loss": 0.2863,
      "num_input_tokens_seen": 1110448,
      "step": 672
    },
    {
      "epoch": 1.1050903119868638,
      "grad_norm": 7.571164131164551,
      "learning_rate": 1.4592980193489975e-05,
      "loss": 0.2374,
      "num_input_tokens_seen": 1111984,
      "step": 673
    },
    {
      "epoch": 1.1067323481116584,
      "grad_norm": 10.78582763671875,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 0.6064,
      "num_input_tokens_seen": 1113520,
      "step": 674
    },
    {
      "epoch": 1.1083743842364533,
      "grad_norm": 5.635714054107666,
      "learning_rate": 1.443454345648252e-05,
      "loss": 0.1924,
      "num_input_tokens_seen": 1115056,
      "step": 675
    },
    {
      "epoch": 1.110016420361248,
      "grad_norm": 6.370663642883301,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 0.1686,
      "num_input_tokens_seen": 1116592,
      "step": 676
    },
    {
      "epoch": 1.1116584564860428,
      "grad_norm": 9.749847412109375,
      "learning_rate": 1.4276621664921357e-05,
      "loss": 0.3166,
      "num_input_tokens_seen": 1118384,
      "step": 677
    },
    {
      "epoch": 1.1133004926108374,
      "grad_norm": 10.593022346496582,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 0.3232,
      "num_input_tokens_seen": 1119920,
      "step": 678
    },
    {
      "epoch": 1.1149425287356323,
      "grad_norm": 6.292352199554443,
      "learning_rate": 1.4119222515691816e-05,
      "loss": 0.1781,
      "num_input_tokens_seen": 1121968,
      "step": 679
    },
    {
      "epoch": 1.116584564860427,
      "grad_norm": 14.802712440490723,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.7355,
      "num_input_tokens_seen": 1123504,
      "step": 680
    },
    {
      "epoch": 1.1182266009852218,
      "grad_norm": 6.438601016998291,
      "learning_rate": 1.3962353680206373e-05,
      "loss": 0.2235,
      "num_input_tokens_seen": 1125040,
      "step": 681
    },
    {
      "epoch": 1.1198686371100164,
      "grad_norm": 6.275203704833984,
      "learning_rate": 1.388412052037682e-05,
      "loss": 0.3228,
      "num_input_tokens_seen": 1126576,
      "step": 682
    },
    {
      "epoch": 1.1215106732348112,
      "grad_norm": 12.625669479370117,
      "learning_rate": 1.380602280403076e-05,
      "loss": 0.4197,
      "num_input_tokens_seen": 1128368,
      "step": 683
    },
    {
      "epoch": 1.1231527093596059,
      "grad_norm": 9.24632740020752,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 0.3287,
      "num_input_tokens_seen": 1129904,
      "step": 684
    },
    {
      "epoch": 1.1247947454844007,
      "grad_norm": 15.16049861907959,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 0.746,
      "num_input_tokens_seen": 1131440,
      "step": 685
    },
    {
      "epoch": 1.1264367816091954,
      "grad_norm": 14.770515441894531,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 0.3981,
      "num_input_tokens_seen": 1133232,
      "step": 686
    },
    {
      "epoch": 1.1280788177339902,
      "grad_norm": 5.776052951812744,
      "learning_rate": 1.349500538040371e-05,
      "loss": 0.1065,
      "num_input_tokens_seen": 1134768,
      "step": 687
    },
    {
      "epoch": 1.1297208538587848,
      "grad_norm": 15.373254776000977,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 0.4953,
      "num_input_tokens_seen": 1136304,
      "step": 688
    },
    {
      "epoch": 1.1313628899835797,
      "grad_norm": 13.391886711120605,
      "learning_rate": 1.3340333991502724e-05,
      "loss": 0.5036,
      "num_input_tokens_seen": 1137840,
      "step": 689
    },
    {
      "epoch": 1.1330049261083743,
      "grad_norm": 19.07480239868164,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 0.7534,
      "num_input_tokens_seen": 1139888,
      "step": 690
    },
    {
      "epoch": 1.1346469622331692,
      "grad_norm": 16.428300857543945,
      "learning_rate": 1.3186230878273653e-05,
      "loss": 0.3988,
      "num_input_tokens_seen": 1141424,
      "step": 691
    },
    {
      "epoch": 1.1362889983579638,
      "grad_norm": 17.48094940185547,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 0.563,
      "num_input_tokens_seen": 1142960,
      "step": 692
    },
    {
      "epoch": 1.1379310344827587,
      "grad_norm": 13.989459991455078,
      "learning_rate": 1.3032703551484832e-05,
      "loss": 0.4104,
      "num_input_tokens_seen": 1144752,
      "step": 693
    },
    {
      "epoch": 1.1395730706075533,
      "grad_norm": 16.62628173828125,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 0.6013,
      "num_input_tokens_seen": 1146288,
      "step": 694
    },
    {
      "epoch": 1.1412151067323482,
      "grad_norm": 17.06060028076172,
      "learning_rate": 1.2879759493841575e-05,
      "loss": 0.8965,
      "num_input_tokens_seen": 1147824,
      "step": 695
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 11.480998039245605,
      "learning_rate": 1.280350852153168e-05,
      "loss": 0.5835,
      "num_input_tokens_seen": 1149360,
      "step": 696
    },
    {
      "epoch": 1.1444991789819376,
      "grad_norm": 10.89837646484375,
      "learning_rate": 1.272740615962148e-05,
      "loss": 0.4243,
      "num_input_tokens_seen": 1150896,
      "step": 697
    },
    {
      "epoch": 1.1461412151067323,
      "grad_norm": 15.836161613464355,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 1.0487,
      "num_input_tokens_seen": 1152432,
      "step": 698
    },
    {
      "epoch": 1.1477832512315271,
      "grad_norm": 18.879961013793945,
      "learning_rate": 1.2575650974311119e-05,
      "loss": 1.0721,
      "num_input_tokens_seen": 1154224,
      "step": 699
    },
    {
      "epoch": 1.1494252873563218,
      "grad_norm": 11.448482513427734,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 0.3711,
      "num_input_tokens_seen": 1155760,
      "step": 700
    },
    {
      "epoch": 1.1510673234811166,
      "grad_norm": 7.1345038414001465,
      "learning_rate": 1.2424501334244123e-05,
      "loss": 0.2084,
      "num_input_tokens_seen": 1157552,
      "step": 701
    },
    {
      "epoch": 1.1527093596059113,
      "grad_norm": 8.997842788696289,
      "learning_rate": 1.234915589697091e-05,
      "loss": 0.5005,
      "num_input_tokens_seen": 1159088,
      "step": 702
    },
    {
      "epoch": 1.154351395730706,
      "grad_norm": 12.306397438049316,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 0.6133,
      "num_input_tokens_seen": 1160624,
      "step": 703
    },
    {
      "epoch": 1.1559934318555007,
      "grad_norm": 17.858766555786133,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 0.6082,
      "num_input_tokens_seen": 1162160,
      "step": 704
    },
    {
      "epoch": 1.1576354679802956,
      "grad_norm": 11.423599243164062,
      "learning_rate": 1.2124048127248644e-05,
      "loss": 0.6453,
      "num_input_tokens_seen": 1163696,
      "step": 705
    },
    {
      "epoch": 1.1592775041050902,
      "grad_norm": 16.09710121154785,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 0.6171,
      "num_input_tokens_seen": 1165232,
      "step": 706
    },
    {
      "epoch": 1.160919540229885,
      "grad_norm": 7.116977691650391,
      "learning_rate": 1.19747592039856e-05,
      "loss": 0.394,
      "num_input_tokens_seen": 1167024,
      "step": 707
    },
    {
      "epoch": 1.1625615763546797,
      "grad_norm": 8.746833801269531,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 0.538,
      "num_input_tokens_seen": 1168560,
      "step": 708
    },
    {
      "epoch": 1.1642036124794746,
      "grad_norm": 8.378342628479004,
      "learning_rate": 1.1826105112583061e-05,
      "loss": 0.3283,
      "num_input_tokens_seen": 1170096,
      "step": 709
    },
    {
      "epoch": 1.1658456486042692,
      "grad_norm": 15.409492492675781,
      "learning_rate": 1.175201839416988e-05,
      "loss": 0.7525,
      "num_input_tokens_seen": 1171888,
      "step": 710
    },
    {
      "epoch": 1.167487684729064,
      "grad_norm": 6.538908958435059,
      "learning_rate": 1.167809309823175e-05,
      "loss": 0.3172,
      "num_input_tokens_seen": 1173424,
      "step": 711
    },
    {
      "epoch": 1.1691297208538587,
      "grad_norm": 10.155729293823242,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 0.4668,
      "num_input_tokens_seen": 1175216,
      "step": 712
    },
    {
      "epoch": 1.1707717569786535,
      "grad_norm": 6.618157863616943,
      "learning_rate": 1.1530730374828422e-05,
      "loss": 0.201,
      "num_input_tokens_seen": 1176752,
      "step": 713
    },
    {
      "epoch": 1.1724137931034484,
      "grad_norm": 7.287567138671875,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 0.2315,
      "num_input_tokens_seen": 1178288,
      "step": 714
    },
    {
      "epoch": 1.174055829228243,
      "grad_norm": 8.044340133666992,
      "learning_rate": 1.1384024124624324e-05,
      "loss": 0.3546,
      "num_input_tokens_seen": 1179824,
      "step": 715
    },
    {
      "epoch": 1.1756978653530377,
      "grad_norm": 9.45065975189209,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 0.3704,
      "num_input_tokens_seen": 1181360,
      "step": 716
    },
    {
      "epoch": 1.1773399014778325,
      "grad_norm": 11.264669418334961,
      "learning_rate": 1.123798149787511e-05,
      "loss": 0.4183,
      "num_input_tokens_seen": 1182896,
      "step": 717
    },
    {
      "epoch": 1.1789819376026274,
      "grad_norm": 7.788905143737793,
      "learning_rate": 1.11652112689164e-05,
      "loss": 0.2784,
      "num_input_tokens_seen": 1184432,
      "step": 718
    },
    {
      "epoch": 1.180623973727422,
      "grad_norm": 7.910073280334473,
      "learning_rate": 1.109260961249238e-05,
      "loss": 0.2743,
      "num_input_tokens_seen": 1185968,
      "step": 719
    },
    {
      "epoch": 1.1822660098522166,
      "grad_norm": 7.054251194000244,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 0.23,
      "num_input_tokens_seen": 1187760,
      "step": 720
    },
    {
      "epoch": 1.1839080459770115,
      "grad_norm": 9.139023780822754,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 0.2716,
      "num_input_tokens_seen": 1189296,
      "step": 721
    },
    {
      "epoch": 1.1855500821018063,
      "grad_norm": 11.179971694946289,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 0.4236,
      "num_input_tokens_seen": 1190832,
      "step": 722
    },
    {
      "epoch": 1.187192118226601,
      "grad_norm": 10.771069526672363,
      "learning_rate": 1.0803906373672476e-05,
      "loss": 0.7605,
      "num_input_tokens_seen": 1192368,
      "step": 723
    },
    {
      "epoch": 1.1888341543513956,
      "grad_norm": 5.012805938720703,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 0.1407,
      "num_input_tokens_seen": 1194160,
      "step": 724
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 11.519990921020508,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 0.3487,
      "num_input_tokens_seen": 1195696,
      "step": 725
    },
    {
      "epoch": 1.1921182266009853,
      "grad_norm": 9.319905281066895,
      "learning_rate": 1.058919209575517e-05,
      "loss": 0.3924,
      "num_input_tokens_seen": 1197744,
      "step": 726
    },
    {
      "epoch": 1.19376026272578,
      "grad_norm": 15.698502540588379,
      "learning_rate": 1.0517970691433035e-05,
      "loss": 0.6796,
      "num_input_tokens_seen": 1199280,
      "step": 727
    },
    {
      "epoch": 1.1954022988505748,
      "grad_norm": 14.967554092407227,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 0.5887,
      "num_input_tokens_seen": 1200816,
      "step": 728
    },
    {
      "epoch": 1.1970443349753694,
      "grad_norm": 12.271068572998047,
      "learning_rate": 1.0376058125319613e-05,
      "loss": 0.6263,
      "num_input_tokens_seen": 1202608,
      "step": 729
    },
    {
      "epoch": 1.1986863711001643,
      "grad_norm": 17.353784561157227,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 0.6724,
      "num_input_tokens_seen": 1204400,
      "step": 730
    },
    {
      "epoch": 1.200328407224959,
      "grad_norm": 13.904858589172363,
      "learning_rate": 1.0234858309501862e-05,
      "loss": 0.5141,
      "num_input_tokens_seen": 1206192,
      "step": 731
    },
    {
      "epoch": 1.2019704433497538,
      "grad_norm": 9.663973808288574,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 0.2542,
      "num_input_tokens_seen": 1207728,
      "step": 732
    },
    {
      "epoch": 1.2036124794745484,
      "grad_norm": 11.417986869812012,
      "learning_rate": 1.0094378125859602e-05,
      "loss": 0.4863,
      "num_input_tokens_seen": 1209520,
      "step": 733
    },
    {
      "epoch": 1.2052545155993433,
      "grad_norm": 13.04702377319336,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 0.4561,
      "num_input_tokens_seen": 1211056,
      "step": 734
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 12.643677711486816,
      "learning_rate": 9.954624421198792e-06,
      "loss": 0.8889,
      "num_input_tokens_seen": 1212592,
      "step": 735
    },
    {
      "epoch": 1.2085385878489328,
      "grad_norm": 14.470638275146484,
      "learning_rate": 9.88502212844063e-06,
      "loss": 0.6565,
      "num_input_tokens_seen": 1214128,
      "step": 736
    },
    {
      "epoch": 1.2101806239737274,
      "grad_norm": 10.083754539489746,
      "learning_rate": 9.815604006917839e-06,
      "loss": 0.3427,
      "num_input_tokens_seen": 1215664,
      "step": 737
    },
    {
      "epoch": 1.2118226600985222,
      "grad_norm": 12.365099906921387,
      "learning_rate": 9.746370902468311e-06,
      "loss": 0.3837,
      "num_input_tokens_seen": 1217200,
      "step": 738
    },
    {
      "epoch": 1.2134646962233169,
      "grad_norm": 12.048296928405762,
      "learning_rate": 9.677323658675594e-06,
      "loss": 0.7697,
      "num_input_tokens_seen": 1218992,
      "step": 739
    },
    {
      "epoch": 1.2151067323481117,
      "grad_norm": 19.542001724243164,
      "learning_rate": 9.608463116858542e-06,
      "loss": 0.7138,
      "num_input_tokens_seen": 1220784,
      "step": 740
    },
    {
      "epoch": 1.2167487684729064,
      "grad_norm": 9.516098022460938,
      "learning_rate": 9.539790116061151e-06,
      "loss": 0.4881,
      "num_input_tokens_seen": 1222320,
      "step": 741
    },
    {
      "epoch": 1.2183908045977012,
      "grad_norm": 8.150201797485352,
      "learning_rate": 9.471305493042243e-06,
      "loss": 0.3592,
      "num_input_tokens_seen": 1223856,
      "step": 742
    },
    {
      "epoch": 1.2200328407224958,
      "grad_norm": 12.1910400390625,
      "learning_rate": 9.403010082265351e-06,
      "loss": 0.4662,
      "num_input_tokens_seen": 1225392,
      "step": 743
    },
    {
      "epoch": 1.2216748768472907,
      "grad_norm": 10.272061347961426,
      "learning_rate": 9.334904715888495e-06,
      "loss": 0.3269,
      "num_input_tokens_seen": 1227184,
      "step": 744
    },
    {
      "epoch": 1.2233169129720853,
      "grad_norm": 6.172972679138184,
      "learning_rate": 9.266990223754069e-06,
      "loss": 0.26,
      "num_input_tokens_seen": 1228720,
      "step": 745
    },
    {
      "epoch": 1.2249589490968802,
      "grad_norm": 15.312021255493164,
      "learning_rate": 9.199267433378727e-06,
      "loss": 0.72,
      "num_input_tokens_seen": 1230512,
      "step": 746
    },
    {
      "epoch": 1.2266009852216748,
      "grad_norm": 12.497981071472168,
      "learning_rate": 9.131737169943314e-06,
      "loss": 0.5268,
      "num_input_tokens_seen": 1232048,
      "step": 747
    },
    {
      "epoch": 1.2282430213464697,
      "grad_norm": 14.313923835754395,
      "learning_rate": 9.064400256282757e-06,
      "loss": 0.7411,
      "num_input_tokens_seen": 1233840,
      "step": 748
    },
    {
      "epoch": 1.2298850574712643,
      "grad_norm": 13.238240242004395,
      "learning_rate": 8.997257512876108e-06,
      "loss": 0.4827,
      "num_input_tokens_seen": 1235632,
      "step": 749
    },
    {
      "epoch": 1.2315270935960592,
      "grad_norm": 6.712271690368652,
      "learning_rate": 8.930309757836517e-06,
      "loss": 0.2396,
      "num_input_tokens_seen": 1237424,
      "step": 750
    },
    {
      "epoch": 1.2331691297208538,
      "grad_norm": 14.578856468200684,
      "learning_rate": 8.863557806901233e-06,
      "loss": 0.4331,
      "num_input_tokens_seen": 1238960,
      "step": 751
    },
    {
      "epoch": 1.2348111658456487,
      "grad_norm": 13.390666007995605,
      "learning_rate": 8.797002473421728e-06,
      "loss": 0.5618,
      "num_input_tokens_seen": 1240752,
      "step": 752
    },
    {
      "epoch": 1.2364532019704433,
      "grad_norm": 5.404168605804443,
      "learning_rate": 8.73064456835373e-06,
      "loss": 0.1809,
      "num_input_tokens_seen": 1242288,
      "step": 753
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 3.6469039916992188,
      "learning_rate": 8.664484900247363e-06,
      "loss": 0.0872,
      "num_input_tokens_seen": 1243824,
      "step": 754
    },
    {
      "epoch": 1.2397372742200328,
      "grad_norm": 6.652228355407715,
      "learning_rate": 8.598524275237322e-06,
      "loss": 0.1946,
      "num_input_tokens_seen": 1245360,
      "step": 755
    },
    {
      "epoch": 1.2413793103448276,
      "grad_norm": 11.932924270629883,
      "learning_rate": 8.532763497032987e-06,
      "loss": 0.5665,
      "num_input_tokens_seen": 1247152,
      "step": 756
    },
    {
      "epoch": 1.2430213464696223,
      "grad_norm": 9.027915000915527,
      "learning_rate": 8.467203366908707e-06,
      "loss": 0.2881,
      "num_input_tokens_seen": 1248944,
      "step": 757
    },
    {
      "epoch": 1.2446633825944171,
      "grad_norm": 11.19984245300293,
      "learning_rate": 8.40184468369396e-06,
      "loss": 0.5331,
      "num_input_tokens_seen": 1250480,
      "step": 758
    },
    {
      "epoch": 1.2463054187192117,
      "grad_norm": 9.139632225036621,
      "learning_rate": 8.33668824376369e-06,
      "loss": 0.4025,
      "num_input_tokens_seen": 1252016,
      "step": 759
    },
    {
      "epoch": 1.2479474548440066,
      "grad_norm": 4.899367809295654,
      "learning_rate": 8.271734841028553e-06,
      "loss": 0.1731,
      "num_input_tokens_seen": 1253552,
      "step": 760
    },
    {
      "epoch": 1.2495894909688012,
      "grad_norm": 15.279624938964844,
      "learning_rate": 8.206985266925249e-06,
      "loss": 0.8809,
      "num_input_tokens_seen": 1255088,
      "step": 761
    },
    {
      "epoch": 1.251231527093596,
      "grad_norm": 8.74679183959961,
      "learning_rate": 8.142440310406924e-06,
      "loss": 0.5093,
      "num_input_tokens_seen": 1256624,
      "step": 762
    },
    {
      "epoch": 1.2528735632183907,
      "grad_norm": 15.51058292388916,
      "learning_rate": 8.078100757933485e-06,
      "loss": 0.6015,
      "num_input_tokens_seen": 1258416,
      "step": 763
    },
    {
      "epoch": 1.2545155993431856,
      "grad_norm": 3.7967443466186523,
      "learning_rate": 8.013967393462094e-06,
      "loss": 0.0916,
      "num_input_tokens_seen": 1259952,
      "step": 764
    },
    {
      "epoch": 1.2561576354679804,
      "grad_norm": 8.253113746643066,
      "learning_rate": 7.950040998437542e-06,
      "loss": 0.3879,
      "num_input_tokens_seen": 1261744,
      "step": 765
    },
    {
      "epoch": 1.257799671592775,
      "grad_norm": 9.477970123291016,
      "learning_rate": 7.886322351782783e-06,
      "loss": 0.4113,
      "num_input_tokens_seen": 1263280,
      "step": 766
    },
    {
      "epoch": 1.2594417077175697,
      "grad_norm": 7.282094955444336,
      "learning_rate": 7.822812229889428e-06,
      "loss": 0.3608,
      "num_input_tokens_seen": 1265072,
      "step": 767
    },
    {
      "epoch": 1.2610837438423645,
      "grad_norm": 13.237692832946777,
      "learning_rate": 7.759511406608255e-06,
      "loss": 0.6477,
      "num_input_tokens_seen": 1266608,
      "step": 768
    },
    {
      "epoch": 1.2627257799671594,
      "grad_norm": 15.345897674560547,
      "learning_rate": 7.696420653239833e-06,
      "loss": 0.8691,
      "num_input_tokens_seen": 1268144,
      "step": 769
    },
    {
      "epoch": 1.264367816091954,
      "grad_norm": 19.76552391052246,
      "learning_rate": 7.633540738525066e-06,
      "loss": 1.0555,
      "num_input_tokens_seen": 1269936,
      "step": 770
    },
    {
      "epoch": 1.2660098522167487,
      "grad_norm": 6.838841438293457,
      "learning_rate": 7.570872428635889e-06,
      "loss": 0.2459,
      "num_input_tokens_seen": 1271472,
      "step": 771
    },
    {
      "epoch": 1.2676518883415435,
      "grad_norm": 10.303356170654297,
      "learning_rate": 7.508416487165862e-06,
      "loss": 0.4664,
      "num_input_tokens_seen": 1273008,
      "step": 772
    },
    {
      "epoch": 1.2692939244663384,
      "grad_norm": 5.657759189605713,
      "learning_rate": 7.4461736751209405e-06,
      "loss": 0.2243,
      "num_input_tokens_seen": 1274800,
      "step": 773
    },
    {
      "epoch": 1.270935960591133,
      "grad_norm": 15.812253952026367,
      "learning_rate": 7.384144750910133e-06,
      "loss": 0.6955,
      "num_input_tokens_seen": 1276336,
      "step": 774
    },
    {
      "epoch": 1.2725779967159276,
      "grad_norm": 17.634485244750977,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 0.5859,
      "num_input_tokens_seen": 1277872,
      "step": 775
    },
    {
      "epoch": 1.2742200328407225,
      "grad_norm": 19.16039276123047,
      "learning_rate": 7.260731586586983e-06,
      "loss": 0.6942,
      "num_input_tokens_seen": 1279920,
      "step": 776
    },
    {
      "epoch": 1.2758620689655173,
      "grad_norm": 12.211969375610352,
      "learning_rate": 7.19934885022509e-06,
      "loss": 0.7186,
      "num_input_tokens_seen": 1281712,
      "step": 777
    },
    {
      "epoch": 1.277504105090312,
      "grad_norm": 11.468416213989258,
      "learning_rate": 7.138183009179922e-06,
      "loss": 0.4064,
      "num_input_tokens_seen": 1283504,
      "step": 778
    },
    {
      "epoch": 1.2791461412151066,
      "grad_norm": 9.000165939331055,
      "learning_rate": 7.0772348087379315e-06,
      "loss": 0.4104,
      "num_input_tokens_seen": 1285040,
      "step": 779
    },
    {
      "epoch": 1.2807881773399015,
      "grad_norm": 9.836174964904785,
      "learning_rate": 7.016504991533726e-06,
      "loss": 0.8311,
      "num_input_tokens_seen": 1286832,
      "step": 780
    },
    {
      "epoch": 1.2824302134646963,
      "grad_norm": 9.952849388122559,
      "learning_rate": 6.9559942975409465e-06,
      "loss": 0.384,
      "num_input_tokens_seen": 1288368,
      "step": 781
    },
    {
      "epoch": 1.284072249589491,
      "grad_norm": 7.034417152404785,
      "learning_rate": 6.895703464063319e-06,
      "loss": 0.2789,
      "num_input_tokens_seen": 1289904,
      "step": 782
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 7.474240779876709,
      "learning_rate": 6.835633225725605e-06,
      "loss": 0.1994,
      "num_input_tokens_seen": 1291440,
      "step": 783
    },
    {
      "epoch": 1.2873563218390804,
      "grad_norm": 7.071294784545898,
      "learning_rate": 6.775784314464717e-06,
      "loss": 0.2221,
      "num_input_tokens_seen": 1292976,
      "step": 784
    },
    {
      "epoch": 1.2889983579638753,
      "grad_norm": 10.379705429077148,
      "learning_rate": 6.716157459520739e-06,
      "loss": 0.4279,
      "num_input_tokens_seen": 1294768,
      "step": 785
    },
    {
      "epoch": 1.29064039408867,
      "grad_norm": 12.010738372802734,
      "learning_rate": 6.656753387428089e-06,
      "loss": 0.8367,
      "num_input_tokens_seen": 1296304,
      "step": 786
    },
    {
      "epoch": 1.2922824302134646,
      "grad_norm": 13.891785621643066,
      "learning_rate": 6.5975728220066425e-06,
      "loss": 0.4233,
      "num_input_tokens_seen": 1297840,
      "step": 787
    },
    {
      "epoch": 1.2939244663382594,
      "grad_norm": 3.887078046798706,
      "learning_rate": 6.538616484352902e-06,
      "loss": 0.0487,
      "num_input_tokens_seen": 1299376,
      "step": 788
    },
    {
      "epoch": 1.2955665024630543,
      "grad_norm": 13.42442798614502,
      "learning_rate": 6.47988509283125e-06,
      "loss": 0.7107,
      "num_input_tokens_seen": 1301168,
      "step": 789
    },
    {
      "epoch": 1.297208538587849,
      "grad_norm": 8.19041919708252,
      "learning_rate": 6.421379363065142e-06,
      "loss": 0.3486,
      "num_input_tokens_seen": 1302704,
      "step": 790
    },
    {
      "epoch": 1.2988505747126438,
      "grad_norm": 4.3488993644714355,
      "learning_rate": 6.363100007928446e-06,
      "loss": 0.1181,
      "num_input_tokens_seen": 1304240,
      "step": 791
    },
    {
      "epoch": 1.3004926108374384,
      "grad_norm": 3.0889177322387695,
      "learning_rate": 6.305047737536707e-06,
      "loss": 0.0548,
      "num_input_tokens_seen": 1306032,
      "step": 792
    },
    {
      "epoch": 1.3021346469622332,
      "grad_norm": 13.680887222290039,
      "learning_rate": 6.247223259238511e-06,
      "loss": 0.4375,
      "num_input_tokens_seen": 1307568,
      "step": 793
    },
    {
      "epoch": 1.3037766830870279,
      "grad_norm": 7.17772912979126,
      "learning_rate": 6.189627277606894e-06,
      "loss": 0.1511,
      "num_input_tokens_seen": 1309360,
      "step": 794
    },
    {
      "epoch": 1.3054187192118227,
      "grad_norm": 5.28561544418335,
      "learning_rate": 6.1322604944307e-06,
      "loss": 0.1633,
      "num_input_tokens_seen": 1310896,
      "step": 795
    },
    {
      "epoch": 1.3070607553366174,
      "grad_norm": 15.555086135864258,
      "learning_rate": 6.075123608706093e-06,
      "loss": 0.8482,
      "num_input_tokens_seen": 1312688,
      "step": 796
    },
    {
      "epoch": 1.3087027914614122,
      "grad_norm": 11.86722469329834,
      "learning_rate": 6.01821731662798e-06,
      "loss": 0.5187,
      "num_input_tokens_seen": 1314224,
      "step": 797
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 10.418692588806152,
      "learning_rate": 5.961542311581586e-06,
      "loss": 0.4253,
      "num_input_tokens_seen": 1315760,
      "step": 798
    },
    {
      "epoch": 1.3119868637110017,
      "grad_norm": 10.174422264099121,
      "learning_rate": 5.905099284133952e-06,
      "loss": 0.5501,
      "num_input_tokens_seen": 1317552,
      "step": 799
    },
    {
      "epoch": 1.3136288998357963,
      "grad_norm": 12.246493339538574,
      "learning_rate": 5.848888922025553e-06,
      "loss": 0.7256,
      "num_input_tokens_seen": 1319088,
      "step": 800
    },
    {
      "epoch": 1.3152709359605912,
      "grad_norm": 7.137866497039795,
      "learning_rate": 5.792911910161922e-06,
      "loss": 0.2147,
      "num_input_tokens_seen": 1320624,
      "step": 801
    },
    {
      "epoch": 1.3169129720853858,
      "grad_norm": 4.902153491973877,
      "learning_rate": 5.737168930605272e-06,
      "loss": 0.1787,
      "num_input_tokens_seen": 1322160,
      "step": 802
    },
    {
      "epoch": 1.3185550082101807,
      "grad_norm": 7.781048774719238,
      "learning_rate": 5.681660662566224e-06,
      "loss": 0.2895,
      "num_input_tokens_seen": 1323952,
      "step": 803
    },
    {
      "epoch": 1.3201970443349753,
      "grad_norm": 5.868434429168701,
      "learning_rate": 5.626387782395512e-06,
      "loss": 0.2464,
      "num_input_tokens_seen": 1325488,
      "step": 804
    },
    {
      "epoch": 1.3218390804597702,
      "grad_norm": 12.775634765625,
      "learning_rate": 5.571350963575728e-06,
      "loss": 0.6065,
      "num_input_tokens_seen": 1327024,
      "step": 805
    },
    {
      "epoch": 1.3234811165845648,
      "grad_norm": 11.698238372802734,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 0.4259,
      "num_input_tokens_seen": 1328816,
      "step": 806
    },
    {
      "epoch": 1.3251231527093597,
      "grad_norm": 9.173810958862305,
      "learning_rate": 5.461988189529529e-06,
      "loss": 0.4527,
      "num_input_tokens_seen": 1330352,
      "step": 807
    },
    {
      "epoch": 1.3267651888341543,
      "grad_norm": 21.813129425048828,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 0.7547,
      "num_input_tokens_seen": 1332400,
      "step": 808
    },
    {
      "epoch": 1.3284072249589491,
      "grad_norm": 13.827738761901855,
      "learning_rate": 5.3535776706149505e-06,
      "loss": 0.666,
      "num_input_tokens_seen": 1333936,
      "step": 809
    },
    {
      "epoch": 1.3300492610837438,
      "grad_norm": 20.906869888305664,
      "learning_rate": 5.299731159831953e-06,
      "loss": 0.5595,
      "num_input_tokens_seen": 1335472,
      "step": 810
    },
    {
      "epoch": 1.3316912972085386,
      "grad_norm": 14.970897674560547,
      "learning_rate": 5.24612469060774e-06,
      "loss": 0.5975,
      "num_input_tokens_seen": 1337264,
      "step": 811
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 4.952927112579346,
      "learning_rate": 5.192758916120236e-06,
      "loss": 0.1058,
      "num_input_tokens_seen": 1338800,
      "step": 812
    },
    {
      "epoch": 1.3349753694581281,
      "grad_norm": 9.111865043640137,
      "learning_rate": 5.139634486614544e-06,
      "loss": 0.2809,
      "num_input_tokens_seen": 1340336,
      "step": 813
    },
    {
      "epoch": 1.3366174055829227,
      "grad_norm": 11.367166519165039,
      "learning_rate": 5.086752049395094e-06,
      "loss": 0.2152,
      "num_input_tokens_seen": 1342384,
      "step": 814
    },
    {
      "epoch": 1.3382594417077176,
      "grad_norm": 15.012801170349121,
      "learning_rate": 5.034112248817685e-06,
      "loss": 0.5223,
      "num_input_tokens_seen": 1344176,
      "step": 815
    },
    {
      "epoch": 1.3399014778325122,
      "grad_norm": 13.386804580688477,
      "learning_rate": 4.981715726281666e-06,
      "loss": 0.537,
      "num_input_tokens_seen": 1345712,
      "step": 816
    },
    {
      "epoch": 1.341543513957307,
      "grad_norm": 14.411375045776367,
      "learning_rate": 4.929563120222141e-06,
      "loss": 0.6943,
      "num_input_tokens_seen": 1347504,
      "step": 817
    },
    {
      "epoch": 1.3431855500821017,
      "grad_norm": 12.643198013305664,
      "learning_rate": 4.877655066102149e-06,
      "loss": 0.6052,
      "num_input_tokens_seen": 1349040,
      "step": 818
    },
    {
      "epoch": 1.3448275862068966,
      "grad_norm": 11.543643951416016,
      "learning_rate": 4.825992196404957e-06,
      "loss": 0.4768,
      "num_input_tokens_seen": 1350832,
      "step": 819
    },
    {
      "epoch": 1.3464696223316914,
      "grad_norm": 10.272476196289062,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 0.5065,
      "num_input_tokens_seen": 1352624,
      "step": 820
    },
    {
      "epoch": 1.348111658456486,
      "grad_norm": 14.009817123413086,
      "learning_rate": 4.723404525266839e-06,
      "loss": 0.5182,
      "num_input_tokens_seen": 1354160,
      "step": 821
    },
    {
      "epoch": 1.3497536945812807,
      "grad_norm": 8.8602933883667,
      "learning_rate": 4.672480973824311e-06,
      "loss": 0.2093,
      "num_input_tokens_seen": 1355696,
      "step": 822
    },
    {
      "epoch": 1.3513957307060755,
      "grad_norm": 11.652544975280762,
      "learning_rate": 4.621805106786142e-06,
      "loss": 0.5523,
      "num_input_tokens_seen": 1357232,
      "step": 823
    },
    {
      "epoch": 1.3530377668308704,
      "grad_norm": 7.522380352020264,
      "learning_rate": 4.571377541621788e-06,
      "loss": 0.2588,
      "num_input_tokens_seen": 1359024,
      "step": 824
    },
    {
      "epoch": 1.354679802955665,
      "grad_norm": 14.571446418762207,
      "learning_rate": 4.521198892775203e-06,
      "loss": 0.8194,
      "num_input_tokens_seen": 1360816,
      "step": 825
    },
    {
      "epoch": 1.3563218390804597,
      "grad_norm": 11.0194730758667,
      "learning_rate": 4.4712697716574e-06,
      "loss": 0.3692,
      "num_input_tokens_seen": 1362352,
      "step": 826
    },
    {
      "epoch": 1.3579638752052545,
      "grad_norm": 17.428213119506836,
      "learning_rate": 4.421590786638951e-06,
      "loss": 0.1621,
      "num_input_tokens_seen": 1363888,
      "step": 827
    },
    {
      "epoch": 1.3596059113300494,
      "grad_norm": 17.750167846679688,
      "learning_rate": 4.372162543042624e-06,
      "loss": 0.7496,
      "num_input_tokens_seen": 1365680,
      "step": 828
    },
    {
      "epoch": 1.361247947454844,
      "grad_norm": 11.92014217376709,
      "learning_rate": 4.322985643135952e-06,
      "loss": 0.5359,
      "num_input_tokens_seen": 1367216,
      "step": 829
    },
    {
      "epoch": 1.3628899835796386,
      "grad_norm": 13.709952354431152,
      "learning_rate": 4.274060686123959e-06,
      "loss": 0.5879,
      "num_input_tokens_seen": 1368752,
      "step": 830
    },
    {
      "epoch": 1.3645320197044335,
      "grad_norm": 11.457778930664062,
      "learning_rate": 4.225388268141797e-06,
      "loss": 0.3491,
      "num_input_tokens_seen": 1370288,
      "step": 831
    },
    {
      "epoch": 1.3661740558292284,
      "grad_norm": 10.937066078186035,
      "learning_rate": 4.176968982247514e-06,
      "loss": 0.2882,
      "num_input_tokens_seen": 1371824,
      "step": 832
    },
    {
      "epoch": 1.367816091954023,
      "grad_norm": 23.902240753173828,
      "learning_rate": 4.128803418414839e-06,
      "loss": 0.5576,
      "num_input_tokens_seen": 1373360,
      "step": 833
    },
    {
      "epoch": 1.3694581280788176,
      "grad_norm": 22.908794403076172,
      "learning_rate": 4.08089216352596e-06,
      "loss": 1.1125,
      "num_input_tokens_seen": 1375152,
      "step": 834
    },
    {
      "epoch": 1.3711001642036125,
      "grad_norm": 11.424241065979004,
      "learning_rate": 4.0332358013644016e-06,
      "loss": 0.3549,
      "num_input_tokens_seen": 1376944,
      "step": 835
    },
    {
      "epoch": 1.3727422003284073,
      "grad_norm": 11.154426574707031,
      "learning_rate": 3.985834912607894e-06,
      "loss": 0.2483,
      "num_input_tokens_seen": 1378736,
      "step": 836
    },
    {
      "epoch": 1.374384236453202,
      "grad_norm": 15.514876365661621,
      "learning_rate": 3.938690074821313e-06,
      "loss": 0.7533,
      "num_input_tokens_seen": 1380272,
      "step": 837
    },
    {
      "epoch": 1.3760262725779966,
      "grad_norm": 13.346345901489258,
      "learning_rate": 3.891801862449629e-06,
      "loss": 0.5539,
      "num_input_tokens_seen": 1381808,
      "step": 838
    },
    {
      "epoch": 1.3776683087027914,
      "grad_norm": 13.258369445800781,
      "learning_rate": 3.845170846810902e-06,
      "loss": 0.4398,
      "num_input_tokens_seen": 1383600,
      "step": 839
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 23.665481567382812,
      "learning_rate": 3.798797596089351e-06,
      "loss": 1.3282,
      "num_input_tokens_seen": 1385136,
      "step": 840
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 16.240568161010742,
      "learning_rate": 3.752682675328406e-06,
      "loss": 0.5273,
      "num_input_tokens_seen": 1386672,
      "step": 841
    },
    {
      "epoch": 1.3825944170771756,
      "grad_norm": 11.282292366027832,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 0.5129,
      "num_input_tokens_seen": 1388208,
      "step": 842
    },
    {
      "epoch": 1.3842364532019704,
      "grad_norm": 5.073177337646484,
      "learning_rate": 3.661230068116811e-06,
      "loss": 0.1345,
      "num_input_tokens_seen": 1390000,
      "step": 843
    },
    {
      "epoch": 1.3858784893267653,
      "grad_norm": 9.341377258300781,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 0.345,
      "num_input_tokens_seen": 1391792,
      "step": 844
    },
    {
      "epoch": 1.38752052545156,
      "grad_norm": 8.567121505737305,
      "learning_rate": 3.5708174824471947e-06,
      "loss": 0.3376,
      "num_input_tokens_seen": 1393328,
      "step": 845
    },
    {
      "epoch": 1.3891625615763548,
      "grad_norm": 7.012502670288086,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 0.2254,
      "num_input_tokens_seen": 1395120,
      "step": 846
    },
    {
      "epoch": 1.3908045977011494,
      "grad_norm": 9.676634788513184,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 0.2539,
      "num_input_tokens_seen": 1396656,
      "step": 847
    },
    {
      "epoch": 1.3924466338259442,
      "grad_norm": 10.016961097717285,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 0.2747,
      "num_input_tokens_seen": 1398192,
      "step": 848
    },
    {
      "epoch": 1.3940886699507389,
      "grad_norm": 5.444889545440674,
      "learning_rate": 3.393129951157384e-06,
      "loss": 0.2104,
      "num_input_tokens_seen": 1399728,
      "step": 849
    },
    {
      "epoch": 1.3957307060755337,
      "grad_norm": 10.936053276062012,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.5466,
      "num_input_tokens_seen": 1401264,
      "step": 850
    },
    {
      "epoch": 1.3973727422003284,
      "grad_norm": 5.960758209228516,
      "learning_rate": 3.305863665776793e-06,
      "loss": 0.1502,
      "num_input_tokens_seen": 1403312,
      "step": 851
    },
    {
      "epoch": 1.3990147783251232,
      "grad_norm": 6.862599849700928,
      "learning_rate": 3.262626762369525e-06,
      "loss": 0.1954,
      "num_input_tokens_seen": 1404848,
      "step": 852
    },
    {
      "epoch": 1.4006568144499179,
      "grad_norm": 15.996443748474121,
      "learning_rate": 3.219654721995266e-06,
      "loss": 0.7425,
      "num_input_tokens_seen": 1406384,
      "step": 853
    },
    {
      "epoch": 1.4022988505747127,
      "grad_norm": 15.003381729125977,
      "learning_rate": 3.176948068254762e-06,
      "loss": 0.609,
      "num_input_tokens_seen": 1408176,
      "step": 854
    },
    {
      "epoch": 1.4039408866995073,
      "grad_norm": 9.772607803344727,
      "learning_rate": 3.1345073215151066e-06,
      "loss": 0.4062,
      "num_input_tokens_seen": 1409712,
      "step": 855
    },
    {
      "epoch": 1.4055829228243022,
      "grad_norm": 21.340167999267578,
      "learning_rate": 3.092332998903416e-06,
      "loss": 0.8085,
      "num_input_tokens_seen": 1411504,
      "step": 856
    },
    {
      "epoch": 1.4072249589490968,
      "grad_norm": 10.022299766540527,
      "learning_rate": 3.0504256143004866e-06,
      "loss": 0.3082,
      "num_input_tokens_seen": 1413296,
      "step": 857
    },
    {
      "epoch": 1.4088669950738917,
      "grad_norm": 9.950339317321777,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 0.2786,
      "num_input_tokens_seen": 1415088,
      "step": 858
    },
    {
      "epoch": 1.4105090311986863,
      "grad_norm": 11.098962783813477,
      "learning_rate": 2.967413698375196e-06,
      "loss": 0.4963,
      "num_input_tokens_seen": 1416624,
      "step": 859
    },
    {
      "epoch": 1.4121510673234812,
      "grad_norm": 10.427085876464844,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 0.2798,
      "num_input_tokens_seen": 1418672,
      "step": 860
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 4.981451988220215,
      "learning_rate": 2.8854756196229016e-06,
      "loss": 0.1459,
      "num_input_tokens_seen": 1420464,
      "step": 861
    },
    {
      "epoch": 1.4154351395730707,
      "grad_norm": 17.60355567932129,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 0.9818,
      "num_input_tokens_seen": 1422000,
      "step": 862
    },
    {
      "epoch": 1.4170771756978653,
      "grad_norm": 20.547264099121094,
      "learning_rate": 2.8046153715899692e-06,
      "loss": 0.7161,
      "num_input_tokens_seen": 1423536,
      "step": 863
    },
    {
      "epoch": 1.4187192118226601,
      "grad_norm": 11.45111083984375,
      "learning_rate": 2.764590667717562e-06,
      "loss": 0.4153,
      "num_input_tokens_seen": 1425328,
      "step": 864
    },
    {
      "epoch": 1.4203612479474548,
      "grad_norm": 15.28264331817627,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 0.74,
      "num_input_tokens_seen": 1426864,
      "step": 865
    },
    {
      "epoch": 1.4220032840722496,
      "grad_norm": 9.890094757080078,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 0.4052,
      "num_input_tokens_seen": 1428400,
      "step": 866
    },
    {
      "epoch": 1.4236453201970443,
      "grad_norm": 13.916177749633789,
      "learning_rate": 2.646144079015797e-06,
      "loss": 0.5464,
      "num_input_tokens_seen": 1430192,
      "step": 867
    },
    {
      "epoch": 1.4252873563218391,
      "grad_norm": 19.760517120361328,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 0.8375,
      "num_input_tokens_seen": 1431728,
      "step": 868
    },
    {
      "epoch": 1.4269293924466337,
      "grad_norm": 9.679533958435059,
      "learning_rate": 2.5685407581417907e-06,
      "loss": 0.4266,
      "num_input_tokens_seen": 1433264,
      "step": 869
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 7.92029333114624,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 0.4241,
      "num_input_tokens_seen": 1434800,
      "step": 870
    },
    {
      "epoch": 1.4302134646962232,
      "grad_norm": 10.744665145874023,
      "learning_rate": 2.492030714945162e-06,
      "loss": 0.2845,
      "num_input_tokens_seen": 1436336,
      "step": 871
    },
    {
      "epoch": 1.431855500821018,
      "grad_norm": 5.522737503051758,
      "learning_rate": 2.454186839872158e-06,
      "loss": 0.2413,
      "num_input_tokens_seen": 1438128,
      "step": 872
    },
    {
      "epoch": 1.4334975369458127,
      "grad_norm": 14.914284706115723,
      "learning_rate": 2.4166176784174795e-06,
      "loss": 0.4009,
      "num_input_tokens_seen": 1439920,
      "step": 873
    },
    {
      "epoch": 1.4351395730706076,
      "grad_norm": 12.73422908782959,
      "learning_rate": 2.379323688349516e-06,
      "loss": 0.6416,
      "num_input_tokens_seen": 1441456,
      "step": 874
    },
    {
      "epoch": 1.4367816091954024,
      "grad_norm": 13.819966316223145,
      "learning_rate": 2.3423053240837515e-06,
      "loss": 0.5079,
      "num_input_tokens_seen": 1442992,
      "step": 875
    },
    {
      "epoch": 1.438423645320197,
      "grad_norm": 12.874691009521484,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 0.5553,
      "num_input_tokens_seen": 1444528,
      "step": 876
    },
    {
      "epoch": 1.4400656814449917,
      "grad_norm": 15.695448875427246,
      "learning_rate": 2.269097273823287e-06,
      "loss": 0.619,
      "num_input_tokens_seen": 1446064,
      "step": 877
    },
    {
      "epoch": 1.4417077175697866,
      "grad_norm": 6.6404032707214355,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 0.245,
      "num_input_tokens_seen": 1447600,
      "step": 878
    },
    {
      "epoch": 1.4433497536945814,
      "grad_norm": 10.024596214294434,
      "learning_rate": 2.1969970956931762e-06,
      "loss": 0.3139,
      "num_input_tokens_seen": 1449136,
      "step": 879
    },
    {
      "epoch": 1.444991789819376,
      "grad_norm": 7.214747905731201,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 0.2322,
      "num_input_tokens_seen": 1450672,
      "step": 880
    },
    {
      "epoch": 1.4466338259441707,
      "grad_norm": 11.079927444458008,
      "learning_rate": 2.1260083037543817e-06,
      "loss": 0.4161,
      "num_input_tokens_seen": 1452464,
      "step": 881
    },
    {
      "epoch": 1.4482758620689655,
      "grad_norm": 12.113347053527832,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 0.8059,
      "num_input_tokens_seen": 1454000,
      "step": 882
    },
    {
      "epoch": 1.4499178981937604,
      "grad_norm": 18.128625869750977,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 0.8827,
      "num_input_tokens_seen": 1455536,
      "step": 883
    },
    {
      "epoch": 1.451559934318555,
      "grad_norm": 13.790226936340332,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 0.6343,
      "num_input_tokens_seen": 1457328,
      "step": 884
    },
    {
      "epoch": 1.4532019704433496,
      "grad_norm": 7.819671630859375,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 0.2606,
      "num_input_tokens_seen": 1458864,
      "step": 885
    },
    {
      "epoch": 1.4548440065681445,
      "grad_norm": 5.621817588806152,
      "learning_rate": 1.95342121028749e-06,
      "loss": 0.1448,
      "num_input_tokens_seen": 1460400,
      "step": 886
    },
    {
      "epoch": 1.4564860426929394,
      "grad_norm": 19.1003475189209,
      "learning_rate": 1.9197445721754776e-06,
      "loss": 1.0383,
      "num_input_tokens_seen": 1461936,
      "step": 887
    },
    {
      "epoch": 1.458128078817734,
      "grad_norm": 9.073497772216797,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 0.578,
      "num_input_tokens_seen": 1463728,
      "step": 888
    },
    {
      "epoch": 1.4597701149425286,
      "grad_norm": 15.558971405029297,
      "learning_rate": 1.8532353797501318e-06,
      "loss": 0.6487,
      "num_input_tokens_seen": 1466032,
      "step": 889
    },
    {
      "epoch": 1.4614121510673235,
      "grad_norm": 8.705544471740723,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 0.4785,
      "num_input_tokens_seen": 1467824,
      "step": 890
    },
    {
      "epoch": 1.4630541871921183,
      "grad_norm": 10.803262710571289,
      "learning_rate": 1.787854327977162e-06,
      "loss": 0.3181,
      "num_input_tokens_seen": 1469616,
      "step": 891
    },
    {
      "epoch": 1.464696223316913,
      "grad_norm": 4.704570293426514,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 0.1318,
      "num_input_tokens_seen": 1471408,
      "step": 892
    },
    {
      "epoch": 1.4663382594417076,
      "grad_norm": 7.237060070037842,
      "learning_rate": 1.7236046034367958e-06,
      "loss": 0.347,
      "num_input_tokens_seen": 1472944,
      "step": 893
    },
    {
      "epoch": 1.4679802955665024,
      "grad_norm": 11.53706169128418,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 0.3803,
      "num_input_tokens_seen": 1474480,
      "step": 894
    },
    {
      "epoch": 1.4696223316912973,
      "grad_norm": 12.550789833068848,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 0.4123,
      "num_input_tokens_seen": 1476016,
      "step": 895
    },
    {
      "epoch": 1.471264367816092,
      "grad_norm": 16.029836654663086,
      "learning_rate": 1.629358090099639e-06,
      "loss": 0.6862,
      "num_input_tokens_seen": 1477808,
      "step": 896
    },
    {
      "epoch": 1.4729064039408866,
      "grad_norm": 11.8528413772583,
      "learning_rate": 1.5985116065256684e-06,
      "loss": 0.6362,
      "num_input_tokens_seen": 1479344,
      "step": 897
    },
    {
      "epoch": 1.4745484400656814,
      "grad_norm": 12.346001625061035,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 0.4395,
      "num_input_tokens_seen": 1480880,
      "step": 898
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 6.210726737976074,
      "learning_rate": 1.5376744310111019e-06,
      "loss": 0.2197,
      "num_input_tokens_seen": 1482672,
      "step": 899
    },
    {
      "epoch": 1.477832512315271,
      "grad_norm": 11.841882705688477,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 0.3309,
      "num_input_tokens_seen": 1484208,
      "step": 900
    },
    {
      "epoch": 1.4794745484400658,
      "grad_norm": 9.877828598022461,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 0.3452,
      "num_input_tokens_seen": 1485744,
      "step": 901
    },
    {
      "epoch": 1.4811165845648604,
      "grad_norm": 10.766880989074707,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 0.3591,
      "num_input_tokens_seen": 1487536,
      "step": 902
    },
    {
      "epoch": 1.4827586206896552,
      "grad_norm": 12.060425758361816,
      "learning_rate": 1.4194335513099761e-06,
      "loss": 0.6128,
      "num_input_tokens_seen": 1489072,
      "step": 903
    },
    {
      "epoch": 1.4844006568144499,
      "grad_norm": 13.285649299621582,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 0.7551,
      "num_input_tokens_seen": 1490864,
      "step": 904
    },
    {
      "epoch": 1.4860426929392447,
      "grad_norm": 17.894962310791016,
      "learning_rate": 1.362035610017079e-06,
      "loss": 0.6048,
      "num_input_tokens_seen": 1492400,
      "step": 905
    },
    {
      "epoch": 1.4876847290640394,
      "grad_norm": 11.881617546081543,
      "learning_rate": 1.333768497107593e-06,
      "loss": 0.3721,
      "num_input_tokens_seen": 1493936,
      "step": 906
    },
    {
      "epoch": 1.4893267651888342,
      "grad_norm": 17.62555503845215,
      "learning_rate": 1.305789749760361e-06,
      "loss": 0.8585,
      "num_input_tokens_seen": 1495472,
      "step": 907
    },
    {
      "epoch": 1.4909688013136289,
      "grad_norm": 14.889278411865234,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 0.6253,
      "num_input_tokens_seen": 1497264,
      "step": 908
    },
    {
      "epoch": 1.4926108374384237,
      "grad_norm": 6.685609340667725,
      "learning_rate": 1.250698711883691e-06,
      "loss": 0.1715,
      "num_input_tokens_seen": 1498800,
      "step": 909
    },
    {
      "epoch": 1.4942528735632183,
      "grad_norm": 8.350495338439941,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 0.3395,
      "num_input_tokens_seen": 1500592,
      "step": 910
    },
    {
      "epoch": 1.4958949096880132,
      "grad_norm": 11.487955093383789,
      "learning_rate": 1.1967651814465354e-06,
      "loss": 0.6144,
      "num_input_tokens_seen": 1502128,
      "step": 911
    },
    {
      "epoch": 1.4975369458128078,
      "grad_norm": 8.641677856445312,
      "learning_rate": 1.170233305176327e-06,
      "loss": 0.3748,
      "num_input_tokens_seen": 1503920,
      "step": 912
    },
    {
      "epoch": 1.4991789819376027,
      "grad_norm": 17.225872039794922,
      "learning_rate": 1.1439917870930793e-06,
      "loss": 0.442,
      "num_input_tokens_seen": 1505968,
      "step": 913
    },
    {
      "epoch": 1.5008210180623975,
      "grad_norm": 7.956470012664795,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 0.32,
      "num_input_tokens_seen": 1507760,
      "step": 914
    },
    {
      "epoch": 1.5024630541871922,
      "grad_norm": 12.422910690307617,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 0.8068,
      "num_input_tokens_seen": 1509296,
      "step": 915
    },
    {
      "epoch": 1.5041050903119868,
      "grad_norm": 15.5726900100708,
      "learning_rate": 1.067012561698319e-06,
      "loss": 0.7555,
      "num_input_tokens_seen": 1510832,
      "step": 916
    },
    {
      "epoch": 1.5057471264367817,
      "grad_norm": 8.474631309509277,
      "learning_rate": 1.0419356383716688e-06,
      "loss": 0.3204,
      "num_input_tokens_seen": 1512624,
      "step": 917
    },
    {
      "epoch": 1.5073891625615765,
      "grad_norm": 9.456557273864746,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 0.4682,
      "num_input_tokens_seen": 1514416,
      "step": 918
    },
    {
      "epoch": 1.5090311986863711,
      "grad_norm": 8.405433654785156,
      "learning_rate": 9.926578580764234e-07,
      "loss": 0.3258,
      "num_input_tokens_seen": 1516208,
      "step": 919
    },
    {
      "epoch": 1.5106732348111658,
      "grad_norm": 10.479921340942383,
      "learning_rate": 9.684576015420278e-07,
      "loss": 0.4145,
      "num_input_tokens_seen": 1517744,
      "step": 920
    },
    {
      "epoch": 1.5123152709359606,
      "grad_norm": 11.943156242370605,
      "learning_rate": 9.445501617678654e-07,
      "loss": 0.4436,
      "num_input_tokens_seen": 1519280,
      "step": 921
    },
    {
      "epoch": 1.5139573070607555,
      "grad_norm": 12.170308113098145,
      "learning_rate": 9.209358300585474e-07,
      "loss": 0.4186,
      "num_input_tokens_seen": 1520816,
      "step": 922
    },
    {
      "epoch": 1.5155993431855501,
      "grad_norm": 9.061483383178711,
      "learning_rate": 8.976148941472501e-07,
      "loss": 0.2047,
      "num_input_tokens_seen": 1522352,
      "step": 923
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 10.013359069824219,
      "learning_rate": 8.745876381922147e-07,
      "loss": 0.3666,
      "num_input_tokens_seen": 1524144,
      "step": 924
    },
    {
      "epoch": 1.5188834154351396,
      "grad_norm": 8.677803993225098,
      "learning_rate": 8.51854342773295e-07,
      "loss": 0.4081,
      "num_input_tokens_seen": 1525936,
      "step": 925
    },
    {
      "epoch": 1.5205254515599345,
      "grad_norm": 3.790904998779297,
      "learning_rate": 8.294152848885157e-07,
      "loss": 0.1276,
      "num_input_tokens_seen": 1527472,
      "step": 926
    },
    {
      "epoch": 1.522167487684729,
      "grad_norm": 6.268991470336914,
      "learning_rate": 8.072707379507216e-07,
      "loss": 0.2275,
      "num_input_tokens_seen": 1529008,
      "step": 927
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 8.50784969329834,
      "learning_rate": 7.854209717842231e-07,
      "loss": 0.2702,
      "num_input_tokens_seen": 1530544,
      "step": 928
    },
    {
      "epoch": 1.5254515599343186,
      "grad_norm": 7.5726728439331055,
      "learning_rate": 7.638662526215284e-07,
      "loss": 0.3228,
      "num_input_tokens_seen": 1532080,
      "step": 929
    },
    {
      "epoch": 1.5270935960591134,
      "grad_norm": 19.715543746948242,
      "learning_rate": 7.426068431000882e-07,
      "loss": 0.748,
      "num_input_tokens_seen": 1533872,
      "step": 930
    },
    {
      "epoch": 1.528735632183908,
      "grad_norm": 17.435623168945312,
      "learning_rate": 7.216430022591008e-07,
      "loss": 0.6122,
      "num_input_tokens_seen": 1535408,
      "step": 931
    },
    {
      "epoch": 1.5303776683087027,
      "grad_norm": 7.221951007843018,
      "learning_rate": 7.009749855363456e-07,
      "loss": 0.1985,
      "num_input_tokens_seen": 1537200,
      "step": 932
    },
    {
      "epoch": 1.5320197044334976,
      "grad_norm": 16.20228385925293,
      "learning_rate": 6.806030447650879e-07,
      "loss": 0.6591,
      "num_input_tokens_seen": 1538992,
      "step": 933
    },
    {
      "epoch": 1.5336617405582924,
      "grad_norm": 9.848550796508789,
      "learning_rate": 6.605274281709928e-07,
      "loss": 0.1807,
      "num_input_tokens_seen": 1540528,
      "step": 934
    },
    {
      "epoch": 1.535303776683087,
      "grad_norm": 12.17349910736084,
      "learning_rate": 6.407483803691216e-07,
      "loss": 0.7891,
      "num_input_tokens_seen": 1542064,
      "step": 935
    },
    {
      "epoch": 1.5369458128078817,
      "grad_norm": 12.866559028625488,
      "learning_rate": 6.212661423609184e-07,
      "loss": 0.3669,
      "num_input_tokens_seen": 1543856,
      "step": 936
    },
    {
      "epoch": 1.5385878489326765,
      "grad_norm": 13.856314659118652,
      "learning_rate": 6.020809515313142e-07,
      "loss": 0.4217,
      "num_input_tokens_seen": 1545392,
      "step": 937
    },
    {
      "epoch": 1.5402298850574714,
      "grad_norm": 10.17542552947998,
      "learning_rate": 5.83193041645802e-07,
      "loss": 0.2753,
      "num_input_tokens_seen": 1546928,
      "step": 938
    },
    {
      "epoch": 1.541871921182266,
      "grad_norm": 14.225144386291504,
      "learning_rate": 5.646026428476031e-07,
      "loss": 0.5874,
      "num_input_tokens_seen": 1548464,
      "step": 939
    },
    {
      "epoch": 1.5435139573070606,
      "grad_norm": 4.626683712005615,
      "learning_rate": 5.463099816548579e-07,
      "loss": 0.1068,
      "num_input_tokens_seen": 1550256,
      "step": 940
    },
    {
      "epoch": 1.5451559934318555,
      "grad_norm": 12.37719440460205,
      "learning_rate": 5.283152809578751e-07,
      "loss": 0.6333,
      "num_input_tokens_seen": 1551792,
      "step": 941
    },
    {
      "epoch": 1.5467980295566504,
      "grad_norm": 11.950813293457031,
      "learning_rate": 5.106187600163987e-07,
      "loss": 0.4718,
      "num_input_tokens_seen": 1553328,
      "step": 942
    },
    {
      "epoch": 1.548440065681445,
      "grad_norm": 9.027544021606445,
      "learning_rate": 4.932206344569562e-07,
      "loss": 0.324,
      "num_input_tokens_seen": 1554864,
      "step": 943
    },
    {
      "epoch": 1.5500821018062396,
      "grad_norm": 9.25435733795166,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 0.2837,
      "num_input_tokens_seen": 1556400,
      "step": 944
    },
    {
      "epoch": 1.5517241379310345,
      "grad_norm": 19.254154205322266,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 0.3007,
      "num_input_tokens_seen": 1558192,
      "step": 945
    },
    {
      "epoch": 1.5533661740558293,
      "grad_norm": 14.833452224731445,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 0.4682,
      "num_input_tokens_seen": 1559984,
      "step": 946
    },
    {
      "epoch": 1.555008210180624,
      "grad_norm": 4.987056732177734,
      "learning_rate": 4.26616271261146e-07,
      "loss": 0.1404,
      "num_input_tokens_seen": 1561520,
      "step": 947
    },
    {
      "epoch": 1.5566502463054186,
      "grad_norm": 13.08688735961914,
      "learning_rate": 4.107132296653549e-07,
      "loss": 0.5397,
      "num_input_tokens_seen": 1563056,
      "step": 948
    },
    {
      "epoch": 1.5582922824302134,
      "grad_norm": 14.666396141052246,
      "learning_rate": 3.95109800768953e-07,
      "loss": 0.8979,
      "num_input_tokens_seen": 1564848,
      "step": 949
    },
    {
      "epoch": 1.5599343185550083,
      "grad_norm": 7.832017421722412,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 0.3497,
      "num_input_tokens_seen": 1566384,
      "step": 950
    },
    {
      "epoch": 1.561576354679803,
      "grad_norm": 17.53074073791504,
      "learning_rate": 3.6480253791274786e-07,
      "loss": 0.6617,
      "num_input_tokens_seen": 1568176,
      "step": 951
    },
    {
      "epoch": 1.5632183908045976,
      "grad_norm": 12.52601146697998,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 0.3074,
      "num_input_tokens_seen": 1569712,
      "step": 952
    },
    {
      "epoch": 1.5648604269293924,
      "grad_norm": 10.86441707611084,
      "learning_rate": 3.3569595982576583e-07,
      "loss": 0.3962,
      "num_input_tokens_seen": 1571248,
      "step": 953
    },
    {
      "epoch": 1.5665024630541873,
      "grad_norm": 5.959725856781006,
      "learning_rate": 3.215933731753024e-07,
      "loss": 0.1503,
      "num_input_tokens_seen": 1572784,
      "step": 954
    },
    {
      "epoch": 1.568144499178982,
      "grad_norm": 10.259957313537598,
      "learning_rate": 3.077914851215585e-07,
      "loss": 0.483,
      "num_input_tokens_seen": 1574320,
      "step": 955
    },
    {
      "epoch": 1.5697865353037765,
      "grad_norm": 10.199813842773438,
      "learning_rate": 2.942904638361804e-07,
      "loss": 0.3008,
      "num_input_tokens_seen": 1575856,
      "step": 956
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 9.475740432739258,
      "learning_rate": 2.810904738248549e-07,
      "loss": 0.3127,
      "num_input_tokens_seen": 1577648,
      "step": 957
    },
    {
      "epoch": 1.5730706075533663,
      "grad_norm": 18.381847381591797,
      "learning_rate": 2.681916759252917e-07,
      "loss": 0.7427,
      "num_input_tokens_seen": 1579184,
      "step": 958
    },
    {
      "epoch": 1.5747126436781609,
      "grad_norm": 17.96950912475586,
      "learning_rate": 2.555942273052753e-07,
      "loss": 0.6566,
      "num_input_tokens_seen": 1580976,
      "step": 959
    },
    {
      "epoch": 1.5763546798029555,
      "grad_norm": 15.111069679260254,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 0.4198,
      "num_input_tokens_seen": 1582512,
      "step": 960
    },
    {
      "epoch": 1.5779967159277504,
      "grad_norm": 12.480873107910156,
      "learning_rate": 2.3130398821391007e-07,
      "loss": 0.2827,
      "num_input_tokens_seen": 1584048,
      "step": 961
    },
    {
      "epoch": 1.5796387520525452,
      "grad_norm": 12.758831024169922,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 0.5833,
      "num_input_tokens_seen": 1585840,
      "step": 962
    },
    {
      "epoch": 1.5812807881773399,
      "grad_norm": 7.124483108520508,
      "learning_rate": 2.0822094042274032e-07,
      "loss": 0.2951,
      "num_input_tokens_seen": 1587376,
      "step": 963
    },
    {
      "epoch": 1.5829228243021345,
      "grad_norm": 16.58106231689453,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 0.4255,
      "num_input_tokens_seen": 1588912,
      "step": 964
    },
    {
      "epoch": 1.5845648604269293,
      "grad_norm": 9.150734901428223,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 0.3462,
      "num_input_tokens_seen": 1590704,
      "step": 965
    },
    {
      "epoch": 1.5862068965517242,
      "grad_norm": 19.54640007019043,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 0.7553,
      "num_input_tokens_seen": 1593008,
      "step": 966
    },
    {
      "epoch": 1.5878489326765188,
      "grad_norm": 15.014461517333984,
      "learning_rate": 1.6568085999008888e-07,
      "loss": 1.1676,
      "num_input_tokens_seen": 1594544,
      "step": 967
    },
    {
      "epoch": 1.5894909688013135,
      "grad_norm": 9.97701644897461,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 0.2764,
      "num_input_tokens_seen": 1596336,
      "step": 968
    },
    {
      "epoch": 1.5911330049261085,
      "grad_norm": 14.134190559387207,
      "learning_rate": 1.4622590069211516e-07,
      "loss": 0.4316,
      "num_input_tokens_seen": 1598128,
      "step": 969
    },
    {
      "epoch": 1.5927750410509032,
      "grad_norm": 13.227944374084473,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 0.2718,
      "num_input_tokens_seen": 1599664,
      "step": 970
    },
    {
      "epoch": 1.5944170771756978,
      "grad_norm": 15.648421287536621,
      "learning_rate": 1.2798227928029482e-07,
      "loss": 0.6491,
      "num_input_tokens_seen": 1601200,
      "step": 971
    },
    {
      "epoch": 1.5960591133004927,
      "grad_norm": 10.65359878540039,
      "learning_rate": 1.193150004542204e-07,
      "loss": 0.5494,
      "num_input_tokens_seen": 1602736,
      "step": 972
    },
    {
      "epoch": 1.5977011494252875,
      "grad_norm": 13.25188159942627,
      "learning_rate": 1.109508849230001e-07,
      "loss": 0.536,
      "num_input_tokens_seen": 1604528,
      "step": 973
    },
    {
      "epoch": 1.5993431855500821,
      "grad_norm": 6.56326150894165,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 0.2168,
      "num_input_tokens_seen": 1606320,
      "step": 974
    },
    {
      "epoch": 1.6009852216748768,
      "grad_norm": 13.650506973266602,
      "learning_rate": 9.513254770636137e-08,
      "loss": 0.7921,
      "num_input_tokens_seen": 1608112,
      "step": 975
    },
    {
      "epoch": 1.6026272577996716,
      "grad_norm": 11.850807189941406,
      "learning_rate": 8.767851876239074e-08,
      "loss": 0.6155,
      "num_input_tokens_seen": 1609648,
      "step": 976
    },
    {
      "epoch": 1.6042692939244665,
      "grad_norm": 11.018970489501953,
      "learning_rate": 8.052803859382174e-08,
      "loss": 0.4006,
      "num_input_tokens_seen": 1611184,
      "step": 977
    },
    {
      "epoch": 1.6059113300492611,
      "grad_norm": 13.850626945495605,
      "learning_rate": 7.368119432699383e-08,
      "loss": 0.436,
      "num_input_tokens_seen": 1612720,
      "step": 978
    },
    {
      "epoch": 1.6075533661740558,
      "grad_norm": 20.115964889526367,
      "learning_rate": 6.71380693885476e-08,
      "loss": 0.5061,
      "num_input_tokens_seen": 1614256,
      "step": 979
    },
    {
      "epoch": 1.6091954022988506,
      "grad_norm": 13.490277290344238,
      "learning_rate": 6.089874350439506e-08,
      "loss": 0.2686,
      "num_input_tokens_seen": 1615792,
      "step": 980
    },
    {
      "epoch": 1.6108374384236455,
      "grad_norm": 7.900205612182617,
      "learning_rate": 5.496329269875089e-08,
      "loss": 0.4221,
      "num_input_tokens_seen": 1617584,
      "step": 981
    },
    {
      "epoch": 1.61247947454844,
      "grad_norm": 10.6383695602417,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 0.302,
      "num_input_tokens_seen": 1619120,
      "step": 982
    },
    {
      "epoch": 1.6141215106732347,
      "grad_norm": 12.769503593444824,
      "learning_rate": 4.400430190586724e-08,
      "loss": 0.3462,
      "num_input_tokens_seen": 1621168,
      "step": 983
    },
    {
      "epoch": 1.6157635467980296,
      "grad_norm": 12.14033031463623,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 0.5279,
      "num_input_tokens_seen": 1622704,
      "step": 984
    },
    {
      "epoch": 1.6174055829228244,
      "grad_norm": 10.263139724731445,
      "learning_rate": 3.426163113565417e-08,
      "loss": 0.2967,
      "num_input_tokens_seen": 1624240,
      "step": 985
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 10.25717544555664,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 0.3311,
      "num_input_tokens_seen": 1626032,
      "step": 986
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 4.75950813293457,
      "learning_rate": 2.5735755232134118e-08,
      "loss": 0.1617,
      "num_input_tokens_seen": 1628080,
      "step": 987
    },
    {
      "epoch": 1.6223316912972086,
      "grad_norm": 18.561649322509766,
      "learning_rate": 2.192924752854042e-08,
      "loss": 0.3272,
      "num_input_tokens_seen": 1629616,
      "step": 988
    },
    {
      "epoch": 1.6239737274220034,
      "grad_norm": 10.607986450195312,
      "learning_rate": 1.842708973447127e-08,
      "loss": 0.4573,
      "num_input_tokens_seen": 1631152,
      "step": 989
    },
    {
      "epoch": 1.625615763546798,
      "grad_norm": 18.315441131591797,
      "learning_rate": 1.522932452260595e-08,
      "loss": 0.6901,
      "num_input_tokens_seen": 1632944,
      "step": 990
    },
    {
      "epoch": 1.6272577996715927,
      "grad_norm": 11.641156196594238,
      "learning_rate": 1.233599085671e-08,
      "loss": 0.4447,
      "num_input_tokens_seen": 1634736,
      "step": 991
    },
    {
      "epoch": 1.6288998357963875,
      "grad_norm": 9.349448204040527,
      "learning_rate": 9.747123991141194e-09,
      "loss": 0.4505,
      "num_input_tokens_seen": 1636272,
      "step": 992
    },
    {
      "epoch": 1.6305418719211824,
      "grad_norm": 9.897985458374023,
      "learning_rate": 7.462755470422078e-09,
      "loss": 0.42,
      "num_input_tokens_seen": 1637808,
      "step": 993
    },
    {
      "epoch": 1.632183908045977,
      "grad_norm": 10.006303787231445,
      "learning_rate": 5.48291312886251e-09,
      "loss": 0.4004,
      "num_input_tokens_seen": 1639856,
      "step": 994
    },
    {
      "epoch": 1.6338259441707716,
      "grad_norm": 19.718942642211914,
      "learning_rate": 3.807621090218261e-09,
      "loss": 1.084,
      "num_input_tokens_seen": 1641392,
      "step": 995
    },
    {
      "epoch": 1.6354679802955665,
      "grad_norm": 9.886137008666992,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 0.3594,
      "num_input_tokens_seen": 1643184,
      "step": 996
    },
    {
      "epoch": 1.6371100164203614,
      "grad_norm": 7.477581977844238,
      "learning_rate": 1.3707658621964215e-09,
      "loss": 0.2279,
      "num_input_tokens_seen": 1644720,
      "step": 997
    },
    {
      "epoch": 1.638752052545156,
      "grad_norm": 7.808615684509277,
      "learning_rate": 6.092323651313292e-10,
      "loss": 0.2887,
      "num_input_tokens_seen": 1646256,
      "step": 998
    },
    {
      "epoch": 1.6403940886699506,
      "grad_norm": 8.35030746459961,
      "learning_rate": 1.5230855524017708e-10,
      "loss": 0.2623,
      "num_input_tokens_seen": 1647792,
      "step": 999
    },
    {
      "epoch": 1.6420361247947455,
      "grad_norm": 11.180130958557129,
      "learning_rate": 0.0,
      "loss": 0.3356,
      "num_input_tokens_seen": 1649328,
      "step": 1000
    },
    {
      "epoch": 1.6420361247947455,
      "eval_loss": 0.4453779458999634,
      "eval_runtime": 10.7001,
      "eval_samples_per_second": 114.111,
      "eval_steps_per_second": 14.299,
      "num_input_tokens_seen": 1649328,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 1649328,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.43022185887826e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
