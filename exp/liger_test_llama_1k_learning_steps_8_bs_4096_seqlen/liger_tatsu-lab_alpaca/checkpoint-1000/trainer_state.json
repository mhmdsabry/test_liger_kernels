{
  "best_metric": 1.2649905681610107,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_8_bs_4096_seqlen/liger_tatsu-lab_alpaca/checkpoint-1000",
  "epoch": 0.17091095539224063,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00017091095539224064,
      "grad_norm": 0.7907206416130066,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9094,
      "num_input_tokens_seen": 1152,
      "step": 1
    },
    {
      "epoch": 0.0003418219107844813,
      "grad_norm": 0.8914700746536255,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.6283,
      "num_input_tokens_seen": 2688,
      "step": 2
    },
    {
      "epoch": 0.0005127328661767219,
      "grad_norm": 1.0658190250396729,
      "learning_rate": 1.5e-06,
      "loss": 1.6436,
      "num_input_tokens_seen": 3840,
      "step": 3
    },
    {
      "epoch": 0.0006836438215689625,
      "grad_norm": 1.0952811241149902,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.8889,
      "num_input_tokens_seen": 4992,
      "step": 4
    },
    {
      "epoch": 0.0008545547769612033,
      "grad_norm": 1.0650466680526733,
      "learning_rate": 2.5e-06,
      "loss": 1.7337,
      "num_input_tokens_seen": 6784,
      "step": 5
    },
    {
      "epoch": 0.0010254657323534439,
      "grad_norm": 0.7500959038734436,
      "learning_rate": 3e-06,
      "loss": 1.7843,
      "num_input_tokens_seen": 9216,
      "step": 6
    },
    {
      "epoch": 0.0011963766877456845,
      "grad_norm": 0.8274104595184326,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.6338,
      "num_input_tokens_seen": 10496,
      "step": 7
    },
    {
      "epoch": 0.001367287643137925,
      "grad_norm": 0.8032723069190979,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6697,
      "num_input_tokens_seen": 12288,
      "step": 8
    },
    {
      "epoch": 0.0015381985985301657,
      "grad_norm": 0.8193010687828064,
      "learning_rate": 4.5e-06,
      "loss": 1.6921,
      "num_input_tokens_seen": 14336,
      "step": 9
    },
    {
      "epoch": 0.0017091095539224065,
      "grad_norm": 1.0585631132125854,
      "learning_rate": 5e-06,
      "loss": 1.7148,
      "num_input_tokens_seen": 15488,
      "step": 10
    },
    {
      "epoch": 0.0018800205093146471,
      "grad_norm": 0.8178015947341919,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.5572,
      "num_input_tokens_seen": 17024,
      "step": 11
    },
    {
      "epoch": 0.0020509314647068877,
      "grad_norm": 0.8932339549064636,
      "learning_rate": 6e-06,
      "loss": 1.9175,
      "num_input_tokens_seen": 19072,
      "step": 12
    },
    {
      "epoch": 0.002221842420099128,
      "grad_norm": 0.9341757297515869,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.7613,
      "num_input_tokens_seen": 20224,
      "step": 13
    },
    {
      "epoch": 0.002392753375491369,
      "grad_norm": 1.5779045820236206,
      "learning_rate": 7.000000000000001e-06,
      "loss": 2.1106,
      "num_input_tokens_seen": 21760,
      "step": 14
    },
    {
      "epoch": 0.00256366433088361,
      "grad_norm": 0.9306272864341736,
      "learning_rate": 7.5e-06,
      "loss": 1.7292,
      "num_input_tokens_seen": 23296,
      "step": 15
    },
    {
      "epoch": 0.00273457528627585,
      "grad_norm": 0.7506834864616394,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.4895,
      "num_input_tokens_seen": 24832,
      "step": 16
    },
    {
      "epoch": 0.002905486241668091,
      "grad_norm": 0.8036146759986877,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.7478,
      "num_input_tokens_seen": 26752,
      "step": 17
    },
    {
      "epoch": 0.0030763971970603314,
      "grad_norm": 1.1381086111068726,
      "learning_rate": 9e-06,
      "loss": 1.6796,
      "num_input_tokens_seen": 28544,
      "step": 18
    },
    {
      "epoch": 0.0032473081524525722,
      "grad_norm": 0.9976112246513367,
      "learning_rate": 9.5e-06,
      "loss": 1.7052,
      "num_input_tokens_seen": 30336,
      "step": 19
    },
    {
      "epoch": 0.003418219107844813,
      "grad_norm": 0.8842933773994446,
      "learning_rate": 1e-05,
      "loss": 1.6645,
      "num_input_tokens_seen": 32896,
      "step": 20
    },
    {
      "epoch": 0.0035891300632370535,
      "grad_norm": 1.1540521383285522,
      "learning_rate": 1.05e-05,
      "loss": 1.81,
      "num_input_tokens_seen": 34048,
      "step": 21
    },
    {
      "epoch": 0.0037600410186292943,
      "grad_norm": 1.0786230564117432,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.5575,
      "num_input_tokens_seen": 35328,
      "step": 22
    },
    {
      "epoch": 0.003930951974021535,
      "grad_norm": 0.9142602682113647,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.337,
      "num_input_tokens_seen": 37120,
      "step": 23
    },
    {
      "epoch": 0.0041018629294137755,
      "grad_norm": 0.8519313335418701,
      "learning_rate": 1.2e-05,
      "loss": 1.6333,
      "num_input_tokens_seen": 39808,
      "step": 24
    },
    {
      "epoch": 0.004272773884806016,
      "grad_norm": 1.4652704000473022,
      "learning_rate": 1.25e-05,
      "loss": 1.6262,
      "num_input_tokens_seen": 40832,
      "step": 25
    },
    {
      "epoch": 0.004443684840198256,
      "grad_norm": 1.0800515413284302,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.8657,
      "num_input_tokens_seen": 42624,
      "step": 26
    },
    {
      "epoch": 0.004614595795590497,
      "grad_norm": 1.0732853412628174,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.645,
      "num_input_tokens_seen": 43776,
      "step": 27
    },
    {
      "epoch": 0.004785506750982738,
      "grad_norm": 1.0347689390182495,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.7725,
      "num_input_tokens_seen": 45952,
      "step": 28
    },
    {
      "epoch": 0.004956417706374979,
      "grad_norm": 0.872858464717865,
      "learning_rate": 1.45e-05,
      "loss": 1.5487,
      "num_input_tokens_seen": 48256,
      "step": 29
    },
    {
      "epoch": 0.00512732866176722,
      "grad_norm": 0.9243598580360413,
      "learning_rate": 1.5e-05,
      "loss": 1.2245,
      "num_input_tokens_seen": 49792,
      "step": 30
    },
    {
      "epoch": 0.0052982396171594595,
      "grad_norm": 1.313820242881775,
      "learning_rate": 1.55e-05,
      "loss": 1.5364,
      "num_input_tokens_seen": 51072,
      "step": 31
    },
    {
      "epoch": 0.0054691505725517,
      "grad_norm": 1.3887286186218262,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9788,
      "num_input_tokens_seen": 52480,
      "step": 32
    },
    {
      "epoch": 0.005640061527943941,
      "grad_norm": 1.2399479150772095,
      "learning_rate": 1.65e-05,
      "loss": 1.1006,
      "num_input_tokens_seen": 53504,
      "step": 33
    },
    {
      "epoch": 0.005810972483336182,
      "grad_norm": 1.1816385984420776,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6195,
      "num_input_tokens_seen": 55296,
      "step": 34
    },
    {
      "epoch": 0.005981883438728423,
      "grad_norm": 1.2386447191238403,
      "learning_rate": 1.75e-05,
      "loss": 1.8026,
      "num_input_tokens_seen": 56960,
      "step": 35
    },
    {
      "epoch": 0.006152794394120663,
      "grad_norm": 1.7232959270477295,
      "learning_rate": 1.8e-05,
      "loss": 1.5175,
      "num_input_tokens_seen": 58368,
      "step": 36
    },
    {
      "epoch": 0.006323705349512904,
      "grad_norm": 1.4889291524887085,
      "learning_rate": 1.85e-05,
      "loss": 1.693,
      "num_input_tokens_seen": 59904,
      "step": 37
    },
    {
      "epoch": 0.0064946163049051445,
      "grad_norm": 1.4038572311401367,
      "learning_rate": 1.9e-05,
      "loss": 1.4081,
      "num_input_tokens_seen": 61440,
      "step": 38
    },
    {
      "epoch": 0.006665527260297385,
      "grad_norm": 1.6856451034545898,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.7973,
      "num_input_tokens_seen": 62848,
      "step": 39
    },
    {
      "epoch": 0.006836438215689626,
      "grad_norm": 1.2253004312515259,
      "learning_rate": 2e-05,
      "loss": 0.9644,
      "num_input_tokens_seen": 64512,
      "step": 40
    },
    {
      "epoch": 0.007007349171081866,
      "grad_norm": 1.803726315498352,
      "learning_rate": 2.05e-05,
      "loss": 1.612,
      "num_input_tokens_seen": 65664,
      "step": 41
    },
    {
      "epoch": 0.007178260126474107,
      "grad_norm": 1.197450876235962,
      "learning_rate": 2.1e-05,
      "loss": 1.5695,
      "num_input_tokens_seen": 68480,
      "step": 42
    },
    {
      "epoch": 0.007349171081866348,
      "grad_norm": 1.8340693712234497,
      "learning_rate": 2.15e-05,
      "loss": 1.672,
      "num_input_tokens_seen": 69760,
      "step": 43
    },
    {
      "epoch": 0.0075200820372585886,
      "grad_norm": 1.7688970565795898,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.53,
      "num_input_tokens_seen": 71168,
      "step": 44
    },
    {
      "epoch": 0.0076909929926508285,
      "grad_norm": 1.646988868713379,
      "learning_rate": 2.25e-05,
      "loss": 1.4873,
      "num_input_tokens_seen": 72960,
      "step": 45
    },
    {
      "epoch": 0.00786190394804307,
      "grad_norm": 2.0346603393554688,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.6978,
      "num_input_tokens_seen": 74240,
      "step": 46
    },
    {
      "epoch": 0.00803281490343531,
      "grad_norm": 1.692510724067688,
      "learning_rate": 2.35e-05,
      "loss": 1.7048,
      "num_input_tokens_seen": 75520,
      "step": 47
    },
    {
      "epoch": 0.008203725858827551,
      "grad_norm": 2.0287411212921143,
      "learning_rate": 2.4e-05,
      "loss": 1.689,
      "num_input_tokens_seen": 77056,
      "step": 48
    },
    {
      "epoch": 0.008374636814219792,
      "grad_norm": 1.1493277549743652,
      "learning_rate": 2.45e-05,
      "loss": 1.1432,
      "num_input_tokens_seen": 79104,
      "step": 49
    },
    {
      "epoch": 0.008545547769612033,
      "grad_norm": 1.7630761861801147,
      "learning_rate": 2.5e-05,
      "loss": 1.5044,
      "num_input_tokens_seen": 81152,
      "step": 50
    },
    {
      "epoch": 0.008716458725004273,
      "grad_norm": 1.658677577972412,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.2931,
      "num_input_tokens_seen": 82176,
      "step": 51
    },
    {
      "epoch": 0.008887369680396513,
      "grad_norm": 1.6113003492355347,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1541,
      "num_input_tokens_seen": 83200,
      "step": 52
    },
    {
      "epoch": 0.009058280635788753,
      "grad_norm": 1.2303085327148438,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.4333,
      "num_input_tokens_seen": 85888,
      "step": 53
    },
    {
      "epoch": 0.009229191591180994,
      "grad_norm": 1.3314460515975952,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.5986,
      "num_input_tokens_seen": 87424,
      "step": 54
    },
    {
      "epoch": 0.009400102546573235,
      "grad_norm": 1.3873116970062256,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.4746,
      "num_input_tokens_seen": 88576,
      "step": 55
    },
    {
      "epoch": 0.009571013501965476,
      "grad_norm": 1.5677098035812378,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2653,
      "num_input_tokens_seen": 89856,
      "step": 56
    },
    {
      "epoch": 0.009741924457357717,
      "grad_norm": 0.8101087212562561,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.3195,
      "num_input_tokens_seen": 91136,
      "step": 57
    },
    {
      "epoch": 0.009912835412749958,
      "grad_norm": 0.7974069118499756,
      "learning_rate": 2.9e-05,
      "loss": 1.5329,
      "num_input_tokens_seen": 93568,
      "step": 58
    },
    {
      "epoch": 0.010083746368142198,
      "grad_norm": 0.6657775044441223,
      "learning_rate": 2.95e-05,
      "loss": 1.3944,
      "num_input_tokens_seen": 94848,
      "step": 59
    },
    {
      "epoch": 0.01025465732353444,
      "grad_norm": 0.9138549566268921,
      "learning_rate": 3e-05,
      "loss": 1.4534,
      "num_input_tokens_seen": 96000,
      "step": 60
    },
    {
      "epoch": 0.01042556827892668,
      "grad_norm": 1.1777219772338867,
      "learning_rate": 3.05e-05,
      "loss": 1.5262,
      "num_input_tokens_seen": 97024,
      "step": 61
    },
    {
      "epoch": 0.010596479234318919,
      "grad_norm": 1.0155426263809204,
      "learning_rate": 3.1e-05,
      "loss": 0.9561,
      "num_input_tokens_seen": 98176,
      "step": 62
    },
    {
      "epoch": 0.01076739018971116,
      "grad_norm": 1.0608012676239014,
      "learning_rate": 3.15e-05,
      "loss": 1.7934,
      "num_input_tokens_seen": 99456,
      "step": 63
    },
    {
      "epoch": 0.0109383011451034,
      "grad_norm": 0.5718676447868347,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.9887,
      "num_input_tokens_seen": 101888,
      "step": 64
    },
    {
      "epoch": 0.011109212100495642,
      "grad_norm": 0.9521288275718689,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.2803,
      "num_input_tokens_seen": 103168,
      "step": 65
    },
    {
      "epoch": 0.011280123055887882,
      "grad_norm": 0.8176235556602478,
      "learning_rate": 3.3e-05,
      "loss": 1.1419,
      "num_input_tokens_seen": 105472,
      "step": 66
    },
    {
      "epoch": 0.011451034011280123,
      "grad_norm": 0.7291742563247681,
      "learning_rate": 3.35e-05,
      "loss": 1.4857,
      "num_input_tokens_seen": 107136,
      "step": 67
    },
    {
      "epoch": 0.011621944966672364,
      "grad_norm": 1.1998927593231201,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.4814,
      "num_input_tokens_seen": 108288,
      "step": 68
    },
    {
      "epoch": 0.011792855922064605,
      "grad_norm": 0.7082191705703735,
      "learning_rate": 3.45e-05,
      "loss": 1.6198,
      "num_input_tokens_seen": 110464,
      "step": 69
    },
    {
      "epoch": 0.011963766877456846,
      "grad_norm": 0.5798860192298889,
      "learning_rate": 3.5e-05,
      "loss": 0.9746,
      "num_input_tokens_seen": 113408,
      "step": 70
    },
    {
      "epoch": 0.012134677832849085,
      "grad_norm": 1.1587566137313843,
      "learning_rate": 3.55e-05,
      "loss": 1.5828,
      "num_input_tokens_seen": 114304,
      "step": 71
    },
    {
      "epoch": 0.012305588788241326,
      "grad_norm": 0.6456112861633301,
      "learning_rate": 3.6e-05,
      "loss": 1.5777,
      "num_input_tokens_seen": 116352,
      "step": 72
    },
    {
      "epoch": 0.012476499743633566,
      "grad_norm": 0.7348100543022156,
      "learning_rate": 3.65e-05,
      "loss": 1.0868,
      "num_input_tokens_seen": 117760,
      "step": 73
    },
    {
      "epoch": 0.012647410699025807,
      "grad_norm": 0.6721492409706116,
      "learning_rate": 3.7e-05,
      "loss": 1.502,
      "num_input_tokens_seen": 119936,
      "step": 74
    },
    {
      "epoch": 0.012818321654418048,
      "grad_norm": 0.8412894010543823,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.2776,
      "num_input_tokens_seen": 121728,
      "step": 75
    },
    {
      "epoch": 0.012989232609810289,
      "grad_norm": 0.7683102488517761,
      "learning_rate": 3.8e-05,
      "loss": 1.5017,
      "num_input_tokens_seen": 124160,
      "step": 76
    },
    {
      "epoch": 0.01316014356520253,
      "grad_norm": 0.8340085744857788,
      "learning_rate": 3.85e-05,
      "loss": 1.7854,
      "num_input_tokens_seen": 125568,
      "step": 77
    },
    {
      "epoch": 0.01333105452059477,
      "grad_norm": 0.6884552836418152,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.2482,
      "num_input_tokens_seen": 126976,
      "step": 78
    },
    {
      "epoch": 0.013501965475987011,
      "grad_norm": 0.5326904058456421,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.1758,
      "num_input_tokens_seen": 128896,
      "step": 79
    },
    {
      "epoch": 0.013672876431379252,
      "grad_norm": 1.0893115997314453,
      "learning_rate": 4e-05,
      "loss": 1.449,
      "num_input_tokens_seen": 130304,
      "step": 80
    },
    {
      "epoch": 0.013843787386771491,
      "grad_norm": 1.1728020906448364,
      "learning_rate": 4.05e-05,
      "loss": 1.265,
      "num_input_tokens_seen": 131328,
      "step": 81
    },
    {
      "epoch": 0.014014698342163732,
      "grad_norm": 0.758629560470581,
      "learning_rate": 4.1e-05,
      "loss": 1.2444,
      "num_input_tokens_seen": 132736,
      "step": 82
    },
    {
      "epoch": 0.014185609297555973,
      "grad_norm": 0.592172384262085,
      "learning_rate": 4.15e-05,
      "loss": 0.9141,
      "num_input_tokens_seen": 135424,
      "step": 83
    },
    {
      "epoch": 0.014356520252948214,
      "grad_norm": 0.7826918959617615,
      "learning_rate": 4.2e-05,
      "loss": 1.2283,
      "num_input_tokens_seen": 136832,
      "step": 84
    },
    {
      "epoch": 0.014527431208340455,
      "grad_norm": 0.7026346325874329,
      "learning_rate": 4.25e-05,
      "loss": 1.187,
      "num_input_tokens_seen": 138240,
      "step": 85
    },
    {
      "epoch": 0.014698342163732695,
      "grad_norm": 0.8627390265464783,
      "learning_rate": 4.3e-05,
      "loss": 1.9613,
      "num_input_tokens_seen": 139520,
      "step": 86
    },
    {
      "epoch": 0.014869253119124936,
      "grad_norm": 0.5815972089767456,
      "learning_rate": 4.35e-05,
      "loss": 1.5097,
      "num_input_tokens_seen": 140800,
      "step": 87
    },
    {
      "epoch": 0.015040164074517177,
      "grad_norm": 1.0751360654830933,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.697,
      "num_input_tokens_seen": 141952,
      "step": 88
    },
    {
      "epoch": 0.015211075029909418,
      "grad_norm": 0.6175169348716736,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.4125,
      "num_input_tokens_seen": 143616,
      "step": 89
    },
    {
      "epoch": 0.015381985985301657,
      "grad_norm": 0.8690312504768372,
      "learning_rate": 4.5e-05,
      "loss": 1.5604,
      "num_input_tokens_seen": 145024,
      "step": 90
    },
    {
      "epoch": 0.015552896940693898,
      "grad_norm": 0.6648961305618286,
      "learning_rate": 4.55e-05,
      "loss": 1.3585,
      "num_input_tokens_seen": 146560,
      "step": 91
    },
    {
      "epoch": 0.01572380789608614,
      "grad_norm": 0.7622138857841492,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.4091,
      "num_input_tokens_seen": 147712,
      "step": 92
    },
    {
      "epoch": 0.01589471885147838,
      "grad_norm": 0.4379872679710388,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.1777,
      "num_input_tokens_seen": 149888,
      "step": 93
    },
    {
      "epoch": 0.01606562980687062,
      "grad_norm": 0.8372400999069214,
      "learning_rate": 4.7e-05,
      "loss": 1.3346,
      "num_input_tokens_seen": 151424,
      "step": 94
    },
    {
      "epoch": 0.01623654076226286,
      "grad_norm": 0.7573032379150391,
      "learning_rate": 4.75e-05,
      "loss": 1.4223,
      "num_input_tokens_seen": 152832,
      "step": 95
    },
    {
      "epoch": 0.016407451717655102,
      "grad_norm": 0.7972332239151001,
      "learning_rate": 4.8e-05,
      "loss": 1.2211,
      "num_input_tokens_seen": 153856,
      "step": 96
    },
    {
      "epoch": 0.016578362673047343,
      "grad_norm": 0.5493025183677673,
      "learning_rate": 4.85e-05,
      "loss": 1.4782,
      "num_input_tokens_seen": 156800,
      "step": 97
    },
    {
      "epoch": 0.016749273628439584,
      "grad_norm": 1.0481206178665161,
      "learning_rate": 4.9e-05,
      "loss": 1.115,
      "num_input_tokens_seen": 158336,
      "step": 98
    },
    {
      "epoch": 0.016920184583831824,
      "grad_norm": 0.5128641128540039,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 1.285,
      "num_input_tokens_seen": 160512,
      "step": 99
    },
    {
      "epoch": 0.017091095539224065,
      "grad_norm": 1.2289623022079468,
      "learning_rate": 5e-05,
      "loss": 0.8542,
      "num_input_tokens_seen": 161536,
      "step": 100
    },
    {
      "epoch": 0.017262006494616306,
      "grad_norm": 0.5224534273147583,
      "learning_rate": 4.999984769144476e-05,
      "loss": 1.2976,
      "num_input_tokens_seen": 163456,
      "step": 101
    },
    {
      "epoch": 0.017432917450008547,
      "grad_norm": 1.0925666093826294,
      "learning_rate": 4.999939076763487e-05,
      "loss": 1.4534,
      "num_input_tokens_seen": 164864,
      "step": 102
    },
    {
      "epoch": 0.017603828405400788,
      "grad_norm": 1.1681404113769531,
      "learning_rate": 4.999862923413781e-05,
      "loss": 1.0881,
      "num_input_tokens_seen": 165888,
      "step": 103
    },
    {
      "epoch": 0.017774739360793025,
      "grad_norm": 0.6809857487678528,
      "learning_rate": 4.999756310023261e-05,
      "loss": 1.291,
      "num_input_tokens_seen": 167168,
      "step": 104
    },
    {
      "epoch": 0.017945650316185266,
      "grad_norm": 0.7465459704399109,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 1.6584,
      "num_input_tokens_seen": 168960,
      "step": 105
    },
    {
      "epoch": 0.018116561271577507,
      "grad_norm": 0.9226880669593811,
      "learning_rate": 4.999451708687114e-05,
      "loss": 1.5299,
      "num_input_tokens_seen": 170752,
      "step": 106
    },
    {
      "epoch": 0.018287472226969748,
      "grad_norm": 0.7468276619911194,
      "learning_rate": 4.999253724452958e-05,
      "loss": 1.4885,
      "num_input_tokens_seen": 172800,
      "step": 107
    },
    {
      "epoch": 0.01845838318236199,
      "grad_norm": 0.8997945189476013,
      "learning_rate": 4.999025287600886e-05,
      "loss": 1.0714,
      "num_input_tokens_seen": 174208,
      "step": 108
    },
    {
      "epoch": 0.01862929413775423,
      "grad_norm": 0.7583211660385132,
      "learning_rate": 4.998766400914329e-05,
      "loss": 1.1821,
      "num_input_tokens_seen": 175488,
      "step": 109
    },
    {
      "epoch": 0.01880020509314647,
      "grad_norm": 1.0675338506698608,
      "learning_rate": 4.99847706754774e-05,
      "loss": 1.749,
      "num_input_tokens_seen": 176512,
      "step": 110
    },
    {
      "epoch": 0.01897111604853871,
      "grad_norm": 1.0712292194366455,
      "learning_rate": 4.998157291026553e-05,
      "loss": 1.0207,
      "num_input_tokens_seen": 177408,
      "step": 111
    },
    {
      "epoch": 0.019142027003930952,
      "grad_norm": 0.6759844422340393,
      "learning_rate": 4.997807075247146e-05,
      "loss": 1.4394,
      "num_input_tokens_seen": 178944,
      "step": 112
    },
    {
      "epoch": 0.019312937959323193,
      "grad_norm": 0.6902356743812561,
      "learning_rate": 4.997426424476787e-05,
      "loss": 1.3965,
      "num_input_tokens_seen": 180224,
      "step": 113
    },
    {
      "epoch": 0.019483848914715433,
      "grad_norm": 0.6108776330947876,
      "learning_rate": 4.997015343353585e-05,
      "loss": 1.5421,
      "num_input_tokens_seen": 181888,
      "step": 114
    },
    {
      "epoch": 0.019654759870107674,
      "grad_norm": 0.9498350024223328,
      "learning_rate": 4.996573836886435e-05,
      "loss": 1.3406,
      "num_input_tokens_seen": 183296,
      "step": 115
    },
    {
      "epoch": 0.019825670825499915,
      "grad_norm": 0.5751810669898987,
      "learning_rate": 4.996101910454953e-05,
      "loss": 1.1375,
      "num_input_tokens_seen": 184960,
      "step": 116
    },
    {
      "epoch": 0.019996581780892156,
      "grad_norm": 0.7451227307319641,
      "learning_rate": 4.995599569809414e-05,
      "loss": 1.4529,
      "num_input_tokens_seen": 186496,
      "step": 117
    },
    {
      "epoch": 0.020167492736284397,
      "grad_norm": 0.5920264720916748,
      "learning_rate": 4.995066821070679e-05,
      "loss": 1.1817,
      "num_input_tokens_seen": 187904,
      "step": 118
    },
    {
      "epoch": 0.020338403691676638,
      "grad_norm": 0.6465930342674255,
      "learning_rate": 4.994503670730125e-05,
      "loss": 1.6827,
      "num_input_tokens_seen": 190080,
      "step": 119
    },
    {
      "epoch": 0.02050931464706888,
      "grad_norm": 1.1861387491226196,
      "learning_rate": 4.993910125649561e-05,
      "loss": 1.3227,
      "num_input_tokens_seen": 191104,
      "step": 120
    },
    {
      "epoch": 0.02068022560246112,
      "grad_norm": 0.6208736300468445,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 1.3198,
      "num_input_tokens_seen": 192512,
      "step": 121
    },
    {
      "epoch": 0.02085113655785336,
      "grad_norm": 0.8427135944366455,
      "learning_rate": 4.992631880567301e-05,
      "loss": 1.23,
      "num_input_tokens_seen": 193664,
      "step": 122
    },
    {
      "epoch": 0.021022047513245597,
      "grad_norm": 1.1139335632324219,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.7566,
      "num_input_tokens_seen": 194688,
      "step": 123
    },
    {
      "epoch": 0.021192958468637838,
      "grad_norm": 0.7256762385368347,
      "learning_rate": 4.991232148123761e-05,
      "loss": 1.5274,
      "num_input_tokens_seen": 195968,
      "step": 124
    },
    {
      "epoch": 0.02136386942403008,
      "grad_norm": 0.9797255992889404,
      "learning_rate": 4.990486745229364e-05,
      "loss": 1.046,
      "num_input_tokens_seen": 197248,
      "step": 125
    },
    {
      "epoch": 0.02153478037942232,
      "grad_norm": 0.9680280089378357,
      "learning_rate": 4.989710996539926e-05,
      "loss": 1.4145,
      "num_input_tokens_seen": 198912,
      "step": 126
    },
    {
      "epoch": 0.02170569133481456,
      "grad_norm": 0.6243770718574524,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 1.5319,
      "num_input_tokens_seen": 202240,
      "step": 127
    },
    {
      "epoch": 0.0218766022902068,
      "grad_norm": 1.015550971031189,
      "learning_rate": 4.988068499954578e-05,
      "loss": 1.1199,
      "num_input_tokens_seen": 203904,
      "step": 128
    },
    {
      "epoch": 0.022047513245599042,
      "grad_norm": 0.8970980644226074,
      "learning_rate": 4.987201772071971e-05,
      "loss": 1.5824,
      "num_input_tokens_seen": 205440,
      "step": 129
    },
    {
      "epoch": 0.022218424200991283,
      "grad_norm": 0.7833566665649414,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 1.3764,
      "num_input_tokens_seen": 206848,
      "step": 130
    },
    {
      "epoch": 0.022389335156383524,
      "grad_norm": 0.5131605267524719,
      "learning_rate": 4.985377409930789e-05,
      "loss": 1.4287,
      "num_input_tokens_seen": 208640,
      "step": 131
    },
    {
      "epoch": 0.022560246111775765,
      "grad_norm": 1.2452776432037354,
      "learning_rate": 4.984419797901491e-05,
      "loss": 1.3026,
      "num_input_tokens_seen": 210048,
      "step": 132
    },
    {
      "epoch": 0.022731157067168006,
      "grad_norm": 0.7792142629623413,
      "learning_rate": 4.983431914000991e-05,
      "loss": 1.2221,
      "num_input_tokens_seen": 211712,
      "step": 133
    },
    {
      "epoch": 0.022902068022560246,
      "grad_norm": 0.752255916595459,
      "learning_rate": 4.982413770266342e-05,
      "loss": 1.7612,
      "num_input_tokens_seen": 213248,
      "step": 134
    },
    {
      "epoch": 0.023072978977952487,
      "grad_norm": 1.5053048133850098,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 1.414,
      "num_input_tokens_seen": 214272,
      "step": 135
    },
    {
      "epoch": 0.023243889933344728,
      "grad_norm": 1.0404748916625977,
      "learning_rate": 4.980286753286195e-05,
      "loss": 1.0426,
      "num_input_tokens_seen": 215424,
      "step": 136
    },
    {
      "epoch": 0.02341480088873697,
      "grad_norm": 0.8174368739128113,
      "learning_rate": 4.979177905957726e-05,
      "loss": 1.3731,
      "num_input_tokens_seen": 216960,
      "step": 137
    },
    {
      "epoch": 0.02358571184412921,
      "grad_norm": 1.373140573501587,
      "learning_rate": 4.978038850628854e-05,
      "loss": 1.1085,
      "num_input_tokens_seen": 217984,
      "step": 138
    },
    {
      "epoch": 0.02375662279952145,
      "grad_norm": 0.7178047299385071,
      "learning_rate": 4.976869601178609e-05,
      "loss": 1.3894,
      "num_input_tokens_seen": 220544,
      "step": 139
    },
    {
      "epoch": 0.02392753375491369,
      "grad_norm": 0.9684825539588928,
      "learning_rate": 4.975670171853926e-05,
      "loss": 1.2507,
      "num_input_tokens_seen": 221696,
      "step": 140
    },
    {
      "epoch": 0.024098444710305932,
      "grad_norm": 0.7934660315513611,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 1.2628,
      "num_input_tokens_seen": 222976,
      "step": 141
    },
    {
      "epoch": 0.02426935566569817,
      "grad_norm": 0.8748626112937927,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 1.8698,
      "num_input_tokens_seen": 224512,
      "step": 142
    },
    {
      "epoch": 0.02444026662109041,
      "grad_norm": 1.1227372884750366,
      "learning_rate": 4.971890952617515e-05,
      "loss": 1.3818,
      "num_input_tokens_seen": 225792,
      "step": 143
    },
    {
      "epoch": 0.02461117757648265,
      "grad_norm": 0.9610251784324646,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 1.5521,
      "num_input_tokens_seen": 226816,
      "step": 144
    },
    {
      "epoch": 0.024782088531874892,
      "grad_norm": 0.9209680557250977,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 1.6436,
      "num_input_tokens_seen": 229248,
      "step": 145
    },
    {
      "epoch": 0.024952999487267133,
      "grad_norm": 1.0223954916000366,
      "learning_rate": 4.96784066268247e-05,
      "loss": 1.387,
      "num_input_tokens_seen": 230656,
      "step": 146
    },
    {
      "epoch": 0.025123910442659374,
      "grad_norm": 0.9041973948478699,
      "learning_rate": 4.966430404017424e-05,
      "loss": 1.4107,
      "num_input_tokens_seen": 232064,
      "step": 147
    },
    {
      "epoch": 0.025294821398051615,
      "grad_norm": 0.7746880054473877,
      "learning_rate": 4.964990092676263e-05,
      "loss": 1.4883,
      "num_input_tokens_seen": 233728,
      "step": 148
    },
    {
      "epoch": 0.025465732353443855,
      "grad_norm": 0.8582224249839783,
      "learning_rate": 4.963519746208726e-05,
      "loss": 1.3573,
      "num_input_tokens_seen": 235008,
      "step": 149
    },
    {
      "epoch": 0.025636643308836096,
      "grad_norm": 1.4457972049713135,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.8529,
      "num_input_tokens_seen": 235776,
      "step": 150
    },
    {
      "epoch": 0.025807554264228337,
      "grad_norm": 0.8064973950386047,
      "learning_rate": 4.960489019923105e-05,
      "loss": 1.5248,
      "num_input_tokens_seen": 237440,
      "step": 151
    },
    {
      "epoch": 0.025978465219620578,
      "grad_norm": 1.072171926498413,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 1.2609,
      "num_input_tokens_seen": 238720,
      "step": 152
    },
    {
      "epoch": 0.02614937617501282,
      "grad_norm": 0.9470587372779846,
      "learning_rate": 4.957338372873886e-05,
      "loss": 1.4012,
      "num_input_tokens_seen": 240128,
      "step": 153
    },
    {
      "epoch": 0.02632028713040506,
      "grad_norm": 0.6038584113121033,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.8969,
      "num_input_tokens_seen": 242560,
      "step": 154
    },
    {
      "epoch": 0.0264911980857973,
      "grad_norm": 0.7969830632209778,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 1.4661,
      "num_input_tokens_seen": 244608,
      "step": 155
    },
    {
      "epoch": 0.02666210904118954,
      "grad_norm": 0.9686374664306641,
      "learning_rate": 4.952387888372979e-05,
      "loss": 1.1431,
      "num_input_tokens_seen": 245760,
      "step": 156
    },
    {
      "epoch": 0.026833019996581782,
      "grad_norm": 1.1564379930496216,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 1.4964,
      "num_input_tokens_seen": 247296,
      "step": 157
    },
    {
      "epoch": 0.027003930951974023,
      "grad_norm": 1.0260522365570068,
      "learning_rate": 4.94893812399836e-05,
      "loss": 1.5471,
      "num_input_tokens_seen": 248448,
      "step": 158
    },
    {
      "epoch": 0.027174841907366264,
      "grad_norm": 0.7655704021453857,
      "learning_rate": 4.947168471904213e-05,
      "loss": 1.0457,
      "num_input_tokens_seen": 249856,
      "step": 159
    },
    {
      "epoch": 0.027345752862758504,
      "grad_norm": 1.0952098369598389,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 1.5478,
      "num_input_tokens_seen": 251392,
      "step": 160
    },
    {
      "epoch": 0.027516663818150742,
      "grad_norm": 1.157685399055481,
      "learning_rate": 4.94353973571524e-05,
      "loss": 1.0962,
      "num_input_tokens_seen": 252416,
      "step": 161
    },
    {
      "epoch": 0.027687574773542983,
      "grad_norm": 1.5388200283050537,
      "learning_rate": 4.94168069583542e-05,
      "loss": 1.4709,
      "num_input_tokens_seen": 253696,
      "step": 162
    },
    {
      "epoch": 0.027858485728935223,
      "grad_norm": 1.0423165559768677,
      "learning_rate": 4.939791904846869e-05,
      "loss": 1.2554,
      "num_input_tokens_seen": 255104,
      "step": 163
    },
    {
      "epoch": 0.028029396684327464,
      "grad_norm": 0.9460205435752869,
      "learning_rate": 4.937873385763908e-05,
      "loss": 1.5589,
      "num_input_tokens_seen": 257408,
      "step": 164
    },
    {
      "epoch": 0.028200307639719705,
      "grad_norm": 0.8264089226722717,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 1.4874,
      "num_input_tokens_seen": 259584,
      "step": 165
    },
    {
      "epoch": 0.028371218595111946,
      "grad_norm": 1.0740530490875244,
      "learning_rate": 4.933947257182901e-05,
      "loss": 1.6209,
      "num_input_tokens_seen": 260992,
      "step": 166
    },
    {
      "epoch": 0.028542129550504187,
      "grad_norm": 0.8970735669136047,
      "learning_rate": 4.931939695523492e-05,
      "loss": 1.3864,
      "num_input_tokens_seen": 262400,
      "step": 167
    },
    {
      "epoch": 0.028713040505896428,
      "grad_norm": 0.8494254946708679,
      "learning_rate": 4.929902501446366e-05,
      "loss": 1.3823,
      "num_input_tokens_seen": 264192,
      "step": 168
    },
    {
      "epoch": 0.02888395146128867,
      "grad_norm": 0.9879199266433716,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 1.4577,
      "num_input_tokens_seen": 265728,
      "step": 169
    },
    {
      "epoch": 0.02905486241668091,
      "grad_norm": 1.4643335342407227,
      "learning_rate": 4.925739315689991e-05,
      "loss": 1.3026,
      "num_input_tokens_seen": 267008,
      "step": 170
    },
    {
      "epoch": 0.02922577337207315,
      "grad_norm": 1.2245392799377441,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 1.3462,
      "num_input_tokens_seen": 268416,
      "step": 171
    },
    {
      "epoch": 0.02939668432746539,
      "grad_norm": 0.8849291205406189,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 1.2733,
      "num_input_tokens_seen": 270208,
      "step": 172
    },
    {
      "epoch": 0.029567595282857632,
      "grad_norm": 1.1456185579299927,
      "learning_rate": 4.919272926204929e-05,
      "loss": 1.1124,
      "num_input_tokens_seen": 271232,
      "step": 173
    },
    {
      "epoch": 0.029738506238249873,
      "grad_norm": 1.0209137201309204,
      "learning_rate": 4.917058471511149e-05,
      "loss": 1.2736,
      "num_input_tokens_seen": 272512,
      "step": 174
    },
    {
      "epoch": 0.029909417193642113,
      "grad_norm": 1.1935070753097534,
      "learning_rate": 4.914814565722671e-05,
      "loss": 1.3493,
      "num_input_tokens_seen": 273920,
      "step": 175
    },
    {
      "epoch": 0.030080328149034354,
      "grad_norm": 1.3955111503601074,
      "learning_rate": 4.912541236180779e-05,
      "loss": 1.3044,
      "num_input_tokens_seen": 275072,
      "step": 176
    },
    {
      "epoch": 0.030251239104426595,
      "grad_norm": 1.2924625873565674,
      "learning_rate": 4.910238510585276e-05,
      "loss": 1.2506,
      "num_input_tokens_seen": 275968,
      "step": 177
    },
    {
      "epoch": 0.030422150059818836,
      "grad_norm": 1.1721792221069336,
      "learning_rate": 4.907906416994146e-05,
      "loss": 1.3141,
      "num_input_tokens_seen": 277632,
      "step": 178
    },
    {
      "epoch": 0.030593061015211077,
      "grad_norm": 0.9998344779014587,
      "learning_rate": 4.905544983823214e-05,
      "loss": 1.5414,
      "num_input_tokens_seen": 279808,
      "step": 179
    },
    {
      "epoch": 0.030763971970603314,
      "grad_norm": 1.0309098958969116,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 1.2306,
      "num_input_tokens_seen": 281728,
      "step": 180
    },
    {
      "epoch": 0.030934882925995555,
      "grad_norm": 1.1442972421646118,
      "learning_rate": 4.900734214192358e-05,
      "loss": 1.0028,
      "num_input_tokens_seen": 283520,
      "step": 181
    },
    {
      "epoch": 0.031105793881387796,
      "grad_norm": 0.8100587129592896,
      "learning_rate": 4.898284936350144e-05,
      "loss": 1.2568,
      "num_input_tokens_seen": 285184,
      "step": 182
    },
    {
      "epoch": 0.03127670483678004,
      "grad_norm": 0.8473615050315857,
      "learning_rate": 4.895806436162833e-05,
      "loss": 1.5276,
      "num_input_tokens_seen": 287360,
      "step": 183
    },
    {
      "epoch": 0.03144761579217228,
      "grad_norm": 1.404748797416687,
      "learning_rate": 4.893298743830168e-05,
      "loss": 1.1616,
      "num_input_tokens_seen": 288512,
      "step": 184
    },
    {
      "epoch": 0.03161852674756452,
      "grad_norm": 1.2591317892074585,
      "learning_rate": 4.890761889907589e-05,
      "loss": 1.3266,
      "num_input_tokens_seen": 289792,
      "step": 185
    },
    {
      "epoch": 0.03178943770295676,
      "grad_norm": 1.0237151384353638,
      "learning_rate": 4.888195905305859e-05,
      "loss": 1.2293,
      "num_input_tokens_seen": 291072,
      "step": 186
    },
    {
      "epoch": 0.031960348658349,
      "grad_norm": 0.950425386428833,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 1.3982,
      "num_input_tokens_seen": 292736,
      "step": 187
    },
    {
      "epoch": 0.03213125961374124,
      "grad_norm": 1.0797029733657837,
      "learning_rate": 4.882976669482367e-05,
      "loss": 1.4672,
      "num_input_tokens_seen": 293760,
      "step": 188
    },
    {
      "epoch": 0.03230217056913348,
      "grad_norm": 0.6685189604759216,
      "learning_rate": 4.880323481855347e-05,
      "loss": 1.57,
      "num_input_tokens_seen": 295808,
      "step": 189
    },
    {
      "epoch": 0.03247308152452572,
      "grad_norm": 1.0710910558700562,
      "learning_rate": 4.877641290737884e-05,
      "loss": 1.5428,
      "num_input_tokens_seen": 297088,
      "step": 190
    },
    {
      "epoch": 0.03264399247991796,
      "grad_norm": 1.0919619798660278,
      "learning_rate": 4.874930128811631e-05,
      "loss": 1.5335,
      "num_input_tokens_seen": 298368,
      "step": 191
    },
    {
      "epoch": 0.032814903435310204,
      "grad_norm": 0.8317040801048279,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 1.4324,
      "num_input_tokens_seen": 300672,
      "step": 192
    },
    {
      "epoch": 0.032985814390702445,
      "grad_norm": 1.4305824041366577,
      "learning_rate": 4.869421025023965e-05,
      "loss": 0.8021,
      "num_input_tokens_seen": 302208,
      "step": 193
    },
    {
      "epoch": 0.033156725346094686,
      "grad_norm": 1.5349167585372925,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 1.1039,
      "num_input_tokens_seen": 302976,
      "step": 194
    },
    {
      "epoch": 0.033327636301486926,
      "grad_norm": 0.8238972425460815,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 1.0056,
      "num_input_tokens_seen": 304128,
      "step": 195
    },
    {
      "epoch": 0.03349854725687917,
      "grad_norm": 1.0181448459625244,
      "learning_rate": 4.860940925593703e-05,
      "loss": 1.3315,
      "num_input_tokens_seen": 305536,
      "step": 196
    },
    {
      "epoch": 0.03366945821227141,
      "grad_norm": 0.996379554271698,
      "learning_rate": 4.858056644869002e-05,
      "loss": 1.3188,
      "num_input_tokens_seen": 306688,
      "step": 197
    },
    {
      "epoch": 0.03384036916766365,
      "grad_norm": 1.1317856311798096,
      "learning_rate": 4.855143631968242e-05,
      "loss": 2.0925,
      "num_input_tokens_seen": 308096,
      "step": 198
    },
    {
      "epoch": 0.03401128012305589,
      "grad_norm": 1.0492273569107056,
      "learning_rate": 4.852201922385564e-05,
      "loss": 1.5701,
      "num_input_tokens_seen": 309760,
      "step": 199
    },
    {
      "epoch": 0.03418219107844813,
      "grad_norm": 1.454372763633728,
      "learning_rate": 4.849231551964771e-05,
      "loss": 1.0981,
      "num_input_tokens_seen": 311168,
      "step": 200
    },
    {
      "epoch": 0.03435310203384037,
      "grad_norm": 0.99012291431427,
      "learning_rate": 4.84623255689889e-05,
      "loss": 1.4209,
      "num_input_tokens_seen": 312576,
      "step": 201
    },
    {
      "epoch": 0.03452401298923261,
      "grad_norm": 0.8556084632873535,
      "learning_rate": 4.843204973729729e-05,
      "loss": 1.4112,
      "num_input_tokens_seen": 314496,
      "step": 202
    },
    {
      "epoch": 0.03469492394462485,
      "grad_norm": 0.9420782923698425,
      "learning_rate": 4.840148839347434e-05,
      "loss": 1.3178,
      "num_input_tokens_seen": 316416,
      "step": 203
    },
    {
      "epoch": 0.034865834900017094,
      "grad_norm": 1.0862818956375122,
      "learning_rate": 4.837064190990036e-05,
      "loss": 1.2835,
      "num_input_tokens_seen": 317696,
      "step": 204
    },
    {
      "epoch": 0.035036745855409335,
      "grad_norm": 0.9796823263168335,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 1.6116,
      "num_input_tokens_seen": 318976,
      "step": 205
    },
    {
      "epoch": 0.035207656810801576,
      "grad_norm": 0.9390233755111694,
      "learning_rate": 4.830809503038781e-05,
      "loss": 1.4225,
      "num_input_tokens_seen": 320896,
      "step": 206
    },
    {
      "epoch": 0.035378567766193816,
      "grad_norm": 1.305353045463562,
      "learning_rate": 4.827639539656321e-05,
      "loss": 1.4014,
      "num_input_tokens_seen": 321920,
      "step": 207
    },
    {
      "epoch": 0.03554947872158605,
      "grad_norm": 1.172760009765625,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 1.4886,
      "num_input_tokens_seen": 323200,
      "step": 208
    },
    {
      "epoch": 0.03572038967697829,
      "grad_norm": 0.9354814887046814,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 1.4214,
      "num_input_tokens_seen": 324608,
      "step": 209
    },
    {
      "epoch": 0.03589130063237053,
      "grad_norm": 1.0709277391433716,
      "learning_rate": 4.817959636416969e-05,
      "loss": 1.1099,
      "num_input_tokens_seen": 326272,
      "step": 210
    },
    {
      "epoch": 0.03606221158776277,
      "grad_norm": 1.051326036453247,
      "learning_rate": 4.814676462024988e-05,
      "loss": 1.1899,
      "num_input_tokens_seen": 327552,
      "step": 211
    },
    {
      "epoch": 0.036233122543155014,
      "grad_norm": 0.9647868871688843,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 1.47,
      "num_input_tokens_seen": 328832,
      "step": 212
    },
    {
      "epoch": 0.036404033498547254,
      "grad_norm": 1.1336575746536255,
      "learning_rate": 4.808025542782453e-05,
      "loss": 1.5177,
      "num_input_tokens_seen": 330240,
      "step": 213
    },
    {
      "epoch": 0.036574944453939495,
      "grad_norm": 1.5714493989944458,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 1.3713,
      "num_input_tokens_seen": 331520,
      "step": 214
    },
    {
      "epoch": 0.036745855409331736,
      "grad_norm": 0.9897662401199341,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 1.2436,
      "num_input_tokens_seen": 333184,
      "step": 215
    },
    {
      "epoch": 0.03691676636472398,
      "grad_norm": 1.762487530708313,
      "learning_rate": 4.797838348138086e-05,
      "loss": 1.4796,
      "num_input_tokens_seen": 334208,
      "step": 216
    },
    {
      "epoch": 0.03708767732011622,
      "grad_norm": 1.3659512996673584,
      "learning_rate": 4.794386564209953e-05,
      "loss": 1.3528,
      "num_input_tokens_seen": 335488,
      "step": 217
    },
    {
      "epoch": 0.03725858827550846,
      "grad_norm": 1.1300534009933472,
      "learning_rate": 4.790906823905599e-05,
      "loss": 1.8002,
      "num_input_tokens_seen": 336640,
      "step": 218
    },
    {
      "epoch": 0.0374294992309007,
      "grad_norm": 1.9881300926208496,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 1.485,
      "num_input_tokens_seen": 337408,
      "step": 219
    },
    {
      "epoch": 0.03760041018629294,
      "grad_norm": 1.1061192750930786,
      "learning_rate": 4.783863644106502e-05,
      "loss": 1.5221,
      "num_input_tokens_seen": 339200,
      "step": 220
    },
    {
      "epoch": 0.03777132114168518,
      "grad_norm": 1.0967758893966675,
      "learning_rate": 4.780300290430682e-05,
      "loss": 1.534,
      "num_input_tokens_seen": 341248,
      "step": 221
    },
    {
      "epoch": 0.03794223209707742,
      "grad_norm": 1.0737638473510742,
      "learning_rate": 4.776709152015443e-05,
      "loss": 1.4996,
      "num_input_tokens_seen": 343040,
      "step": 222
    },
    {
      "epoch": 0.03811314305246966,
      "grad_norm": 1.1055270433425903,
      "learning_rate": 4.773090272617672e-05,
      "loss": 1.2643,
      "num_input_tokens_seen": 344320,
      "step": 223
    },
    {
      "epoch": 0.038284054007861903,
      "grad_norm": 0.7960913777351379,
      "learning_rate": 4.769443696332272e-05,
      "loss": 1.2832,
      "num_input_tokens_seen": 345984,
      "step": 224
    },
    {
      "epoch": 0.038454964963254144,
      "grad_norm": 2.0772523880004883,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.8368,
      "num_input_tokens_seen": 346880,
      "step": 225
    },
    {
      "epoch": 0.038625875918646385,
      "grad_norm": 0.8870755434036255,
      "learning_rate": 4.762067631165049e-05,
      "loss": 1.2086,
      "num_input_tokens_seen": 348160,
      "step": 226
    },
    {
      "epoch": 0.038796786874038626,
      "grad_norm": 1.1136736869812012,
      "learning_rate": 4.758338232158252e-05,
      "loss": 1.6904,
      "num_input_tokens_seen": 349568,
      "step": 227
    },
    {
      "epoch": 0.03896769782943087,
      "grad_norm": 1.280674695968628,
      "learning_rate": 4.754581316012785e-05,
      "loss": 1.1628,
      "num_input_tokens_seen": 350848,
      "step": 228
    },
    {
      "epoch": 0.03913860878482311,
      "grad_norm": 0.9846305251121521,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 1.5351,
      "num_input_tokens_seen": 352256,
      "step": 229
    },
    {
      "epoch": 0.03930951974021535,
      "grad_norm": 0.6738191843032837,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 1.2806,
      "num_input_tokens_seen": 355072,
      "step": 230
    },
    {
      "epoch": 0.03948043069560759,
      "grad_norm": 1.5939009189605713,
      "learning_rate": 4.743145924185821e-05,
      "loss": 1.2574,
      "num_input_tokens_seen": 356352,
      "step": 231
    },
    {
      "epoch": 0.03965134165099983,
      "grad_norm": 0.9070621728897095,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 1.3916,
      "num_input_tokens_seen": 358016,
      "step": 232
    },
    {
      "epoch": 0.03982225260639207,
      "grad_norm": 0.9221788048744202,
      "learning_rate": 4.73538559209842e-05,
      "loss": 1.3209,
      "num_input_tokens_seen": 360064,
      "step": 233
    },
    {
      "epoch": 0.03999316356178431,
      "grad_norm": 1.4134471416473389,
      "learning_rate": 4.731464546130314e-05,
      "loss": 0.7197,
      "num_input_tokens_seen": 361344,
      "step": 234
    },
    {
      "epoch": 0.04016407451717655,
      "grad_norm": 1.2092881202697754,
      "learning_rate": 4.72751631047092e-05,
      "loss": 1.9347,
      "num_input_tokens_seen": 363392,
      "step": 235
    },
    {
      "epoch": 0.04033498547256879,
      "grad_norm": 0.9557560086250305,
      "learning_rate": 4.723540933228244e-05,
      "loss": 1.2559,
      "num_input_tokens_seen": 364672,
      "step": 236
    },
    {
      "epoch": 0.040505896427961034,
      "grad_norm": 1.1435261964797974,
      "learning_rate": 4.719538462841003e-05,
      "loss": 1.0759,
      "num_input_tokens_seen": 366592,
      "step": 237
    },
    {
      "epoch": 0.040676807383353275,
      "grad_norm": 1.8531731367111206,
      "learning_rate": 4.715508948078037e-05,
      "loss": 1.1337,
      "num_input_tokens_seen": 367744,
      "step": 238
    },
    {
      "epoch": 0.040847718338745516,
      "grad_norm": 0.8221838474273682,
      "learning_rate": 4.71145243803771e-05,
      "loss": 1.3211,
      "num_input_tokens_seen": 370048,
      "step": 239
    },
    {
      "epoch": 0.04101862929413776,
      "grad_norm": 1.0834380388259888,
      "learning_rate": 4.707368982147318e-05,
      "loss": 1.1553,
      "num_input_tokens_seen": 371200,
      "step": 240
    },
    {
      "epoch": 0.04118954024953,
      "grad_norm": 0.99672532081604,
      "learning_rate": 4.70325863016248e-05,
      "loss": 1.4922,
      "num_input_tokens_seen": 373504,
      "step": 241
    },
    {
      "epoch": 0.04136045120492224,
      "grad_norm": 1.1210763454437256,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 1.0474,
      "num_input_tokens_seen": 374528,
      "step": 242
    },
    {
      "epoch": 0.04153136216031448,
      "grad_norm": 1.2013628482818604,
      "learning_rate": 4.694957438569951e-05,
      "loss": 1.7018,
      "num_input_tokens_seen": 375936,
      "step": 243
    },
    {
      "epoch": 0.04170227311570672,
      "grad_norm": 1.6756564378738403,
      "learning_rate": 4.690766700109659e-05,
      "loss": 1.0,
      "num_input_tokens_seen": 376960,
      "step": 244
    },
    {
      "epoch": 0.04187318407109896,
      "grad_norm": 2.2576847076416016,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 1.3284,
      "num_input_tokens_seen": 377856,
      "step": 245
    },
    {
      "epoch": 0.042044095026491195,
      "grad_norm": 1.99846351146698,
      "learning_rate": 4.682305193174524e-05,
      "loss": 1.6023,
      "num_input_tokens_seen": 379136,
      "step": 246
    },
    {
      "epoch": 0.042215005981883436,
      "grad_norm": 0.9843366146087646,
      "learning_rate": 4.678034527800474e-05,
      "loss": 1.1592,
      "num_input_tokens_seen": 380544,
      "step": 247
    },
    {
      "epoch": 0.042385916937275676,
      "grad_norm": 1.2811005115509033,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 1.5375,
      "num_input_tokens_seen": 381952,
      "step": 248
    },
    {
      "epoch": 0.04255682789266792,
      "grad_norm": 1.4090063571929932,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.9976,
      "num_input_tokens_seen": 382976,
      "step": 249
    },
    {
      "epoch": 0.04272773884806016,
      "grad_norm": 1.4001675844192505,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.9084,
      "num_input_tokens_seen": 383872,
      "step": 250
    },
    {
      "epoch": 0.0428986498034524,
      "grad_norm": 1.134788990020752,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 1.2541,
      "num_input_tokens_seen": 385280,
      "step": 251
    },
    {
      "epoch": 0.04306956075884464,
      "grad_norm": 1.0920721292495728,
      "learning_rate": 4.656284173018144e-05,
      "loss": 1.3336,
      "num_input_tokens_seen": 387200,
      "step": 252
    },
    {
      "epoch": 0.04324047171423688,
      "grad_norm": 1.4255011081695557,
      "learning_rate": 4.65185506750986e-05,
      "loss": 1.3513,
      "num_input_tokens_seen": 388352,
      "step": 253
    },
    {
      "epoch": 0.04341138266962912,
      "grad_norm": 1.4705305099487305,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 1.657,
      "num_input_tokens_seen": 389888,
      "step": 254
    },
    {
      "epoch": 0.04358229362502136,
      "grad_norm": 1.1332073211669922,
      "learning_rate": 4.642918251755281e-05,
      "loss": 1.6068,
      "num_input_tokens_seen": 391424,
      "step": 255
    },
    {
      "epoch": 0.0437532045804136,
      "grad_norm": 0.718579888343811,
      "learning_rate": 4.638410650401267e-05,
      "loss": 1.4541,
      "num_input_tokens_seen": 393856,
      "step": 256
    },
    {
      "epoch": 0.043924115535805844,
      "grad_norm": 1.484248399734497,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 1.3209,
      "num_input_tokens_seen": 395136,
      "step": 257
    },
    {
      "epoch": 0.044095026491198085,
      "grad_norm": 1.1124175786972046,
      "learning_rate": 4.629317335357619e-05,
      "loss": 1.6722,
      "num_input_tokens_seen": 396928,
      "step": 258
    },
    {
      "epoch": 0.044265937446590325,
      "grad_norm": 1.3876971006393433,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 1.2926,
      "num_input_tokens_seen": 398464,
      "step": 259
    },
    {
      "epoch": 0.044436848401982566,
      "grad_norm": 1.3132555484771729,
      "learning_rate": 4.620120240391065e-05,
      "loss": 1.1658,
      "num_input_tokens_seen": 399488,
      "step": 260
    },
    {
      "epoch": 0.04460775935737481,
      "grad_norm": 1.2301092147827148,
      "learning_rate": 4.615482915318911e-05,
      "loss": 1.1776,
      "num_input_tokens_seen": 401024,
      "step": 261
    },
    {
      "epoch": 0.04477867031276705,
      "grad_norm": 0.9394013285636902,
      "learning_rate": 4.610819813755038e-05,
      "loss": 1.5584,
      "num_input_tokens_seen": 402560,
      "step": 262
    },
    {
      "epoch": 0.04494958126815929,
      "grad_norm": 1.2185633182525635,
      "learning_rate": 4.606130992517869e-05,
      "loss": 1.4395,
      "num_input_tokens_seen": 403840,
      "step": 263
    },
    {
      "epoch": 0.04512049222355153,
      "grad_norm": 0.9794286489486694,
      "learning_rate": 4.601416508739211e-05,
      "loss": 1.4828,
      "num_input_tokens_seen": 405120,
      "step": 264
    },
    {
      "epoch": 0.04529140317894377,
      "grad_norm": 0.7681525945663452,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 1.5926,
      "num_input_tokens_seen": 407424,
      "step": 265
    },
    {
      "epoch": 0.04546231413433601,
      "grad_norm": 1.1325764656066895,
      "learning_rate": 4.591910783647404e-05,
      "loss": 1.3596,
      "num_input_tokens_seen": 408704,
      "step": 266
    },
    {
      "epoch": 0.04563322508972825,
      "grad_norm": 1.3376734256744385,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 1.0783,
      "num_input_tokens_seen": 409984,
      "step": 267
    },
    {
      "epoch": 0.04580413604512049,
      "grad_norm": 1.079081416130066,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 1.2041,
      "num_input_tokens_seen": 411392,
      "step": 268
    },
    {
      "epoch": 0.045975047000512734,
      "grad_norm": 0.9778527617454529,
      "learning_rate": 4.577461173185821e-05,
      "loss": 1.0877,
      "num_input_tokens_seen": 414080,
      "step": 269
    },
    {
      "epoch": 0.046145957955904975,
      "grad_norm": 1.2032514810562134,
      "learning_rate": 4.572593931387604e-05,
      "loss": 1.1698,
      "num_input_tokens_seen": 414976,
      "step": 270
    },
    {
      "epoch": 0.046316868911297215,
      "grad_norm": 1.2221482992172241,
      "learning_rate": 4.567701435686404e-05,
      "loss": 1.4688,
      "num_input_tokens_seen": 415872,
      "step": 271
    },
    {
      "epoch": 0.046487779866689456,
      "grad_norm": 1.1100445985794067,
      "learning_rate": 4.562783745695738e-05,
      "loss": 1.3855,
      "num_input_tokens_seen": 417152,
      "step": 272
    },
    {
      "epoch": 0.0466586908220817,
      "grad_norm": 1.1946079730987549,
      "learning_rate": 4.557840921336105e-05,
      "loss": 1.2361,
      "num_input_tokens_seen": 418304,
      "step": 273
    },
    {
      "epoch": 0.04682960177747394,
      "grad_norm": 1.6539386510849,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 1.2471,
      "num_input_tokens_seen": 419328,
      "step": 274
    },
    {
      "epoch": 0.04700051273286618,
      "grad_norm": 1.0587725639343262,
      "learning_rate": 4.54788011072248e-05,
      "loss": 1.634,
      "num_input_tokens_seen": 421376,
      "step": 275
    },
    {
      "epoch": 0.04717142368825842,
      "grad_norm": 1.0927001237869263,
      "learning_rate": 4.542862245837821e-05,
      "loss": 1.3076,
      "num_input_tokens_seen": 422400,
      "step": 276
    },
    {
      "epoch": 0.04734233464365066,
      "grad_norm": 0.9012113213539124,
      "learning_rate": 4.537819489321386e-05,
      "loss": 1.3003,
      "num_input_tokens_seen": 424832,
      "step": 277
    },
    {
      "epoch": 0.0475132455990429,
      "grad_norm": 1.1746714115142822,
      "learning_rate": 4.532751902617569e-05,
      "loss": 1.2994,
      "num_input_tokens_seen": 425984,
      "step": 278
    },
    {
      "epoch": 0.04768415655443514,
      "grad_norm": 1.4915165901184082,
      "learning_rate": 4.527659547473317e-05,
      "loss": 1.4697,
      "num_input_tokens_seen": 427392,
      "step": 279
    },
    {
      "epoch": 0.04785506750982738,
      "grad_norm": 0.9594665765762329,
      "learning_rate": 4.522542485937369e-05,
      "loss": 1.1534,
      "num_input_tokens_seen": 432128,
      "step": 280
    },
    {
      "epoch": 0.048025978465219624,
      "grad_norm": 1.0884348154067993,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 1.379,
      "num_input_tokens_seen": 433920,
      "step": 281
    },
    {
      "epoch": 0.048196889420611864,
      "grad_norm": 1.2012864351272583,
      "learning_rate": 4.512234493389785e-05,
      "loss": 1.0039,
      "num_input_tokens_seen": 435072,
      "step": 282
    },
    {
      "epoch": 0.048367800376004105,
      "grad_norm": 1.0973515510559082,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 1.1803,
      "num_input_tokens_seen": 436736,
      "step": 283
    },
    {
      "epoch": 0.04853871133139634,
      "grad_norm": 1.1268903017044067,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 1.3355,
      "num_input_tokens_seen": 438016,
      "step": 284
    },
    {
      "epoch": 0.04870962228678858,
      "grad_norm": 1.2013931274414062,
      "learning_rate": 4.496588775118232e-05,
      "loss": 1.4476,
      "num_input_tokens_seen": 439296,
      "step": 285
    },
    {
      "epoch": 0.04888053324218082,
      "grad_norm": 1.1419507265090942,
      "learning_rate": 4.491324795060491e-05,
      "loss": 1.1526,
      "num_input_tokens_seen": 440704,
      "step": 286
    },
    {
      "epoch": 0.04905144419757306,
      "grad_norm": 1.2760834693908691,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.8693,
      "num_input_tokens_seen": 441856,
      "step": 287
    },
    {
      "epoch": 0.0492223551529653,
      "grad_norm": 1.198560118675232,
      "learning_rate": 4.480724108387977e-05,
      "loss": 1.7554,
      "num_input_tokens_seen": 443136,
      "step": 288
    },
    {
      "epoch": 0.04939326610835754,
      "grad_norm": 1.3139100074768066,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 1.4298,
      "num_input_tokens_seen": 445312,
      "step": 289
    },
    {
      "epoch": 0.049564177063749784,
      "grad_norm": 1.6930724382400513,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 1.151,
      "num_input_tokens_seen": 446592,
      "step": 290
    },
    {
      "epoch": 0.049735088019142025,
      "grad_norm": 0.9581266045570374,
      "learning_rate": 4.464642232938505e-05,
      "loss": 1.1656,
      "num_input_tokens_seen": 448128,
      "step": 291
    },
    {
      "epoch": 0.049905998974534266,
      "grad_norm": 1.2575232982635498,
      "learning_rate": 4.4592336433146e-05,
      "loss": 1.07,
      "num_input_tokens_seen": 450432,
      "step": 292
    },
    {
      "epoch": 0.05007690992992651,
      "grad_norm": 1.2482130527496338,
      "learning_rate": 4.453801181047047e-05,
      "loss": 1.609,
      "num_input_tokens_seen": 451584,
      "step": 293
    },
    {
      "epoch": 0.05024782088531875,
      "grad_norm": 1.3409042358398438,
      "learning_rate": 4.448344912328686e-05,
      "loss": 1.5752,
      "num_input_tokens_seen": 453120,
      "step": 294
    },
    {
      "epoch": 0.05041873184071099,
      "grad_norm": 0.9807044267654419,
      "learning_rate": 4.442864903642428e-05,
      "loss": 1.4863,
      "num_input_tokens_seen": 455552,
      "step": 295
    },
    {
      "epoch": 0.05058964279610323,
      "grad_norm": 1.158429503440857,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 1.2137,
      "num_input_tokens_seen": 456704,
      "step": 296
    },
    {
      "epoch": 0.05076055375149547,
      "grad_norm": 1.4021419286727905,
      "learning_rate": 4.431833933743378e-05,
      "loss": 1.0144,
      "num_input_tokens_seen": 457728,
      "step": 297
    },
    {
      "epoch": 0.05093146470688771,
      "grad_norm": 0.9592480659484863,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.9837,
      "num_input_tokens_seen": 460416,
      "step": 298
    },
    {
      "epoch": 0.05110237566227995,
      "grad_norm": 0.8851501941680908,
      "learning_rate": 4.420708808983809e-05,
      "loss": 1.3028,
      "num_input_tokens_seen": 463616,
      "step": 299
    },
    {
      "epoch": 0.05127328661767219,
      "grad_norm": 1.0420891046524048,
      "learning_rate": 4.415111107797445e-05,
      "loss": 1.5192,
      "num_input_tokens_seen": 465664,
      "step": 300
    },
    {
      "epoch": 0.05144419757306443,
      "grad_norm": 1.1155275106430054,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.9059,
      "num_input_tokens_seen": 467328,
      "step": 301
    },
    {
      "epoch": 0.051615108528456674,
      "grad_norm": 1.522392988204956,
      "learning_rate": 4.403845768841842e-05,
      "loss": 1.9665,
      "num_input_tokens_seen": 468864,
      "step": 302
    },
    {
      "epoch": 0.051786019483848915,
      "grad_norm": 0.9271745085716248,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 1.4082,
      "num_input_tokens_seen": 470144,
      "step": 303
    },
    {
      "epoch": 0.051956930439241156,
      "grad_norm": 0.9030784368515015,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 1.3172,
      "num_input_tokens_seen": 471424,
      "step": 304
    },
    {
      "epoch": 0.0521278413946334,
      "grad_norm": 2.264300584793091,
      "learning_rate": 4.386773950556931e-05,
      "loss": 1.141,
      "num_input_tokens_seen": 472320,
      "step": 305
    },
    {
      "epoch": 0.05229875235002564,
      "grad_norm": 0.8233048319816589,
      "learning_rate": 4.381037272239311e-05,
      "loss": 1.2614,
      "num_input_tokens_seen": 473728,
      "step": 306
    },
    {
      "epoch": 0.05246966330541788,
      "grad_norm": 0.9681416153907776,
      "learning_rate": 4.375277674076149e-05,
      "loss": 1.293,
      "num_input_tokens_seen": 474880,
      "step": 307
    },
    {
      "epoch": 0.05264057426081012,
      "grad_norm": 0.9580039978027344,
      "learning_rate": 4.36949522624633e-05,
      "loss": 1.3295,
      "num_input_tokens_seen": 476288,
      "step": 308
    },
    {
      "epoch": 0.05281148521620236,
      "grad_norm": 1.5450685024261475,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.7689,
      "num_input_tokens_seen": 477568,
      "step": 309
    },
    {
      "epoch": 0.0529823961715946,
      "grad_norm": 0.7666329741477966,
      "learning_rate": 4.357862063693486e-05,
      "loss": 1.1508,
      "num_input_tokens_seen": 480512,
      "step": 310
    },
    {
      "epoch": 0.05315330712698684,
      "grad_norm": 1.9918466806411743,
      "learning_rate": 4.352011490716875e-05,
      "loss": 1.4352,
      "num_input_tokens_seen": 481536,
      "step": 311
    },
    {
      "epoch": 0.05332421808237908,
      "grad_norm": 0.8122378587722778,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 1.2795,
      "num_input_tokens_seen": 483712,
      "step": 312
    },
    {
      "epoch": 0.05349512903777132,
      "grad_norm": 0.8766465783119202,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 1.4912,
      "num_input_tokens_seen": 486144,
      "step": 313
    },
    {
      "epoch": 0.053666039993163564,
      "grad_norm": 1.0563558340072632,
      "learning_rate": 4.334324661257191e-05,
      "loss": 1.4116,
      "num_input_tokens_seen": 487808,
      "step": 314
    },
    {
      "epoch": 0.053836950948555805,
      "grad_norm": 1.1601818799972534,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 1.2271,
      "num_input_tokens_seen": 489088,
      "step": 315
    },
    {
      "epoch": 0.054007861903948046,
      "grad_norm": 1.0149890184402466,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 0.9513,
      "num_input_tokens_seen": 490624,
      "step": 316
    },
    {
      "epoch": 0.054178772859340286,
      "grad_norm": 1.195574402809143,
      "learning_rate": 4.31643667742744e-05,
      "loss": 1.3472,
      "num_input_tokens_seen": 492160,
      "step": 317
    },
    {
      "epoch": 0.05434968381473253,
      "grad_norm": 1.8042011260986328,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.9957,
      "num_input_tokens_seen": 493440,
      "step": 318
    },
    {
      "epoch": 0.05452059477012477,
      "grad_norm": 1.1676393747329712,
      "learning_rate": 4.304400570245906e-05,
      "loss": 1.4202,
      "num_input_tokens_seen": 494976,
      "step": 319
    },
    {
      "epoch": 0.05469150572551701,
      "grad_norm": 1.2366454601287842,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 1.3151,
      "num_input_tokens_seen": 496384,
      "step": 320
    },
    {
      "epoch": 0.05486241668090924,
      "grad_norm": 1.3729883432388306,
      "learning_rate": 4.292276519126207e-05,
      "loss": 1.0207,
      "num_input_tokens_seen": 497536,
      "step": 321
    },
    {
      "epoch": 0.055033327636301484,
      "grad_norm": 1.675207257270813,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 1.2772,
      "num_input_tokens_seen": 498816,
      "step": 322
    },
    {
      "epoch": 0.055204238591693724,
      "grad_norm": 1.1025292873382568,
      "learning_rate": 4.280065114977492e-05,
      "loss": 1.6032,
      "num_input_tokens_seen": 501504,
      "step": 323
    },
    {
      "epoch": 0.055375149547085965,
      "grad_norm": 1.159480094909668,
      "learning_rate": 4.273926841341302e-05,
      "loss": 1.5473,
      "num_input_tokens_seen": 503424,
      "step": 324
    },
    {
      "epoch": 0.055546060502478206,
      "grad_norm": 1.0430818796157837,
      "learning_rate": 4.267766952966369e-05,
      "loss": 1.217,
      "num_input_tokens_seen": 504832,
      "step": 325
    },
    {
      "epoch": 0.05571697145787045,
      "grad_norm": 0.9906388521194458,
      "learning_rate": 4.261585524908987e-05,
      "loss": 1.1401,
      "num_input_tokens_seen": 506496,
      "step": 326
    },
    {
      "epoch": 0.05588788241326269,
      "grad_norm": 0.9924626350402832,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 1.2145,
      "num_input_tokens_seen": 508032,
      "step": 327
    },
    {
      "epoch": 0.05605879336865493,
      "grad_norm": 1.1876827478408813,
      "learning_rate": 4.249158351283414e-05,
      "loss": 1.0405,
      "num_input_tokens_seen": 509568,
      "step": 328
    },
    {
      "epoch": 0.05622970432404717,
      "grad_norm": 1.0996421575546265,
      "learning_rate": 4.242912757136412e-05,
      "loss": 1.3766,
      "num_input_tokens_seen": 510848,
      "step": 329
    },
    {
      "epoch": 0.05640061527943941,
      "grad_norm": 0.9395371675491333,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 1.3983,
      "num_input_tokens_seen": 513664,
      "step": 330
    },
    {
      "epoch": 0.05657152623483165,
      "grad_norm": 0.9114913940429688,
      "learning_rate": 4.230357934676017e-05,
      "loss": 1.2363,
      "num_input_tokens_seen": 514944,
      "step": 331
    },
    {
      "epoch": 0.05674243719022389,
      "grad_norm": 1.837098240852356,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.8342,
      "num_input_tokens_seen": 515968,
      "step": 332
    },
    {
      "epoch": 0.05691334814561613,
      "grad_norm": 0.660455048084259,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.4806,
      "num_input_tokens_seen": 520320,
      "step": 333
    },
    {
      "epoch": 0.057084259101008374,
      "grad_norm": 0.9063498377799988,
      "learning_rate": 4.211367764821722e-05,
      "loss": 1.406,
      "num_input_tokens_seen": 522880,
      "step": 334
    },
    {
      "epoch": 0.057255170056400614,
      "grad_norm": 1.4167572259902954,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 1.3685,
      "num_input_tokens_seen": 524160,
      "step": 335
    },
    {
      "epoch": 0.057426081011792855,
      "grad_norm": 1.1179454326629639,
      "learning_rate": 4.198603260653792e-05,
      "loss": 1.4785,
      "num_input_tokens_seen": 526336,
      "step": 336
    },
    {
      "epoch": 0.057596991967185096,
      "grad_norm": 0.9438394904136658,
      "learning_rate": 4.192189924206652e-05,
      "loss": 1.5695,
      "num_input_tokens_seen": 528384,
      "step": 337
    },
    {
      "epoch": 0.05776790292257734,
      "grad_norm": 1.515954852104187,
      "learning_rate": 4.185755968959308e-05,
      "loss": 1.276,
      "num_input_tokens_seen": 529664,
      "step": 338
    },
    {
      "epoch": 0.05793881387796958,
      "grad_norm": 1.2532610893249512,
      "learning_rate": 4.179301473307476e-05,
      "loss": 1.3424,
      "num_input_tokens_seen": 530944,
      "step": 339
    },
    {
      "epoch": 0.05810972483336182,
      "grad_norm": 1.080030918121338,
      "learning_rate": 4.172826515897146e-05,
      "loss": 1.0052,
      "num_input_tokens_seen": 532352,
      "step": 340
    },
    {
      "epoch": 0.05828063578875406,
      "grad_norm": 0.996215283870697,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.5056,
      "num_input_tokens_seen": 534144,
      "step": 341
    },
    {
      "epoch": 0.0584515467441463,
      "grad_norm": 0.7477972507476807,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 1.0091,
      "num_input_tokens_seen": 537472,
      "step": 342
    },
    {
      "epoch": 0.05862245769953854,
      "grad_norm": 1.0495474338531494,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 1.124,
      "num_input_tokens_seen": 539776,
      "step": 343
    },
    {
      "epoch": 0.05879336865493078,
      "grad_norm": 1.6348153352737427,
      "learning_rate": 4.146723650296701e-05,
      "loss": 1.5989,
      "num_input_tokens_seen": 541184,
      "step": 344
    },
    {
      "epoch": 0.05896427961032302,
      "grad_norm": 1.2466942071914673,
      "learning_rate": 4.140147572476268e-05,
      "loss": 1.33,
      "num_input_tokens_seen": 542336,
      "step": 345
    },
    {
      "epoch": 0.059135190565715263,
      "grad_norm": 1.156226634979248,
      "learning_rate": 4.133551509975264e-05,
      "loss": 1.2134,
      "num_input_tokens_seen": 543488,
      "step": 346
    },
    {
      "epoch": 0.059306101521107504,
      "grad_norm": 1.3470704555511475,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 1.3251,
      "num_input_tokens_seen": 544768,
      "step": 347
    },
    {
      "epoch": 0.059477012476499745,
      "grad_norm": 1.2357525825500488,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 1.4592,
      "num_input_tokens_seen": 546688,
      "step": 348
    },
    {
      "epoch": 0.059647923431891986,
      "grad_norm": 1.2814292907714844,
      "learning_rate": 4.113644219309877e-05,
      "loss": 1.4041,
      "num_input_tokens_seen": 547840,
      "step": 349
    },
    {
      "epoch": 0.05981883438728423,
      "grad_norm": 1.486798882484436,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 1.5237,
      "num_input_tokens_seen": 548992,
      "step": 350
    },
    {
      "epoch": 0.05998974534267647,
      "grad_norm": 1.000107765197754,
      "learning_rate": 4.100274248712389e-05,
      "loss": 1.1124,
      "num_input_tokens_seen": 551168,
      "step": 351
    },
    {
      "epoch": 0.06016065629806871,
      "grad_norm": 1.184990406036377,
      "learning_rate": 4.093559974371725e-05,
      "loss": 1.1992,
      "num_input_tokens_seen": 552576,
      "step": 352
    },
    {
      "epoch": 0.06033156725346095,
      "grad_norm": 1.0087660551071167,
      "learning_rate": 4.086826283005669e-05,
      "loss": 1.2163,
      "num_input_tokens_seen": 553728,
      "step": 353
    },
    {
      "epoch": 0.06050247820885319,
      "grad_norm": 1.2171785831451416,
      "learning_rate": 4.080073256662127e-05,
      "loss": 1.6592,
      "num_input_tokens_seen": 556544,
      "step": 354
    },
    {
      "epoch": 0.06067338916424543,
      "grad_norm": 1.2538293600082397,
      "learning_rate": 4.073300977624594e-05,
      "loss": 1.5928,
      "num_input_tokens_seen": 557824,
      "step": 355
    },
    {
      "epoch": 0.06084430011963767,
      "grad_norm": 1.078086018562317,
      "learning_rate": 4.066509528411152e-05,
      "loss": 1.2254,
      "num_input_tokens_seen": 559488,
      "step": 356
    },
    {
      "epoch": 0.06101521107502991,
      "grad_norm": 1.0838961601257324,
      "learning_rate": 4.059698991773466e-05,
      "loss": 1.3384,
      "num_input_tokens_seen": 560896,
      "step": 357
    },
    {
      "epoch": 0.06118612203042215,
      "grad_norm": 1.300172209739685,
      "learning_rate": 4.052869450695776e-05,
      "loss": 1.0054,
      "num_input_tokens_seen": 561920,
      "step": 358
    },
    {
      "epoch": 0.06135703298581439,
      "grad_norm": 1.3555691242218018,
      "learning_rate": 4.046020988393885e-05,
      "loss": 1.5425,
      "num_input_tokens_seen": 563072,
      "step": 359
    },
    {
      "epoch": 0.06152794394120663,
      "grad_norm": 1.4978365898132324,
      "learning_rate": 4.039153688314145e-05,
      "loss": 1.0294,
      "num_input_tokens_seen": 564480,
      "step": 360
    },
    {
      "epoch": 0.06169885489659887,
      "grad_norm": 1.132871389389038,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 1.2747,
      "num_input_tokens_seen": 565760,
      "step": 361
    },
    {
      "epoch": 0.06186976585199111,
      "grad_norm": 1.666398286819458,
      "learning_rate": 4.02536290975317e-05,
      "loss": 1.248,
      "num_input_tokens_seen": 566784,
      "step": 362
    },
    {
      "epoch": 0.06204067680738335,
      "grad_norm": 0.8819852471351624,
      "learning_rate": 4.018439599308217e-05,
      "loss": 1.6202,
      "num_input_tokens_seen": 569088,
      "step": 363
    },
    {
      "epoch": 0.06221158776277559,
      "grad_norm": 0.9172688722610474,
      "learning_rate": 4.011497787155938e-05,
      "loss": 1.2708,
      "num_input_tokens_seen": 572288,
      "step": 364
    },
    {
      "epoch": 0.06238249871816783,
      "grad_norm": 1.4169292449951172,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 1.4832,
      "num_input_tokens_seen": 573824,
      "step": 365
    },
    {
      "epoch": 0.06255340967356007,
      "grad_norm": 1.3159610033035278,
      "learning_rate": 3.997558996288965e-05,
      "loss": 1.258,
      "num_input_tokens_seen": 574976,
      "step": 366
    },
    {
      "epoch": 0.06272432062895232,
      "grad_norm": 1.0336920022964478,
      "learning_rate": 3.99056218741404e-05,
      "loss": 1.0697,
      "num_input_tokens_seen": 576512,
      "step": 367
    },
    {
      "epoch": 0.06289523158434455,
      "grad_norm": 0.9077314138412476,
      "learning_rate": 3.983547216509254e-05,
      "loss": 1.5217,
      "num_input_tokens_seen": 578432,
      "step": 368
    },
    {
      "epoch": 0.0630661425397368,
      "grad_norm": 0.9925026893615723,
      "learning_rate": 3.976514169049814e-05,
      "loss": 1.4519,
      "num_input_tokens_seen": 579968,
      "step": 369
    },
    {
      "epoch": 0.06323705349512904,
      "grad_norm": 1.0111063718795776,
      "learning_rate": 3.969463130731183e-05,
      "loss": 1.5442,
      "num_input_tokens_seen": 582400,
      "step": 370
    },
    {
      "epoch": 0.06340796445052128,
      "grad_norm": 1.449090600013733,
      "learning_rate": 3.962394187468039e-05,
      "loss": 1.8827,
      "num_input_tokens_seen": 583936,
      "step": 371
    },
    {
      "epoch": 0.06357887540591352,
      "grad_norm": 1.318198323249817,
      "learning_rate": 3.955307425393224e-05,
      "loss": 1.6374,
      "num_input_tokens_seen": 585088,
      "step": 372
    },
    {
      "epoch": 0.06374978636130577,
      "grad_norm": 1.2825038433074951,
      "learning_rate": 3.948202930856697e-05,
      "loss": 1.8848,
      "num_input_tokens_seen": 587008,
      "step": 373
    },
    {
      "epoch": 0.063920697316698,
      "grad_norm": 1.9453885555267334,
      "learning_rate": 3.941080790424484e-05,
      "loss": 1.141,
      "num_input_tokens_seen": 587904,
      "step": 374
    },
    {
      "epoch": 0.06409160827209025,
      "grad_norm": 1.2809176445007324,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.6734,
      "num_input_tokens_seen": 589312,
      "step": 375
    },
    {
      "epoch": 0.06426251922748248,
      "grad_norm": 1.309374213218689,
      "learning_rate": 3.92678391921108e-05,
      "loss": 1.3452,
      "num_input_tokens_seen": 590464,
      "step": 376
    },
    {
      "epoch": 0.06443343018287473,
      "grad_norm": 1.1278845071792603,
      "learning_rate": 3.919609362632753e-05,
      "loss": 1.3333,
      "num_input_tokens_seen": 592128,
      "step": 377
    },
    {
      "epoch": 0.06460434113826696,
      "grad_norm": 0.9720110893249512,
      "learning_rate": 3.912417508562345e-05,
      "loss": 1.1876,
      "num_input_tokens_seen": 593664,
      "step": 378
    },
    {
      "epoch": 0.0647752520936592,
      "grad_norm": 1.1381319761276245,
      "learning_rate": 3.905208444630327e-05,
      "loss": 1.0123,
      "num_input_tokens_seen": 594816,
      "step": 379
    },
    {
      "epoch": 0.06494616304905144,
      "grad_norm": 1.1026537418365479,
      "learning_rate": 3.897982258676867e-05,
      "loss": 1.0268,
      "num_input_tokens_seen": 595968,
      "step": 380
    },
    {
      "epoch": 0.06511707400444368,
      "grad_norm": 1.6464358568191528,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 1.3947,
      "num_input_tokens_seen": 597120,
      "step": 381
    },
    {
      "epoch": 0.06528798495983593,
      "grad_norm": 1.4088376760482788,
      "learning_rate": 3.883478873108361e-05,
      "loss": 1.2737,
      "num_input_tokens_seen": 599424,
      "step": 382
    },
    {
      "epoch": 0.06545889591522816,
      "grad_norm": 1.372258186340332,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 1.1619,
      "num_input_tokens_seen": 600576,
      "step": 383
    },
    {
      "epoch": 0.06562980687062041,
      "grad_norm": 1.4650169610977173,
      "learning_rate": 3.868908058731376e-05,
      "loss": 1.3937,
      "num_input_tokens_seen": 601984,
      "step": 384
    },
    {
      "epoch": 0.06580071782601264,
      "grad_norm": 1.130495309829712,
      "learning_rate": 3.861597587537568e-05,
      "loss": 1.3176,
      "num_input_tokens_seen": 603264,
      "step": 385
    },
    {
      "epoch": 0.06597162878140489,
      "grad_norm": 1.2027029991149902,
      "learning_rate": 3.85427052570685e-05,
      "loss": 1.3229,
      "num_input_tokens_seen": 605184,
      "step": 386
    },
    {
      "epoch": 0.06614253973679712,
      "grad_norm": 1.328189492225647,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 1.3206,
      "num_input_tokens_seen": 606848,
      "step": 387
    },
    {
      "epoch": 0.06631345069218937,
      "grad_norm": 1.3082104921340942,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 1.7461,
      "num_input_tokens_seen": 608768,
      "step": 388
    },
    {
      "epoch": 0.0664843616475816,
      "grad_norm": 1.41118323802948,
      "learning_rate": 3.832190690176825e-05,
      "loss": 1.203,
      "num_input_tokens_seen": 610176,
      "step": 389
    },
    {
      "epoch": 0.06665527260297385,
      "grad_norm": 0.9885615706443787,
      "learning_rate": 3.824798160583012e-05,
      "loss": 1.3439,
      "num_input_tokens_seen": 611840,
      "step": 390
    },
    {
      "epoch": 0.06682618355836609,
      "grad_norm": 1.3730019330978394,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 1.1555,
      "num_input_tokens_seen": 613248,
      "step": 391
    },
    {
      "epoch": 0.06699709451375833,
      "grad_norm": 1.0864603519439697,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 1.0853,
      "num_input_tokens_seen": 614912,
      "step": 392
    },
    {
      "epoch": 0.06716800546915057,
      "grad_norm": 1.7290635108947754,
      "learning_rate": 3.802524079601442e-05,
      "loss": 1.2145,
      "num_input_tokens_seen": 615936,
      "step": 393
    },
    {
      "epoch": 0.06733891642454282,
      "grad_norm": 1.0602047443389893,
      "learning_rate": 3.795067523432826e-05,
      "loss": 1.1114,
      "num_input_tokens_seen": 617344,
      "step": 394
    },
    {
      "epoch": 0.06750982737993505,
      "grad_norm": 1.235655426979065,
      "learning_rate": 3.787595187275136e-05,
      "loss": 1.4588,
      "num_input_tokens_seen": 619008,
      "step": 395
    },
    {
      "epoch": 0.0676807383353273,
      "grad_norm": 1.0760589838027954,
      "learning_rate": 3.780107162176429e-05,
      "loss": 1.3696,
      "num_input_tokens_seen": 620544,
      "step": 396
    },
    {
      "epoch": 0.06785164929071953,
      "grad_norm": 0.9407654404640198,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 1.3654,
      "num_input_tokens_seen": 622464,
      "step": 397
    },
    {
      "epoch": 0.06802256024611178,
      "grad_norm": 1.618245244026184,
      "learning_rate": 3.765084410302909e-05,
      "loss": 1.372,
      "num_input_tokens_seen": 623872,
      "step": 398
    },
    {
      "epoch": 0.06819347120150401,
      "grad_norm": 1.0576452016830444,
      "learning_rate": 3.757549866575588e-05,
      "loss": 1.5479,
      "num_input_tokens_seen": 625792,
      "step": 399
    },
    {
      "epoch": 0.06836438215689626,
      "grad_norm": 0.9757975339889526,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.8525,
      "num_input_tokens_seen": 627456,
      "step": 400
    },
    {
      "epoch": 0.0685352931122885,
      "grad_norm": 1.0617526769638062,
      "learning_rate": 3.742434902568889e-05,
      "loss": 1.2672,
      "num_input_tokens_seen": 628736,
      "step": 401
    },
    {
      "epoch": 0.06870620406768074,
      "grad_norm": 1.271671175956726,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 1.3097,
      "num_input_tokens_seen": 630144,
      "step": 402
    },
    {
      "epoch": 0.06887711502307298,
      "grad_norm": 1.062774658203125,
      "learning_rate": 3.727259384037852e-05,
      "loss": 1.3682,
      "num_input_tokens_seen": 631936,
      "step": 403
    },
    {
      "epoch": 0.06904802597846522,
      "grad_norm": 1.053716778755188,
      "learning_rate": 3.719649147846832e-05,
      "loss": 1.2326,
      "num_input_tokens_seen": 633344,
      "step": 404
    },
    {
      "epoch": 0.06921893693385746,
      "grad_norm": 1.0541973114013672,
      "learning_rate": 3.712024050615843e-05,
      "loss": 1.77,
      "num_input_tokens_seen": 636032,
      "step": 405
    },
    {
      "epoch": 0.0693898478892497,
      "grad_norm": 1.958239197731018,
      "learning_rate": 3.704384185254288e-05,
      "loss": 1.493,
      "num_input_tokens_seen": 637568,
      "step": 406
    },
    {
      "epoch": 0.06956075884464194,
      "grad_norm": 0.6836740970611572,
      "learning_rate": 3.696729644851518e-05,
      "loss": 1.0199,
      "num_input_tokens_seen": 642048,
      "step": 407
    },
    {
      "epoch": 0.06973166980003419,
      "grad_norm": 1.4043649435043335,
      "learning_rate": 3.689060522675689e-05,
      "loss": 1.5238,
      "num_input_tokens_seen": 643072,
      "step": 408
    },
    {
      "epoch": 0.06990258075542642,
      "grad_norm": 0.9684212803840637,
      "learning_rate": 3.681376912172636e-05,
      "loss": 1.3129,
      "num_input_tokens_seen": 645248,
      "step": 409
    },
    {
      "epoch": 0.07007349171081867,
      "grad_norm": 1.5162464380264282,
      "learning_rate": 3.673678906964727e-05,
      "loss": 1.5787,
      "num_input_tokens_seen": 646528,
      "step": 410
    },
    {
      "epoch": 0.0702444026662109,
      "grad_norm": 1.0015779733657837,
      "learning_rate": 3.665966600849728e-05,
      "loss": 1.3406,
      "num_input_tokens_seen": 648448,
      "step": 411
    },
    {
      "epoch": 0.07041531362160315,
      "grad_norm": 1.0809545516967773,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 1.2062,
      "num_input_tokens_seen": 649856,
      "step": 412
    },
    {
      "epoch": 0.07058622457699538,
      "grad_norm": 1.3785154819488525,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 1.205,
      "num_input_tokens_seen": 651392,
      "step": 413
    },
    {
      "epoch": 0.07075713553238763,
      "grad_norm": 1.7201511859893799,
      "learning_rate": 3.642744817646736e-05,
      "loss": 1.0768,
      "num_input_tokens_seen": 652416,
      "step": 414
    },
    {
      "epoch": 0.07092804648777987,
      "grad_norm": 0.9477142095565796,
      "learning_rate": 3.634976249348867e-05,
      "loss": 1.6533,
      "num_input_tokens_seen": 653952,
      "step": 415
    },
    {
      "epoch": 0.0710989574431721,
      "grad_norm": 2.6467628479003906,
      "learning_rate": 3.627193851723577e-05,
      "loss": 1.1813,
      "num_input_tokens_seen": 654976,
      "step": 416
    },
    {
      "epoch": 0.07126986839856435,
      "grad_norm": 1.0628551244735718,
      "learning_rate": 3.619397719596924e-05,
      "loss": 1.3294,
      "num_input_tokens_seen": 656384,
      "step": 417
    },
    {
      "epoch": 0.07144077935395658,
      "grad_norm": 1.062886357307434,
      "learning_rate": 3.611587947962319e-05,
      "loss": 1.3121,
      "num_input_tokens_seen": 657792,
      "step": 418
    },
    {
      "epoch": 0.07161169030934883,
      "grad_norm": 0.9547795653343201,
      "learning_rate": 3.603764631979363e-05,
      "loss": 1.3559,
      "num_input_tokens_seen": 661760,
      "step": 419
    },
    {
      "epoch": 0.07178260126474106,
      "grad_norm": 1.3421787023544312,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.9382,
      "num_input_tokens_seen": 663424,
      "step": 420
    },
    {
      "epoch": 0.07195351222013331,
      "grad_norm": 1.7104172706604004,
      "learning_rate": 3.588077748430819e-05,
      "loss": 1.6681,
      "num_input_tokens_seen": 664960,
      "step": 421
    },
    {
      "epoch": 0.07212442317552555,
      "grad_norm": 1.2388502359390259,
      "learning_rate": 3.580214372004956e-05,
      "loss": 1.2037,
      "num_input_tokens_seen": 666112,
      "step": 422
    },
    {
      "epoch": 0.0722953341309178,
      "grad_norm": 1.0019071102142334,
      "learning_rate": 3.572337833507865e-05,
      "loss": 1.5914,
      "num_input_tokens_seen": 667776,
      "step": 423
    },
    {
      "epoch": 0.07246624508631003,
      "grad_norm": 0.9631862640380859,
      "learning_rate": 3.564448228912682e-05,
      "loss": 1.3434,
      "num_input_tokens_seen": 669184,
      "step": 424
    },
    {
      "epoch": 0.07263715604170227,
      "grad_norm": 1.0496569871902466,
      "learning_rate": 3.556545654351749e-05,
      "loss": 1.3728,
      "num_input_tokens_seen": 670720,
      "step": 425
    },
    {
      "epoch": 0.07280806699709451,
      "grad_norm": 1.3434122800827026,
      "learning_rate": 3.548630206115443e-05,
      "loss": 1.6308,
      "num_input_tokens_seen": 673664,
      "step": 426
    },
    {
      "epoch": 0.07297897795248676,
      "grad_norm": 1.5649524927139282,
      "learning_rate": 3.540701980651003e-05,
      "loss": 1.3681,
      "num_input_tokens_seen": 674816,
      "step": 427
    },
    {
      "epoch": 0.07314988890787899,
      "grad_norm": 0.9490523934364319,
      "learning_rate": 3.532761074561355e-05,
      "loss": 1.2939,
      "num_input_tokens_seen": 677632,
      "step": 428
    },
    {
      "epoch": 0.07332079986327124,
      "grad_norm": 1.032360553741455,
      "learning_rate": 3.524807584603932e-05,
      "loss": 1.3117,
      "num_input_tokens_seen": 679680,
      "step": 429
    },
    {
      "epoch": 0.07349171081866347,
      "grad_norm": 0.9927045106887817,
      "learning_rate": 3.516841607689501e-05,
      "loss": 1.2189,
      "num_input_tokens_seen": 680960,
      "step": 430
    },
    {
      "epoch": 0.07366262177405572,
      "grad_norm": 1.1636632680892944,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 1.1362,
      "num_input_tokens_seen": 682112,
      "step": 431
    },
    {
      "epoch": 0.07383353272944795,
      "grad_norm": 1.221009612083435,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.9386,
      "num_input_tokens_seen": 683648,
      "step": 432
    },
    {
      "epoch": 0.0740044436848402,
      "grad_norm": 1.0583759546279907,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 1.2136,
      "num_input_tokens_seen": 685568,
      "step": 433
    },
    {
      "epoch": 0.07417535464023244,
      "grad_norm": 1.2623108625411987,
      "learning_rate": 3.484854773977378e-05,
      "loss": 1.2414,
      "num_input_tokens_seen": 686720,
      "step": 434
    },
    {
      "epoch": 0.07434626559562468,
      "grad_norm": 1.3899545669555664,
      "learning_rate": 3.476827821223184e-05,
      "loss": 1.2496,
      "num_input_tokens_seen": 688768,
      "step": 435
    },
    {
      "epoch": 0.07451717655101692,
      "grad_norm": 0.9954586625099182,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 1.2505,
      "num_input_tokens_seen": 690560,
      "step": 436
    },
    {
      "epoch": 0.07468808750640916,
      "grad_norm": 0.9634677171707153,
      "learning_rate": 3.460738306649509e-05,
      "loss": 1.4957,
      "num_input_tokens_seen": 693888,
      "step": 437
    },
    {
      "epoch": 0.0748589984618014,
      "grad_norm": 1.1984257698059082,
      "learning_rate": 3.452675940875686e-05,
      "loss": 1.2594,
      "num_input_tokens_seen": 695296,
      "step": 438
    },
    {
      "epoch": 0.07502990941719365,
      "grad_norm": 1.10506010055542,
      "learning_rate": 3.444601967046168e-05,
      "loss": 1.3103,
      "num_input_tokens_seen": 697216,
      "step": 439
    },
    {
      "epoch": 0.07520082037258588,
      "grad_norm": 1.1936044692993164,
      "learning_rate": 3.436516483539781e-05,
      "loss": 1.4821,
      "num_input_tokens_seen": 699008,
      "step": 440
    },
    {
      "epoch": 0.07537173132797813,
      "grad_norm": 0.8782915472984314,
      "learning_rate": 3.428419588875588e-05,
      "loss": 1.1069,
      "num_input_tokens_seen": 701184,
      "step": 441
    },
    {
      "epoch": 0.07554264228337036,
      "grad_norm": 1.2761787176132202,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 1.1704,
      "num_input_tokens_seen": 702208,
      "step": 442
    },
    {
      "epoch": 0.07571355323876261,
      "grad_norm": 1.1049991846084595,
      "learning_rate": 3.412191960844049e-05,
      "loss": 1.2815,
      "num_input_tokens_seen": 703872,
      "step": 443
    },
    {
      "epoch": 0.07588446419415484,
      "grad_norm": 0.9550267457962036,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 1.1749,
      "num_input_tokens_seen": 705024,
      "step": 444
    },
    {
      "epoch": 0.07605537514954709,
      "grad_norm": 1.4237216711044312,
      "learning_rate": 3.39591987386325e-05,
      "loss": 1.1893,
      "num_input_tokens_seen": 706176,
      "step": 445
    },
    {
      "epoch": 0.07622628610493933,
      "grad_norm": 2.6857385635375977,
      "learning_rate": 3.387767406020343e-05,
      "loss": 1.1697,
      "num_input_tokens_seen": 707072,
      "step": 446
    },
    {
      "epoch": 0.07639719706033157,
      "grad_norm": 1.4577311277389526,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 1.4139,
      "num_input_tokens_seen": 708480,
      "step": 447
    },
    {
      "epoch": 0.07656810801572381,
      "grad_norm": 1.5870035886764526,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 1.2347,
      "num_input_tokens_seen": 709632,
      "step": 448
    },
    {
      "epoch": 0.07673901897111605,
      "grad_norm": 1.0892013311386108,
      "learning_rate": 3.363245497496337e-05,
      "loss": 1.2006,
      "num_input_tokens_seen": 711680,
      "step": 449
    },
    {
      "epoch": 0.07690992992650829,
      "grad_norm": 1.373367428779602,
      "learning_rate": 3.355050358314172e-05,
      "loss": 1.3548,
      "num_input_tokens_seen": 712832,
      "step": 450
    },
    {
      "epoch": 0.07708084088190054,
      "grad_norm": 1.1960710287094116,
      "learning_rate": 3.346844800613229e-05,
      "loss": 1.3633,
      "num_input_tokens_seen": 714240,
      "step": 451
    },
    {
      "epoch": 0.07725175183729277,
      "grad_norm": 1.0989104509353638,
      "learning_rate": 3.338628924375638e-05,
      "loss": 1.2955,
      "num_input_tokens_seen": 716032,
      "step": 452
    },
    {
      "epoch": 0.077422662792685,
      "grad_norm": 0.9716503024101257,
      "learning_rate": 3.330402829709258e-05,
      "loss": 1.2099,
      "num_input_tokens_seen": 717568,
      "step": 453
    },
    {
      "epoch": 0.07759357374807725,
      "grad_norm": 1.2409899234771729,
      "learning_rate": 3.322166616846458e-05,
      "loss": 1.5245,
      "num_input_tokens_seen": 718720,
      "step": 454
    },
    {
      "epoch": 0.07776448470346949,
      "grad_norm": 1.230776071548462,
      "learning_rate": 3.313920386142892e-05,
      "loss": 1.2827,
      "num_input_tokens_seen": 719872,
      "step": 455
    },
    {
      "epoch": 0.07793539565886173,
      "grad_norm": 1.2728235721588135,
      "learning_rate": 3.305664238076278e-05,
      "loss": 1.7252,
      "num_input_tokens_seen": 721664,
      "step": 456
    },
    {
      "epoch": 0.07810630661425397,
      "grad_norm": 0.7962581515312195,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 1.5285,
      "num_input_tokens_seen": 726144,
      "step": 457
    },
    {
      "epoch": 0.07827721756964622,
      "grad_norm": 1.2330067157745361,
      "learning_rate": 3.289122592367757e-05,
      "loss": 1.3384,
      "num_input_tokens_seen": 727424,
      "step": 458
    },
    {
      "epoch": 0.07844812852503845,
      "grad_norm": 1.383048176765442,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 1.5071,
      "num_input_tokens_seen": 728960,
      "step": 459
    },
    {
      "epoch": 0.0786190394804307,
      "grad_norm": 1.2314529418945312,
      "learning_rate": 3.272542485937369e-05,
      "loss": 1.2109,
      "num_input_tokens_seen": 730624,
      "step": 460
    },
    {
      "epoch": 0.07878995043582293,
      "grad_norm": 1.3087483644485474,
      "learning_rate": 3.264238262407764e-05,
      "loss": 1.5282,
      "num_input_tokens_seen": 732672,
      "step": 461
    },
    {
      "epoch": 0.07896086139121518,
      "grad_norm": 1.6960384845733643,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 1.2244,
      "num_input_tokens_seen": 733952,
      "step": 462
    },
    {
      "epoch": 0.07913177234660741,
      "grad_norm": 0.7458862066268921,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.8639,
      "num_input_tokens_seen": 736512,
      "step": 463
    },
    {
      "epoch": 0.07930268330199966,
      "grad_norm": 1.4890773296356201,
      "learning_rate": 3.239270125110117e-05,
      "loss": 1.3307,
      "num_input_tokens_seen": 737920,
      "step": 464
    },
    {
      "epoch": 0.0794735942573919,
      "grad_norm": 1.050752878189087,
      "learning_rate": 3.230929261806842e-05,
      "loss": 1.0351,
      "num_input_tokens_seen": 739968,
      "step": 465
    },
    {
      "epoch": 0.07964450521278414,
      "grad_norm": 1.1751991510391235,
      "learning_rate": 3.222579492361179e-05,
      "loss": 1.3687,
      "num_input_tokens_seen": 741760,
      "step": 466
    },
    {
      "epoch": 0.07981541616817638,
      "grad_norm": 1.2735940217971802,
      "learning_rate": 3.214220918512434e-05,
      "loss": 1.0986,
      "num_input_tokens_seen": 743296,
      "step": 467
    },
    {
      "epoch": 0.07998632712356862,
      "grad_norm": 1.197540044784546,
      "learning_rate": 3.205853642107192e-05,
      "loss": 1.4092,
      "num_input_tokens_seen": 744704,
      "step": 468
    },
    {
      "epoch": 0.08015723807896086,
      "grad_norm": 2.073537826538086,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 1.404,
      "num_input_tokens_seen": 745856,
      "step": 469
    },
    {
      "epoch": 0.0803281490343531,
      "grad_norm": 1.2037456035614014,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 1.3971,
      "num_input_tokens_seen": 747136,
      "step": 470
    },
    {
      "epoch": 0.08049905998974534,
      "grad_norm": 1.4715259075164795,
      "learning_rate": 3.180700617601436e-05,
      "loss": 1.3118,
      "num_input_tokens_seen": 748160,
      "step": 471
    },
    {
      "epoch": 0.08066997094513759,
      "grad_norm": 1.070153832435608,
      "learning_rate": 3.172299551538164e-05,
      "loss": 1.5541,
      "num_input_tokens_seen": 749824,
      "step": 472
    },
    {
      "epoch": 0.08084088190052982,
      "grad_norm": 1.2933950424194336,
      "learning_rate": 3.163890293717022e-05,
      "loss": 1.1791,
      "num_input_tokens_seen": 751616,
      "step": 473
    },
    {
      "epoch": 0.08101179285592207,
      "grad_norm": 1.1758702993392944,
      "learning_rate": 3.155472946602162e-05,
      "loss": 1.1489,
      "num_input_tokens_seen": 752896,
      "step": 474
    },
    {
      "epoch": 0.0811827038113143,
      "grad_norm": 1.2333168983459473,
      "learning_rate": 3.147047612756302e-05,
      "loss": 1.3081,
      "num_input_tokens_seen": 754176,
      "step": 475
    },
    {
      "epoch": 0.08135361476670655,
      "grad_norm": 1.0120267868041992,
      "learning_rate": 3.138614394839476e-05,
      "loss": 1.2025,
      "num_input_tokens_seen": 755840,
      "step": 476
    },
    {
      "epoch": 0.08152452572209878,
      "grad_norm": 0.8742321133613586,
      "learning_rate": 3.130173395607785e-05,
      "loss": 1.1047,
      "num_input_tokens_seen": 757632,
      "step": 477
    },
    {
      "epoch": 0.08169543667749103,
      "grad_norm": 1.049690842628479,
      "learning_rate": 3.121724717912138e-05,
      "loss": 1.4346,
      "num_input_tokens_seen": 759168,
      "step": 478
    },
    {
      "epoch": 0.08186634763288327,
      "grad_norm": 1.531343936920166,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 1.2988,
      "num_input_tokens_seen": 760320,
      "step": 479
    },
    {
      "epoch": 0.08203725858827551,
      "grad_norm": 2.6969265937805176,
      "learning_rate": 3.104804738999169e-05,
      "loss": 1.187,
      "num_input_tokens_seen": 761088,
      "step": 480
    },
    {
      "epoch": 0.08220816954366775,
      "grad_norm": 2.290670394897461,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 1.3259,
      "num_input_tokens_seen": 762112,
      "step": 481
    },
    {
      "epoch": 0.08237908049906,
      "grad_norm": 1.2477012872695923,
      "learning_rate": 3.087855282756475e-05,
      "loss": 1.5579,
      "num_input_tokens_seen": 763392,
      "step": 482
    },
    {
      "epoch": 0.08254999145445223,
      "grad_norm": 1.0463225841522217,
      "learning_rate": 3.079369758735393e-05,
      "loss": 1.3377,
      "num_input_tokens_seen": 765056,
      "step": 483
    },
    {
      "epoch": 0.08272090240984448,
      "grad_norm": 1.082484483718872,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 1.569,
      "num_input_tokens_seen": 767360,
      "step": 484
    },
    {
      "epoch": 0.08289181336523671,
      "grad_norm": 1.2357441186904907,
      "learning_rate": 3.062377635859663e-05,
      "loss": 1.1861,
      "num_input_tokens_seen": 768384,
      "step": 485
    },
    {
      "epoch": 0.08306272432062896,
      "grad_norm": 1.2375049591064453,
      "learning_rate": 3.053871244048669e-05,
      "loss": 1.2199,
      "num_input_tokens_seen": 769664,
      "step": 486
    },
    {
      "epoch": 0.08323363527602119,
      "grad_norm": 1.2355352640151978,
      "learning_rate": 3.045358103491357e-05,
      "loss": 1.1386,
      "num_input_tokens_seen": 771200,
      "step": 487
    },
    {
      "epoch": 0.08340454623141344,
      "grad_norm": 2.2888669967651367,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 1.8378,
      "num_input_tokens_seen": 772224,
      "step": 488
    },
    {
      "epoch": 0.08357545718680567,
      "grad_norm": 1.3877629041671753,
      "learning_rate": 3.028311991138472e-05,
      "loss": 1.4223,
      "num_input_tokens_seen": 773632,
      "step": 489
    },
    {
      "epoch": 0.08374636814219792,
      "grad_norm": 1.3486262559890747,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 1.5378,
      "num_input_tokens_seen": 775296,
      "step": 490
    },
    {
      "epoch": 0.08391727909759016,
      "grad_norm": 1.526994228363037,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 1.2334,
      "num_input_tokens_seen": 776576,
      "step": 491
    },
    {
      "epoch": 0.08408819005298239,
      "grad_norm": 2.192436933517456,
      "learning_rate": 3.002694802864912e-05,
      "loss": 1.3411,
      "num_input_tokens_seen": 777344,
      "step": 492
    },
    {
      "epoch": 0.08425910100837464,
      "grad_norm": 1.3264728784561157,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 1.1247,
      "num_input_tokens_seen": 778752,
      "step": 493
    },
    {
      "epoch": 0.08443001196376687,
      "grad_norm": 1.420223355293274,
      "learning_rate": 2.98558587804993e-05,
      "loss": 1.3398,
      "num_input_tokens_seen": 780032,
      "step": 494
    },
    {
      "epoch": 0.08460092291915912,
      "grad_norm": 1.3108206987380981,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 1.2432,
      "num_input_tokens_seen": 781184,
      "step": 495
    },
    {
      "epoch": 0.08477183387455135,
      "grad_norm": 1.1293269395828247,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 1.1948,
      "num_input_tokens_seen": 783488,
      "step": 496
    },
    {
      "epoch": 0.0849427448299436,
      "grad_norm": 1.7920563220977783,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.8858,
      "num_input_tokens_seen": 784768,
      "step": 497
    },
    {
      "epoch": 0.08511365578533583,
      "grad_norm": 1.098486065864563,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 1.1772,
      "num_input_tokens_seen": 786304,
      "step": 498
    },
    {
      "epoch": 0.08528456674072808,
      "grad_norm": 1.4208344221115112,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.854,
      "num_input_tokens_seen": 787456,
      "step": 499
    },
    {
      "epoch": 0.08545547769612032,
      "grad_norm": 1.425516963005066,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 1.2323,
      "num_input_tokens_seen": 788608,
      "step": 500
    },
    {
      "epoch": 0.08545547769612032,
      "eval_loss": 1.271417498588562,
      "eval_runtime": 82.4778,
      "eval_samples_per_second": 63.059,
      "eval_steps_per_second": 7.893,
      "num_input_tokens_seen": 788608,
      "step": 500
    },
    {
      "epoch": 0.08562638865151256,
      "grad_norm": 0.9404515027999878,
      "learning_rate": 2.9255237479150816e-05,
      "loss": 0.8083,
      "num_input_tokens_seen": 790784,
      "step": 501
    },
    {
      "epoch": 0.0857972996069048,
      "grad_norm": 1.5904631614685059,
      "learning_rate": 2.916921866790256e-05,
      "loss": 1.1303,
      "num_input_tokens_seen": 791936,
      "step": 502
    },
    {
      "epoch": 0.08596821056229705,
      "grad_norm": 1.6247183084487915,
      "learning_rate": 2.908314905604056e-05,
      "loss": 1.5961,
      "num_input_tokens_seen": 793472,
      "step": 503
    },
    {
      "epoch": 0.08613912151768928,
      "grad_norm": 1.659714698791504,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 1.2806,
      "num_input_tokens_seen": 794496,
      "step": 504
    },
    {
      "epoch": 0.08631003247308153,
      "grad_norm": 1.1118439435958862,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 1.2402,
      "num_input_tokens_seen": 796928,
      "step": 505
    },
    {
      "epoch": 0.08648094342847376,
      "grad_norm": 1.306483507156372,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 1.3111,
      "num_input_tokens_seen": 798720,
      "step": 506
    },
    {
      "epoch": 0.08665185438386601,
      "grad_norm": 1.289237380027771,
      "learning_rate": 2.8738383586092745e-05,
      "loss": 1.36,
      "num_input_tokens_seen": 801024,
      "step": 507
    },
    {
      "epoch": 0.08682276533925824,
      "grad_norm": 1.0791176557540894,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 1.4207,
      "num_input_tokens_seen": 802432,
      "step": 508
    },
    {
      "epoch": 0.08699367629465049,
      "grad_norm": 1.754210352897644,
      "learning_rate": 2.8565723342637796e-05,
      "loss": 1.2998,
      "num_input_tokens_seen": 803584,
      "step": 509
    },
    {
      "epoch": 0.08716458725004272,
      "grad_norm": 1.2468445301055908,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 1.2954,
      "num_input_tokens_seen": 804864,
      "step": 510
    },
    {
      "epoch": 0.08733549820543497,
      "grad_norm": 1.5554629564285278,
      "learning_rate": 2.8392889310857612e-05,
      "loss": 1.0742,
      "num_input_tokens_seen": 806272,
      "step": 511
    },
    {
      "epoch": 0.0875064091608272,
      "grad_norm": 0.8314335346221924,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 0.6851,
      "num_input_tokens_seen": 808832,
      "step": 512
    },
    {
      "epoch": 0.08767732011621945,
      "grad_norm": 1.6626629829406738,
      "learning_rate": 2.8219889914439074e-05,
      "loss": 1.7645,
      "num_input_tokens_seen": 809984,
      "step": 513
    },
    {
      "epoch": 0.08784823107161169,
      "grad_norm": 1.1619975566864014,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 1.4241,
      "num_input_tokens_seen": 811904,
      "step": 514
    },
    {
      "epoch": 0.08801914202700394,
      "grad_norm": 1.1241999864578247,
      "learning_rate": 2.8046733585128687e-05,
      "loss": 0.9868,
      "num_input_tokens_seen": 813056,
      "step": 515
    },
    {
      "epoch": 0.08819005298239617,
      "grad_norm": 0.9996626377105713,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 1.5177,
      "num_input_tokens_seen": 815616,
      "step": 516
    },
    {
      "epoch": 0.08836096393778842,
      "grad_norm": 0.6439025402069092,
      "learning_rate": 2.787342876232167e-05,
      "loss": 0.995,
      "num_input_tokens_seen": 818816,
      "step": 517
    },
    {
      "epoch": 0.08853187489318065,
      "grad_norm": 1.316272497177124,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 1.4382,
      "num_input_tokens_seen": 820096,
      "step": 518
    },
    {
      "epoch": 0.0887027858485729,
      "grad_norm": 1.2006323337554932,
      "learning_rate": 2.7699983892650573e-05,
      "loss": 1.5983,
      "num_input_tokens_seen": 821632,
      "step": 519
    },
    {
      "epoch": 0.08887369680396513,
      "grad_norm": 0.97871994972229,
      "learning_rate": 2.761321158169134e-05,
      "loss": 1.4711,
      "num_input_tokens_seen": 823168,
      "step": 520
    },
    {
      "epoch": 0.08904460775935738,
      "grad_norm": 1.158705711364746,
      "learning_rate": 2.7526407429573657e-05,
      "loss": 1.3702,
      "num_input_tokens_seen": 824704,
      "step": 521
    },
    {
      "epoch": 0.08921551871474961,
      "grad_norm": 0.633298933506012,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 0.7568,
      "num_input_tokens_seen": 828544,
      "step": 522
    },
    {
      "epoch": 0.08938642967014186,
      "grad_norm": 1.4577808380126953,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 1.14,
      "num_input_tokens_seen": 829440,
      "step": 523
    },
    {
      "epoch": 0.0895573406255341,
      "grad_norm": 1.330450177192688,
      "learning_rate": 2.726581450494451e-05,
      "loss": 1.1053,
      "num_input_tokens_seen": 830848,
      "step": 524
    },
    {
      "epoch": 0.08972825158092634,
      "grad_norm": 1.2967547178268433,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.9904,
      "num_input_tokens_seen": 832128,
      "step": 525
    },
    {
      "epoch": 0.08989916253631858,
      "grad_norm": 1.736135482788086,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 1.5678,
      "num_input_tokens_seen": 833664,
      "step": 526
    },
    {
      "epoch": 0.09007007349171083,
      "grad_norm": 1.7141205072402954,
      "learning_rate": 2.7004973108221472e-05,
      "loss": 1.0765,
      "num_input_tokens_seen": 834816,
      "step": 527
    },
    {
      "epoch": 0.09024098444710306,
      "grad_norm": 1.000194787979126,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 1.5502,
      "num_input_tokens_seen": 837504,
      "step": 528
    },
    {
      "epoch": 0.0904118954024953,
      "grad_norm": 1.3918554782867432,
      "learning_rate": 2.6830954928190794e-05,
      "loss": 1.512,
      "num_input_tokens_seen": 838656,
      "step": 529
    },
    {
      "epoch": 0.09058280635788754,
      "grad_norm": 1.2956578731536865,
      "learning_rate": 2.674391184360313e-05,
      "loss": 1.2173,
      "num_input_tokens_seen": 840192,
      "step": 530
    },
    {
      "epoch": 0.09075371731327977,
      "grad_norm": 0.9638955593109131,
      "learning_rate": 2.6656847510000012e-05,
      "loss": 1.6061,
      "num_input_tokens_seen": 841984,
      "step": 531
    },
    {
      "epoch": 0.09092462826867202,
      "grad_norm": 0.9494991302490234,
      "learning_rate": 2.656976298823284e-05,
      "loss": 1.0625,
      "num_input_tokens_seen": 844288,
      "step": 532
    },
    {
      "epoch": 0.09109553922406426,
      "grad_norm": 1.7681097984313965,
      "learning_rate": 2.6482659339399045e-05,
      "loss": 1.188,
      "num_input_tokens_seen": 845696,
      "step": 533
    },
    {
      "epoch": 0.0912664501794565,
      "grad_norm": 1.1317051649093628,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 1.3168,
      "num_input_tokens_seen": 846848,
      "step": 534
    },
    {
      "epoch": 0.09143736113484874,
      "grad_norm": 1.0343838930130005,
      "learning_rate": 2.63083989060736e-05,
      "loss": 1.2876,
      "num_input_tokens_seen": 848768,
      "step": 535
    },
    {
      "epoch": 0.09160827209024099,
      "grad_norm": 1.255652904510498,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 1.0092,
      "num_input_tokens_seen": 849792,
      "step": 536
    },
    {
      "epoch": 0.09177918304563322,
      "grad_norm": 1.1434426307678223,
      "learning_rate": 2.6134074703231344e-05,
      "loss": 1.3486,
      "num_input_tokens_seen": 851456,
      "step": 537
    },
    {
      "epoch": 0.09195009400102547,
      "grad_norm": 0.9826088547706604,
      "learning_rate": 2.604689134322999e-05,
      "loss": 1.1015,
      "num_input_tokens_seen": 853632,
      "step": 538
    },
    {
      "epoch": 0.0921210049564177,
      "grad_norm": 1.48378586769104,
      "learning_rate": 2.5959695227188004e-05,
      "loss": 1.5251,
      "num_input_tokens_seen": 854912,
      "step": 539
    },
    {
      "epoch": 0.09229191591180995,
      "grad_norm": 1.3324694633483887,
      "learning_rate": 2.587248741756253e-05,
      "loss": 1.4237,
      "num_input_tokens_seen": 856832,
      "step": 540
    },
    {
      "epoch": 0.09246282686720218,
      "grad_norm": 0.9739763736724854,
      "learning_rate": 2.578526897695321e-05,
      "loss": 1.4053,
      "num_input_tokens_seen": 858368,
      "step": 541
    },
    {
      "epoch": 0.09263373782259443,
      "grad_norm": 1.4239379167556763,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 1.3737,
      "num_input_tokens_seen": 859520,
      "step": 542
    },
    {
      "epoch": 0.09280464877798666,
      "grad_norm": 2.0863466262817383,
      "learning_rate": 2.5610804453816333e-05,
      "loss": 1.0997,
      "num_input_tokens_seen": 860544,
      "step": 543
    },
    {
      "epoch": 0.09297555973337891,
      "grad_norm": 1.4572337865829468,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 1.4431,
      "num_input_tokens_seen": 861568,
      "step": 544
    },
    {
      "epoch": 0.09314647068877115,
      "grad_norm": 1.3019673824310303,
      "learning_rate": 2.5436310160932092e-05,
      "loss": 1.3968,
      "num_input_tokens_seen": 862976,
      "step": 545
    },
    {
      "epoch": 0.0933173816441634,
      "grad_norm": 1.2845065593719482,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 1.4606,
      "num_input_tokens_seen": 864384,
      "step": 546
    },
    {
      "epoch": 0.09348829259955563,
      "grad_norm": 1.5351407527923584,
      "learning_rate": 2.5261794602906145e-05,
      "loss": 1.742,
      "num_input_tokens_seen": 866048,
      "step": 547
    },
    {
      "epoch": 0.09365920355494788,
      "grad_norm": 1.0955523252487183,
      "learning_rate": 2.517453150744904e-05,
      "loss": 1.0872,
      "num_input_tokens_seen": 867328,
      "step": 548
    },
    {
      "epoch": 0.09383011451034011,
      "grad_norm": 0.5295030474662781,
      "learning_rate": 2.5087266285380596e-05,
      "loss": 0.7249,
      "num_input_tokens_seen": 872320,
      "step": 549
    },
    {
      "epoch": 0.09400102546573236,
      "grad_norm": 1.1359666585922241,
      "learning_rate": 2.5e-05,
      "loss": 1.3751,
      "num_input_tokens_seen": 874240,
      "step": 550
    },
    {
      "epoch": 0.09417193642112459,
      "grad_norm": 1.0586838722229004,
      "learning_rate": 2.4912733714619417e-05,
      "loss": 1.1783,
      "num_input_tokens_seen": 875776,
      "step": 551
    },
    {
      "epoch": 0.09434284737651684,
      "grad_norm": 1.2097939252853394,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 1.2524,
      "num_input_tokens_seen": 877952,
      "step": 552
    },
    {
      "epoch": 0.09451375833190907,
      "grad_norm": 1.0527580976486206,
      "learning_rate": 2.4738205397093864e-05,
      "loss": 1.1375,
      "num_input_tokens_seen": 879360,
      "step": 553
    },
    {
      "epoch": 0.09468466928730132,
      "grad_norm": 0.8612381815910339,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 1.2675,
      "num_input_tokens_seen": 882304,
      "step": 554
    },
    {
      "epoch": 0.09485558024269355,
      "grad_norm": 1.4704831838607788,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.4499,
      "num_input_tokens_seen": 884096,
      "step": 555
    },
    {
      "epoch": 0.0950264911980858,
      "grad_norm": 1.0511754751205444,
      "learning_rate": 2.447643950291608e-05,
      "loss": 1.3084,
      "num_input_tokens_seen": 886272,
      "step": 556
    },
    {
      "epoch": 0.09519740215347804,
      "grad_norm": 1.337883472442627,
      "learning_rate": 2.4389195546183673e-05,
      "loss": 1.4103,
      "num_input_tokens_seen": 887936,
      "step": 557
    },
    {
      "epoch": 0.09536831310887028,
      "grad_norm": 1.1758724451065063,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 1.295,
      "num_input_tokens_seen": 889088,
      "step": 558
    },
    {
      "epoch": 0.09553922406426252,
      "grad_norm": 0.84852135181427,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 1.7797,
      "num_input_tokens_seen": 891520,
      "step": 559
    },
    {
      "epoch": 0.09571013501965477,
      "grad_norm": 1.0396138429641724,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 1.207,
      "num_input_tokens_seen": 893184,
      "step": 560
    },
    {
      "epoch": 0.095881045975047,
      "grad_norm": 1.7958420515060425,
      "learning_rate": 2.4040304772812002e-05,
      "loss": 1.4934,
      "num_input_tokens_seen": 894336,
      "step": 561
    },
    {
      "epoch": 0.09605195693043925,
      "grad_norm": 0.9494518041610718,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 1.3875,
      "num_input_tokens_seen": 896768,
      "step": 562
    },
    {
      "epoch": 0.09622286788583148,
      "grad_norm": 1.5071948766708374,
      "learning_rate": 2.386592529676866e-05,
      "loss": 1.3503,
      "num_input_tokens_seen": 897792,
      "step": 563
    },
    {
      "epoch": 0.09639377884122373,
      "grad_norm": 1.499294638633728,
      "learning_rate": 2.377875575510967e-05,
      "loss": 1.2856,
      "num_input_tokens_seen": 899072,
      "step": 564
    },
    {
      "epoch": 0.09656468979661596,
      "grad_norm": 1.5880539417266846,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 1.2918,
      "num_input_tokens_seen": 900224,
      "step": 565
    },
    {
      "epoch": 0.09673560075200821,
      "grad_norm": 1.485095739364624,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 1.258,
      "num_input_tokens_seen": 901888,
      "step": 566
    },
    {
      "epoch": 0.09690651170740044,
      "grad_norm": 1.2131863832473755,
      "learning_rate": 2.3517340660600964e-05,
      "loss": 1.5616,
      "num_input_tokens_seen": 903552,
      "step": 567
    },
    {
      "epoch": 0.09707742266279268,
      "grad_norm": 0.9727247953414917,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 1.3703,
      "num_input_tokens_seen": 904832,
      "step": 568
    },
    {
      "epoch": 0.09724833361818493,
      "grad_norm": 1.0439934730529785,
      "learning_rate": 2.3343152490000004e-05,
      "loss": 1.4066,
      "num_input_tokens_seen": 907776,
      "step": 569
    },
    {
      "epoch": 0.09741924457357716,
      "grad_norm": 1.1608107089996338,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 1.1778,
      "num_input_tokens_seen": 909312,
      "step": 570
    },
    {
      "epoch": 0.09759015552896941,
      "grad_norm": 1.2145456075668335,
      "learning_rate": 2.3169045071809215e-05,
      "loss": 1.2662,
      "num_input_tokens_seen": 910976,
      "step": 571
    },
    {
      "epoch": 0.09776106648436164,
      "grad_norm": 1.0165804624557495,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 0.8388,
      "num_input_tokens_seen": 912640,
      "step": 572
    },
    {
      "epoch": 0.09793197743975389,
      "grad_norm": 1.1894068717956543,
      "learning_rate": 2.299502689177853e-05,
      "loss": 1.4741,
      "num_input_tokens_seen": 914304,
      "step": 573
    },
    {
      "epoch": 0.09810288839514612,
      "grad_norm": 1.2700762748718262,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 1.1211,
      "num_input_tokens_seen": 915712,
      "step": 574
    },
    {
      "epoch": 0.09827379935053837,
      "grad_norm": 1.241316795349121,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 1.157,
      "num_input_tokens_seen": 917120,
      "step": 575
    },
    {
      "epoch": 0.0984447103059306,
      "grad_norm": 1.5133992433547974,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 1.084,
      "num_input_tokens_seen": 918528,
      "step": 576
    },
    {
      "epoch": 0.09861562126132285,
      "grad_norm": 0.8119156360626221,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 0.7428,
      "num_input_tokens_seen": 921984,
      "step": 577
    },
    {
      "epoch": 0.09878653221671509,
      "grad_norm": 2.0234484672546387,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 1.1981,
      "num_input_tokens_seen": 922880,
      "step": 578
    },
    {
      "epoch": 0.09895744317210733,
      "grad_norm": 1.1853466033935547,
      "learning_rate": 2.247359257042634e-05,
      "loss": 1.2022,
      "num_input_tokens_seen": 924672,
      "step": 579
    },
    {
      "epoch": 0.09912835412749957,
      "grad_norm": 1.106006383895874,
      "learning_rate": 2.238678841830867e-05,
      "loss": 1.3662,
      "num_input_tokens_seen": 926208,
      "step": 580
    },
    {
      "epoch": 0.09929926508289182,
      "grad_norm": 1.2349179983139038,
      "learning_rate": 2.230001610734943e-05,
      "loss": 1.3511,
      "num_input_tokens_seen": 927616,
      "step": 581
    },
    {
      "epoch": 0.09947017603828405,
      "grad_norm": 1.4730595350265503,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 1.2803,
      "num_input_tokens_seen": 929024,
      "step": 582
    },
    {
      "epoch": 0.0996410869936763,
      "grad_norm": 1.6151645183563232,
      "learning_rate": 2.212657123767834e-05,
      "loss": 1.2477,
      "num_input_tokens_seen": 930048,
      "step": 583
    },
    {
      "epoch": 0.09981199794906853,
      "grad_norm": 1.2858794927597046,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 1.5176,
      "num_input_tokens_seen": 931712,
      "step": 584
    },
    {
      "epoch": 0.09998290890446078,
      "grad_norm": 1.5801501274108887,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.9243,
      "num_input_tokens_seen": 932864,
      "step": 585
    },
    {
      "epoch": 0.10015381985985301,
      "grad_norm": 1.169513463973999,
      "learning_rate": 2.186666916089239e-05,
      "loss": 1.7868,
      "num_input_tokens_seen": 934656,
      "step": 586
    },
    {
      "epoch": 0.10032473081524526,
      "grad_norm": 1.175910472869873,
      "learning_rate": 2.1780110085560935e-05,
      "loss": 1.3472,
      "num_input_tokens_seen": 936064,
      "step": 587
    },
    {
      "epoch": 0.1004956417706375,
      "grad_norm": 1.5911178588867188,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 1.3284,
      "num_input_tokens_seen": 937344,
      "step": 588
    },
    {
      "epoch": 0.10066655272602974,
      "grad_norm": 1.30918550491333,
      "learning_rate": 2.1607110689142393e-05,
      "loss": 1.3765,
      "num_input_tokens_seen": 938624,
      "step": 589
    },
    {
      "epoch": 0.10083746368142198,
      "grad_norm": 0.9784234166145325,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 1.1728,
      "num_input_tokens_seen": 940928,
      "step": 590
    },
    {
      "epoch": 0.10100837463681422,
      "grad_norm": 1.5091997385025024,
      "learning_rate": 2.1434276657362213e-05,
      "loss": 1.2474,
      "num_input_tokens_seen": 942336,
      "step": 591
    },
    {
      "epoch": 0.10117928559220646,
      "grad_norm": 1.2310168743133545,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 1.5751,
      "num_input_tokens_seen": 943744,
      "step": 592
    },
    {
      "epoch": 0.1013501965475987,
      "grad_norm": 1.3678622245788574,
      "learning_rate": 2.1261616413907265e-05,
      "loss": 1.3698,
      "num_input_tokens_seen": 945536,
      "step": 593
    },
    {
      "epoch": 0.10152110750299094,
      "grad_norm": 1.2693439722061157,
      "learning_rate": 2.117535409289905e-05,
      "loss": 1.2007,
      "num_input_tokens_seen": 946944,
      "step": 594
    },
    {
      "epoch": 0.10169201845838319,
      "grad_norm": 1.1031460762023926,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 1.3495,
      "num_input_tokens_seen": 948352,
      "step": 595
    },
    {
      "epoch": 0.10186292941377542,
      "grad_norm": 1.2735488414764404,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 1.3207,
      "num_input_tokens_seen": 950016,
      "step": 596
    },
    {
      "epoch": 0.10203384036916767,
      "grad_norm": 0.966762125492096,
      "learning_rate": 2.0916850943959452e-05,
      "loss": 1.2455,
      "num_input_tokens_seen": 951552,
      "step": 597
    },
    {
      "epoch": 0.1022047513245599,
      "grad_norm": 1.6553937196731567,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 1.2417,
      "num_input_tokens_seen": 952704,
      "step": 598
    },
    {
      "epoch": 0.10237566227995215,
      "grad_norm": 1.0698487758636475,
      "learning_rate": 2.0744762520849193e-05,
      "loss": 1.1979,
      "num_input_tokens_seen": 954112,
      "step": 599
    },
    {
      "epoch": 0.10254657323534438,
      "grad_norm": 0.9260419607162476,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 1.5376,
      "num_input_tokens_seen": 961536,
      "step": 600
    },
    {
      "epoch": 0.10271748419073663,
      "grad_norm": 0.9840108752250671,
      "learning_rate": 2.057288149201042e-05,
      "loss": 1.4309,
      "num_input_tokens_seen": 963072,
      "step": 601
    },
    {
      "epoch": 0.10288839514612887,
      "grad_norm": 1.1608808040618896,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 1.096,
      "num_input_tokens_seen": 964352,
      "step": 602
    },
    {
      "epoch": 0.10305930610152111,
      "grad_norm": 1.0283554792404175,
      "learning_rate": 2.0401216234681995e-05,
      "loss": 1.354,
      "num_input_tokens_seen": 965760,
      "step": 603
    },
    {
      "epoch": 0.10323021705691335,
      "grad_norm": 0.8468912243843079,
      "learning_rate": 2.031546713535688e-05,
      "loss": 1.4295,
      "num_input_tokens_seen": 968192,
      "step": 604
    },
    {
      "epoch": 0.10340112801230558,
      "grad_norm": 1.4026457071304321,
      "learning_rate": 2.022977511558638e-05,
      "loss": 1.4349,
      "num_input_tokens_seen": 969472,
      "step": 605
    },
    {
      "epoch": 0.10357203896769783,
      "grad_norm": 1.619971513748169,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 1.597,
      "num_input_tokens_seen": 970624,
      "step": 606
    },
    {
      "epoch": 0.10374294992309006,
      "grad_norm": 1.103879451751709,
      "learning_rate": 2.0058566490521847e-05,
      "loss": 1.0476,
      "num_input_tokens_seen": 972544,
      "step": 607
    },
    {
      "epoch": 0.10391386087848231,
      "grad_norm": 1.3561691045761108,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 1.0611,
      "num_input_tokens_seen": 973696,
      "step": 608
    },
    {
      "epoch": 0.10408477183387455,
      "grad_norm": 1.1062031984329224,
      "learning_rate": 1.9887598703955242e-05,
      "loss": 1.1607,
      "num_input_tokens_seen": 974848,
      "step": 609
    },
    {
      "epoch": 0.1042556827892668,
      "grad_norm": 1.3474622964859009,
      "learning_rate": 1.980220772955602e-05,
      "loss": 1.2695,
      "num_input_tokens_seen": 976256,
      "step": 610
    },
    {
      "epoch": 0.10442659374465903,
      "grad_norm": 1.3730801343917847,
      "learning_rate": 1.9716880088615285e-05,
      "loss": 1.2046,
      "num_input_tokens_seen": 977536,
      "step": 611
    },
    {
      "epoch": 0.10459750470005127,
      "grad_norm": 1.9186397790908813,
      "learning_rate": 1.963161682082342e-05,
      "loss": 1.0464,
      "num_input_tokens_seen": 978432,
      "step": 612
    },
    {
      "epoch": 0.10476841565544351,
      "grad_norm": 1.2192575931549072,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 1.4827,
      "num_input_tokens_seen": 979968,
      "step": 613
    },
    {
      "epoch": 0.10493932661083576,
      "grad_norm": 1.6401647329330444,
      "learning_rate": 1.946128755951332e-05,
      "loss": 1.6449,
      "num_input_tokens_seen": 980992,
      "step": 614
    },
    {
      "epoch": 0.10511023756622799,
      "grad_norm": 1.3974385261535645,
      "learning_rate": 1.937622364140338e-05,
      "loss": 1.6537,
      "num_input_tokens_seen": 982656,
      "step": 615
    },
    {
      "epoch": 0.10528114852162024,
      "grad_norm": 1.4015955924987793,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 1.3577,
      "num_input_tokens_seen": 984576,
      "step": 616
    },
    {
      "epoch": 0.10545205947701247,
      "grad_norm": 1.1069772243499756,
      "learning_rate": 1.920630241264607e-05,
      "loss": 1.2671,
      "num_input_tokens_seen": 985984,
      "step": 617
    },
    {
      "epoch": 0.10562297043240472,
      "grad_norm": 1.4324750900268555,
      "learning_rate": 1.912144717243525e-05,
      "loss": 1.5444,
      "num_input_tokens_seen": 987136,
      "step": 618
    },
    {
      "epoch": 0.10579388138779695,
      "grad_norm": 1.053289771080017,
      "learning_rate": 1.9036663560535483e-05,
      "loss": 1.5875,
      "num_input_tokens_seen": 989696,
      "step": 619
    },
    {
      "epoch": 0.1059647923431892,
      "grad_norm": 0.9087677597999573,
      "learning_rate": 1.895195261000831e-05,
      "loss": 1.3446,
      "num_input_tokens_seen": 991104,
      "step": 620
    },
    {
      "epoch": 0.10613570329858144,
      "grad_norm": 1.4911353588104248,
      "learning_rate": 1.8867315353029935e-05,
      "loss": 1.4252,
      "num_input_tokens_seen": 992896,
      "step": 621
    },
    {
      "epoch": 0.10630661425397368,
      "grad_norm": 1.2964749336242676,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 1.4365,
      "num_input_tokens_seen": 995200,
      "step": 622
    },
    {
      "epoch": 0.10647752520936592,
      "grad_norm": 1.189635157585144,
      "learning_rate": 1.869826604392216e-05,
      "loss": 1.5111,
      "num_input_tokens_seen": 997248,
      "step": 623
    },
    {
      "epoch": 0.10664843616475816,
      "grad_norm": 0.987435519695282,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 0.9815,
      "num_input_tokens_seen": 998912,
      "step": 624
    },
    {
      "epoch": 0.1068193471201504,
      "grad_norm": 1.5457849502563477,
      "learning_rate": 1.852952387243698e-05,
      "loss": 1.4595,
      "num_input_tokens_seen": 1000448,
      "step": 625
    },
    {
      "epoch": 0.10699025807554265,
      "grad_norm": 1.3296024799346924,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 1.0982,
      "num_input_tokens_seen": 1001600,
      "step": 626
    },
    {
      "epoch": 0.10716116903093488,
      "grad_norm": 1.2555257081985474,
      "learning_rate": 1.8361097062829778e-05,
      "loss": 1.4105,
      "num_input_tokens_seen": 1003264,
      "step": 627
    },
    {
      "epoch": 0.10733207998632713,
      "grad_norm": 1.7717653512954712,
      "learning_rate": 1.827700448461836e-05,
      "loss": 1.4827,
      "num_input_tokens_seen": 1004416,
      "step": 628
    },
    {
      "epoch": 0.10750299094171936,
      "grad_norm": 1.0212668180465698,
      "learning_rate": 1.8192993823985643e-05,
      "loss": 1.4186,
      "num_input_tokens_seen": 1006080,
      "step": 629
    },
    {
      "epoch": 0.10767390189711161,
      "grad_norm": 1.437364935874939,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 1.4021,
      "num_input_tokens_seen": 1007616,
      "step": 630
    },
    {
      "epoch": 0.10784481285250384,
      "grad_norm": 1.0098750591278076,
      "learning_rate": 1.802522234901927e-05,
      "loss": 1.321,
      "num_input_tokens_seen": 1009536,
      "step": 631
    },
    {
      "epoch": 0.10801572380789609,
      "grad_norm": 1.4461745023727417,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 1.1641,
      "num_input_tokens_seen": 1010944,
      "step": 632
    },
    {
      "epoch": 0.10818663476328833,
      "grad_norm": 1.3166704177856445,
      "learning_rate": 1.7857790814875663e-05,
      "loss": 1.1481,
      "num_input_tokens_seen": 1012352,
      "step": 633
    },
    {
      "epoch": 0.10835754571868057,
      "grad_norm": 1.4594014883041382,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 1.403,
      "num_input_tokens_seen": 1013632,
      "step": 634
    },
    {
      "epoch": 0.1085284566740728,
      "grad_norm": 1.487465262413025,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 1.2748,
      "num_input_tokens_seen": 1015040,
      "step": 635
    },
    {
      "epoch": 0.10869936762946505,
      "grad_norm": 1.233091115951538,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 1.2168,
      "num_input_tokens_seen": 1016320,
      "step": 636
    },
    {
      "epoch": 0.10887027858485729,
      "grad_norm": 1.1662373542785645,
      "learning_rate": 1.7523980193597836e-05,
      "loss": 0.9696,
      "num_input_tokens_seen": 1017728,
      "step": 637
    },
    {
      "epoch": 0.10904118954024954,
      "grad_norm": 1.6108896732330322,
      "learning_rate": 1.744075273123889e-05,
      "loss": 1.2453,
      "num_input_tokens_seen": 1019136,
      "step": 638
    },
    {
      "epoch": 0.10921210049564177,
      "grad_norm": 1.3326970338821411,
      "learning_rate": 1.735761737592236e-05,
      "loss": 1.1466,
      "num_input_tokens_seen": 1020544,
      "step": 639
    },
    {
      "epoch": 0.10938301145103402,
      "grad_norm": 1.5292636156082153,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 1.2629,
      "num_input_tokens_seen": 1021696,
      "step": 640
    },
    {
      "epoch": 0.10955392240642625,
      "grad_norm": 1.2145384550094604,
      "learning_rate": 1.7191627037194186e-05,
      "loss": 1.308,
      "num_input_tokens_seen": 1023232,
      "step": 641
    },
    {
      "epoch": 0.10972483336181849,
      "grad_norm": 0.9956129193305969,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 1.2827,
      "num_input_tokens_seen": 1024896,
      "step": 642
    },
    {
      "epoch": 0.10989574431721073,
      "grad_norm": 1.0378364324569702,
      "learning_rate": 1.702601726754825e-05,
      "loss": 1.4104,
      "num_input_tokens_seen": 1026304,
      "step": 643
    },
    {
      "epoch": 0.11006665527260297,
      "grad_norm": 1.442474365234375,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 1.4426,
      "num_input_tokens_seen": 1027584,
      "step": 644
    },
    {
      "epoch": 0.11023756622799522,
      "grad_norm": 1.0752371549606323,
      "learning_rate": 1.686079613857109e-05,
      "loss": 1.4425,
      "num_input_tokens_seen": 1029504,
      "step": 645
    },
    {
      "epoch": 0.11040847718338745,
      "grad_norm": 0.924298107624054,
      "learning_rate": 1.677833383153542e-05,
      "loss": 0.9471,
      "num_input_tokens_seen": 1032576,
      "step": 646
    },
    {
      "epoch": 0.1105793881387797,
      "grad_norm": 1.1538715362548828,
      "learning_rate": 1.6695971702907426e-05,
      "loss": 1.2867,
      "num_input_tokens_seen": 1033856,
      "step": 647
    },
    {
      "epoch": 0.11075029909417193,
      "grad_norm": 1.1389930248260498,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 1.1658,
      "num_input_tokens_seen": 1035008,
      "step": 648
    },
    {
      "epoch": 0.11092121004956418,
      "grad_norm": 0.801022469997406,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 1.3208,
      "num_input_tokens_seen": 1038336,
      "step": 649
    },
    {
      "epoch": 0.11109212100495641,
      "grad_norm": 1.179013967514038,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 1.1848,
      "num_input_tokens_seen": 1040512,
      "step": 650
    },
    {
      "epoch": 0.11126303196034866,
      "grad_norm": 1.499605417251587,
      "learning_rate": 1.6367545025036636e-05,
      "loss": 1.395,
      "num_input_tokens_seen": 1042304,
      "step": 651
    },
    {
      "epoch": 0.1114339429157409,
      "grad_norm": 1.0385152101516724,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 1.2753,
      "num_input_tokens_seen": 1044608,
      "step": 652
    },
    {
      "epoch": 0.11160485387113314,
      "grad_norm": 1.637971043586731,
      "learning_rate": 1.6203958789882456e-05,
      "loss": 1.3091,
      "num_input_tokens_seen": 1045504,
      "step": 653
    },
    {
      "epoch": 0.11177576482652538,
      "grad_norm": 1.3249825239181519,
      "learning_rate": 1.612232593979658e-05,
      "loss": 1.3664,
      "num_input_tokens_seen": 1047168,
      "step": 654
    },
    {
      "epoch": 0.11194667578191762,
      "grad_norm": 1.062351107597351,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 1.3488,
      "num_input_tokens_seen": 1049216,
      "step": 655
    },
    {
      "epoch": 0.11211758673730986,
      "grad_norm": 1.2975125312805176,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 1.444,
      "num_input_tokens_seen": 1050752,
      "step": 656
    },
    {
      "epoch": 0.1122884976927021,
      "grad_norm": 2.127821922302246,
      "learning_rate": 1.5878080391559508e-05,
      "loss": 1.2876,
      "num_input_tokens_seen": 1051648,
      "step": 657
    },
    {
      "epoch": 0.11245940864809434,
      "grad_norm": 1.0149811506271362,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 0.6831,
      "num_input_tokens_seen": 1053056,
      "step": 658
    },
    {
      "epoch": 0.11263031960348659,
      "grad_norm": 0.857399582862854,
      "learning_rate": 1.5715804111244137e-05,
      "loss": 1.0217,
      "num_input_tokens_seen": 1054848,
      "step": 659
    },
    {
      "epoch": 0.11280123055887882,
      "grad_norm": 1.5368298292160034,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.9393,
      "num_input_tokens_seen": 1056384,
      "step": 660
    },
    {
      "epoch": 0.11297214151427107,
      "grad_norm": 1.3793786764144897,
      "learning_rate": 1.5553980329538326e-05,
      "loss": 1.2259,
      "num_input_tokens_seen": 1057536,
      "step": 661
    },
    {
      "epoch": 0.1131430524696633,
      "grad_norm": 1.3549153804779053,
      "learning_rate": 1.547324059124315e-05,
      "loss": 0.9667,
      "num_input_tokens_seen": 1058688,
      "step": 662
    },
    {
      "epoch": 0.11331396342505555,
      "grad_norm": 1.667150616645813,
      "learning_rate": 1.539261693350491e-05,
      "loss": 1.3126,
      "num_input_tokens_seen": 1059968,
      "step": 663
    },
    {
      "epoch": 0.11348487438044778,
      "grad_norm": 1.419189453125,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 1.4317,
      "num_input_tokens_seen": 1061376,
      "step": 664
    },
    {
      "epoch": 0.11365578533584003,
      "grad_norm": 1.2902063131332397,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.9292,
      "num_input_tokens_seen": 1062784,
      "step": 665
    },
    {
      "epoch": 0.11382669629123227,
      "grad_norm": 1.496127963066101,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 1.4487,
      "num_input_tokens_seen": 1065088,
      "step": 666
    },
    {
      "epoch": 0.11399760724662451,
      "grad_norm": 1.4712520837783813,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 1.6014,
      "num_input_tokens_seen": 1066368,
      "step": 667
    },
    {
      "epoch": 0.11416851820201675,
      "grad_norm": 1.0760498046875,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 1.3632,
      "num_input_tokens_seen": 1068800,
      "step": 668
    },
    {
      "epoch": 0.114339429157409,
      "grad_norm": 1.1831477880477905,
      "learning_rate": 1.4911367591190248e-05,
      "loss": 1.6534,
      "num_input_tokens_seen": 1070464,
      "step": 669
    },
    {
      "epoch": 0.11451034011280123,
      "grad_norm": 1.2850524187088013,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 1.5661,
      "num_input_tokens_seen": 1072640,
      "step": 670
    },
    {
      "epoch": 0.11468125106819348,
      "grad_norm": 1.0169472694396973,
      "learning_rate": 1.475192415396068e-05,
      "loss": 1.4627,
      "num_input_tokens_seen": 1075328,
      "step": 671
    },
    {
      "epoch": 0.11485216202358571,
      "grad_norm": 1.5321813821792603,
      "learning_rate": 1.467238925438646e-05,
      "loss": 1.3987,
      "num_input_tokens_seen": 1076608,
      "step": 672
    },
    {
      "epoch": 0.11502307297897796,
      "grad_norm": 1.2898873090744019,
      "learning_rate": 1.4592980193489975e-05,
      "loss": 1.3652,
      "num_input_tokens_seen": 1077888,
      "step": 673
    },
    {
      "epoch": 0.11519398393437019,
      "grad_norm": 1.3727118968963623,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 1.3841,
      "num_input_tokens_seen": 1079296,
      "step": 674
    },
    {
      "epoch": 0.11536489488976244,
      "grad_norm": 1.1327168941497803,
      "learning_rate": 1.443454345648252e-05,
      "loss": 1.2437,
      "num_input_tokens_seen": 1080704,
      "step": 675
    },
    {
      "epoch": 0.11553580584515467,
      "grad_norm": 0.8369237780570984,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 1.4446,
      "num_input_tokens_seen": 1083648,
      "step": 676
    },
    {
      "epoch": 0.11570671680054692,
      "grad_norm": 1.0933549404144287,
      "learning_rate": 1.4276621664921357e-05,
      "loss": 1.3119,
      "num_input_tokens_seen": 1085312,
      "step": 677
    },
    {
      "epoch": 0.11587762775593916,
      "grad_norm": 1.1408277750015259,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 1.4164,
      "num_input_tokens_seen": 1086848,
      "step": 678
    },
    {
      "epoch": 0.1160485387113314,
      "grad_norm": 1.2624133825302124,
      "learning_rate": 1.4119222515691816e-05,
      "loss": 1.5089,
      "num_input_tokens_seen": 1088256,
      "step": 679
    },
    {
      "epoch": 0.11621944966672364,
      "grad_norm": 1.1967471837997437,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.9873,
      "num_input_tokens_seen": 1090048,
      "step": 680
    },
    {
      "epoch": 0.11639036062211587,
      "grad_norm": 1.4514740705490112,
      "learning_rate": 1.3962353680206373e-05,
      "loss": 1.4412,
      "num_input_tokens_seen": 1091840,
      "step": 681
    },
    {
      "epoch": 0.11656127157750812,
      "grad_norm": 1.3482745885849,
      "learning_rate": 1.388412052037682e-05,
      "loss": 1.4129,
      "num_input_tokens_seen": 1093120,
      "step": 682
    },
    {
      "epoch": 0.11673218253290035,
      "grad_norm": 1.236077904701233,
      "learning_rate": 1.380602280403076e-05,
      "loss": 1.2386,
      "num_input_tokens_seen": 1094784,
      "step": 683
    },
    {
      "epoch": 0.1169030934882926,
      "grad_norm": 0.8736825585365295,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 0.8619,
      "num_input_tokens_seen": 1096448,
      "step": 684
    },
    {
      "epoch": 0.11707400444368483,
      "grad_norm": 1.5832890272140503,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 1.5295,
      "num_input_tokens_seen": 1097472,
      "step": 685
    },
    {
      "epoch": 0.11724491539907708,
      "grad_norm": 0.9848533272743225,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 1.1669,
      "num_input_tokens_seen": 1099008,
      "step": 686
    },
    {
      "epoch": 0.11741582635446932,
      "grad_norm": 1.0472146272659302,
      "learning_rate": 1.349500538040371e-05,
      "loss": 1.2389,
      "num_input_tokens_seen": 1100800,
      "step": 687
    },
    {
      "epoch": 0.11758673730986156,
      "grad_norm": 1.549160361289978,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 1.4846,
      "num_input_tokens_seen": 1102080,
      "step": 688
    },
    {
      "epoch": 0.1177576482652538,
      "grad_norm": 1.1591551303863525,
      "learning_rate": 1.3340333991502724e-05,
      "loss": 1.3362,
      "num_input_tokens_seen": 1103744,
      "step": 689
    },
    {
      "epoch": 0.11792855922064605,
      "grad_norm": 1.759311318397522,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 1.5217,
      "num_input_tokens_seen": 1104768,
      "step": 690
    },
    {
      "epoch": 0.11809947017603828,
      "grad_norm": 1.546773910522461,
      "learning_rate": 1.3186230878273653e-05,
      "loss": 1.1991,
      "num_input_tokens_seen": 1106304,
      "step": 691
    },
    {
      "epoch": 0.11827038113143053,
      "grad_norm": 1.1018067598342896,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 1.4228,
      "num_input_tokens_seen": 1107840,
      "step": 692
    },
    {
      "epoch": 0.11844129208682276,
      "grad_norm": 1.0196071863174438,
      "learning_rate": 1.3032703551484832e-05,
      "loss": 1.3724,
      "num_input_tokens_seen": 1110528,
      "step": 693
    },
    {
      "epoch": 0.11861220304221501,
      "grad_norm": 0.9928870797157288,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 1.2693,
      "num_input_tokens_seen": 1112832,
      "step": 694
    },
    {
      "epoch": 0.11878311399760724,
      "grad_norm": 1.611116886138916,
      "learning_rate": 1.2879759493841575e-05,
      "loss": 1.3509,
      "num_input_tokens_seen": 1114752,
      "step": 695
    },
    {
      "epoch": 0.11895402495299949,
      "grad_norm": 0.9916453957557678,
      "learning_rate": 1.280350852153168e-05,
      "loss": 1.3235,
      "num_input_tokens_seen": 1117184,
      "step": 696
    },
    {
      "epoch": 0.11912493590839172,
      "grad_norm": 1.1782259941101074,
      "learning_rate": 1.272740615962148e-05,
      "loss": 0.8942,
      "num_input_tokens_seen": 1118336,
      "step": 697
    },
    {
      "epoch": 0.11929584686378397,
      "grad_norm": 1.124538540840149,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 1.1235,
      "num_input_tokens_seen": 1120000,
      "step": 698
    },
    {
      "epoch": 0.1194667578191762,
      "grad_norm": 2.2560579776763916,
      "learning_rate": 1.2575650974311119e-05,
      "loss": 1.0852,
      "num_input_tokens_seen": 1120896,
      "step": 699
    },
    {
      "epoch": 0.11963766877456845,
      "grad_norm": 1.5560222864151,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 1.3017,
      "num_input_tokens_seen": 1122304,
      "step": 700
    },
    {
      "epoch": 0.11980857972996069,
      "grad_norm": 0.9768147468566895,
      "learning_rate": 1.2424501334244123e-05,
      "loss": 1.0049,
      "num_input_tokens_seen": 1124864,
      "step": 701
    },
    {
      "epoch": 0.11997949068535294,
      "grad_norm": 1.1994397640228271,
      "learning_rate": 1.234915589697091e-05,
      "loss": 1.1082,
      "num_input_tokens_seen": 1126400,
      "step": 702
    },
    {
      "epoch": 0.12015040164074517,
      "grad_norm": 1.5477370023727417,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 1.1689,
      "num_input_tokens_seen": 1127552,
      "step": 703
    },
    {
      "epoch": 0.12032131259613742,
      "grad_norm": 1.1795748472213745,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 1.4826,
      "num_input_tokens_seen": 1129344,
      "step": 704
    },
    {
      "epoch": 0.12049222355152965,
      "grad_norm": 1.4725067615509033,
      "learning_rate": 1.2124048127248644e-05,
      "loss": 1.1919,
      "num_input_tokens_seen": 1130368,
      "step": 705
    },
    {
      "epoch": 0.1206631345069219,
      "grad_norm": 1.3832166194915771,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 1.7156,
      "num_input_tokens_seen": 1131776,
      "step": 706
    },
    {
      "epoch": 0.12083404546231413,
      "grad_norm": 1.6462384462356567,
      "learning_rate": 1.19747592039856e-05,
      "loss": 1.2339,
      "num_input_tokens_seen": 1132800,
      "step": 707
    },
    {
      "epoch": 0.12100495641770638,
      "grad_norm": 1.4040213823318481,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 1.4253,
      "num_input_tokens_seen": 1134208,
      "step": 708
    },
    {
      "epoch": 0.12117586737309861,
      "grad_norm": 1.2243554592132568,
      "learning_rate": 1.1826105112583061e-05,
      "loss": 1.2217,
      "num_input_tokens_seen": 1135616,
      "step": 709
    },
    {
      "epoch": 0.12134677832849086,
      "grad_norm": 1.0411616563796997,
      "learning_rate": 1.175201839416988e-05,
      "loss": 1.159,
      "num_input_tokens_seen": 1137920,
      "step": 710
    },
    {
      "epoch": 0.1215176892838831,
      "grad_norm": 1.202208399772644,
      "learning_rate": 1.167809309823175e-05,
      "loss": 1.4263,
      "num_input_tokens_seen": 1139584,
      "step": 711
    },
    {
      "epoch": 0.12168860023927534,
      "grad_norm": 1.8825923204421997,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 1.3394,
      "num_input_tokens_seen": 1140992,
      "step": 712
    },
    {
      "epoch": 0.12185951119466758,
      "grad_norm": 1.3427125215530396,
      "learning_rate": 1.1530730374828422e-05,
      "loss": 1.407,
      "num_input_tokens_seen": 1142400,
      "step": 713
    },
    {
      "epoch": 0.12203042215005983,
      "grad_norm": 1.9530531167984009,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 1.1779,
      "num_input_tokens_seen": 1143552,
      "step": 714
    },
    {
      "epoch": 0.12220133310545206,
      "grad_norm": 0.743361234664917,
      "learning_rate": 1.1384024124624324e-05,
      "loss": 1.5246,
      "num_input_tokens_seen": 1146752,
      "step": 715
    },
    {
      "epoch": 0.1223722440608443,
      "grad_norm": 0.9284049272537231,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 1.2788,
      "num_input_tokens_seen": 1149440,
      "step": 716
    },
    {
      "epoch": 0.12254315501623654,
      "grad_norm": 1.5650255680084229,
      "learning_rate": 1.123798149787511e-05,
      "loss": 1.064,
      "num_input_tokens_seen": 1150464,
      "step": 717
    },
    {
      "epoch": 0.12271406597162877,
      "grad_norm": 1.323313593864441,
      "learning_rate": 1.11652112689164e-05,
      "loss": 1.0377,
      "num_input_tokens_seen": 1151744,
      "step": 718
    },
    {
      "epoch": 0.12288497692702102,
      "grad_norm": 1.720166802406311,
      "learning_rate": 1.109260961249238e-05,
      "loss": 1.0058,
      "num_input_tokens_seen": 1152896,
      "step": 719
    },
    {
      "epoch": 0.12305588788241326,
      "grad_norm": 1.60019850730896,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 1.164,
      "num_input_tokens_seen": 1154048,
      "step": 720
    },
    {
      "epoch": 0.1232267988378055,
      "grad_norm": 1.0083527565002441,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 1.1963,
      "num_input_tokens_seen": 1156096,
      "step": 721
    },
    {
      "epoch": 0.12339770979319774,
      "grad_norm": 1.9554345607757568,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 1.5363,
      "num_input_tokens_seen": 1157376,
      "step": 722
    },
    {
      "epoch": 0.12356862074858999,
      "grad_norm": 1.4090014696121216,
      "learning_rate": 1.0803906373672476e-05,
      "loss": 1.3212,
      "num_input_tokens_seen": 1158784,
      "step": 723
    },
    {
      "epoch": 0.12373953170398222,
      "grad_norm": 2.353886365890503,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 0.9999,
      "num_input_tokens_seen": 1159808,
      "step": 724
    },
    {
      "epoch": 0.12391044265937447,
      "grad_norm": 1.034815788269043,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 1.0778,
      "num_input_tokens_seen": 1161600,
      "step": 725
    },
    {
      "epoch": 0.1240813536147667,
      "grad_norm": 0.853883683681488,
      "learning_rate": 1.058919209575517e-05,
      "loss": 1.3702,
      "num_input_tokens_seen": 1164800,
      "step": 726
    },
    {
      "epoch": 0.12425226457015895,
      "grad_norm": 1.218895435333252,
      "learning_rate": 1.0517970691433035e-05,
      "loss": 1.1983,
      "num_input_tokens_seen": 1166336,
      "step": 727
    },
    {
      "epoch": 0.12442317552555118,
      "grad_norm": 1.4077837467193604,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 1.2098,
      "num_input_tokens_seen": 1167616,
      "step": 728
    },
    {
      "epoch": 0.12459408648094343,
      "grad_norm": 1.1236779689788818,
      "learning_rate": 1.0376058125319613e-05,
      "loss": 1.2765,
      "num_input_tokens_seen": 1169152,
      "step": 729
    },
    {
      "epoch": 0.12476499743633566,
      "grad_norm": 1.2907874584197998,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 1.3681,
      "num_input_tokens_seen": 1170560,
      "step": 730
    },
    {
      "epoch": 0.12493590839172791,
      "grad_norm": 1.347454309463501,
      "learning_rate": 1.0234858309501862e-05,
      "loss": 1.1881,
      "num_input_tokens_seen": 1171968,
      "step": 731
    },
    {
      "epoch": 0.12510681934712015,
      "grad_norm": 0.7594351768493652,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 1.3425,
      "num_input_tokens_seen": 1174656,
      "step": 732
    },
    {
      "epoch": 0.12527773030251238,
      "grad_norm": 1.2517015933990479,
      "learning_rate": 1.0094378125859602e-05,
      "loss": 1.6037,
      "num_input_tokens_seen": 1176320,
      "step": 733
    },
    {
      "epoch": 0.12544864125790464,
      "grad_norm": 1.15589439868927,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 1.4579,
      "num_input_tokens_seen": 1177728,
      "step": 734
    },
    {
      "epoch": 0.12561955221329688,
      "grad_norm": 0.8771323561668396,
      "learning_rate": 9.954624421198792e-06,
      "loss": 1.5869,
      "num_input_tokens_seen": 1179520,
      "step": 735
    },
    {
      "epoch": 0.1257904631686891,
      "grad_norm": 1.241660475730896,
      "learning_rate": 9.88502212844063e-06,
      "loss": 1.1226,
      "num_input_tokens_seen": 1180672,
      "step": 736
    },
    {
      "epoch": 0.12596137412408134,
      "grad_norm": 0.9073563814163208,
      "learning_rate": 9.815604006917839e-06,
      "loss": 1.6426,
      "num_input_tokens_seen": 1182848,
      "step": 737
    },
    {
      "epoch": 0.1261322850794736,
      "grad_norm": 1.5047272443771362,
      "learning_rate": 9.746370902468311e-06,
      "loss": 1.5455,
      "num_input_tokens_seen": 1184768,
      "step": 738
    },
    {
      "epoch": 0.12630319603486584,
      "grad_norm": 1.4601185321807861,
      "learning_rate": 9.677323658675594e-06,
      "loss": 1.4146,
      "num_input_tokens_seen": 1186304,
      "step": 739
    },
    {
      "epoch": 0.12647410699025807,
      "grad_norm": 1.1696341037750244,
      "learning_rate": 9.608463116858542e-06,
      "loss": 1.4031,
      "num_input_tokens_seen": 1187584,
      "step": 740
    },
    {
      "epoch": 0.1266450179456503,
      "grad_norm": 1.355018138885498,
      "learning_rate": 9.539790116061151e-06,
      "loss": 1.3089,
      "num_input_tokens_seen": 1188992,
      "step": 741
    },
    {
      "epoch": 0.12681592890104257,
      "grad_norm": 1.2933521270751953,
      "learning_rate": 9.471305493042243e-06,
      "loss": 1.2222,
      "num_input_tokens_seen": 1190272,
      "step": 742
    },
    {
      "epoch": 0.1269868398564348,
      "grad_norm": 1.3271063566207886,
      "learning_rate": 9.403010082265351e-06,
      "loss": 1.7538,
      "num_input_tokens_seen": 1192064,
      "step": 743
    },
    {
      "epoch": 0.12715775081182704,
      "grad_norm": 0.9197744131088257,
      "learning_rate": 9.334904715888495e-06,
      "loss": 1.2345,
      "num_input_tokens_seen": 1193984,
      "step": 744
    },
    {
      "epoch": 0.12732866176721927,
      "grad_norm": 1.1015379428863525,
      "learning_rate": 9.266990223754069e-06,
      "loss": 1.0935,
      "num_input_tokens_seen": 1195264,
      "step": 745
    },
    {
      "epoch": 0.12749957272261153,
      "grad_norm": 1.4878076314926147,
      "learning_rate": 9.199267433378727e-06,
      "loss": 1.3803,
      "num_input_tokens_seen": 1196416,
      "step": 746
    },
    {
      "epoch": 0.12767048367800377,
      "grad_norm": 1.5479316711425781,
      "learning_rate": 9.131737169943314e-06,
      "loss": 1.2649,
      "num_input_tokens_seen": 1197824,
      "step": 747
    },
    {
      "epoch": 0.127841394633396,
      "grad_norm": 0.99031001329422,
      "learning_rate": 9.064400256282757e-06,
      "loss": 1.1294,
      "num_input_tokens_seen": 1200768,
      "step": 748
    },
    {
      "epoch": 0.12801230558878823,
      "grad_norm": 1.0784908533096313,
      "learning_rate": 8.997257512876108e-06,
      "loss": 1.5283,
      "num_input_tokens_seen": 1202560,
      "step": 749
    },
    {
      "epoch": 0.1281832165441805,
      "grad_norm": 2.140953779220581,
      "learning_rate": 8.930309757836517e-06,
      "loss": 1.1757,
      "num_input_tokens_seen": 1203712,
      "step": 750
    },
    {
      "epoch": 0.12835412749957273,
      "grad_norm": 1.6256788969039917,
      "learning_rate": 8.863557806901233e-06,
      "loss": 1.4694,
      "num_input_tokens_seen": 1204992,
      "step": 751
    },
    {
      "epoch": 0.12852503845496496,
      "grad_norm": 1.2879054546356201,
      "learning_rate": 8.797002473421728e-06,
      "loss": 1.2168,
      "num_input_tokens_seen": 1206144,
      "step": 752
    },
    {
      "epoch": 0.1286959494103572,
      "grad_norm": 1.4319941997528076,
      "learning_rate": 8.73064456835373e-06,
      "loss": 1.6717,
      "num_input_tokens_seen": 1207552,
      "step": 753
    },
    {
      "epoch": 0.12886686036574946,
      "grad_norm": 0.927232563495636,
      "learning_rate": 8.664484900247363e-06,
      "loss": 1.7111,
      "num_input_tokens_seen": 1211264,
      "step": 754
    },
    {
      "epoch": 0.1290377713211417,
      "grad_norm": 1.1681855916976929,
      "learning_rate": 8.598524275237322e-06,
      "loss": 1.3673,
      "num_input_tokens_seen": 1212928,
      "step": 755
    },
    {
      "epoch": 0.12920868227653393,
      "grad_norm": 1.774356722831726,
      "learning_rate": 8.532763497032987e-06,
      "loss": 1.2484,
      "num_input_tokens_seen": 1214336,
      "step": 756
    },
    {
      "epoch": 0.12937959323192616,
      "grad_norm": 0.8763241767883301,
      "learning_rate": 8.467203366908707e-06,
      "loss": 1.2285,
      "num_input_tokens_seen": 1216640,
      "step": 757
    },
    {
      "epoch": 0.1295505041873184,
      "grad_norm": 1.0658588409423828,
      "learning_rate": 8.40184468369396e-06,
      "loss": 1.4898,
      "num_input_tokens_seen": 1218048,
      "step": 758
    },
    {
      "epoch": 0.12972141514271066,
      "grad_norm": 1.0196317434310913,
      "learning_rate": 8.33668824376369e-06,
      "loss": 1.534,
      "num_input_tokens_seen": 1220224,
      "step": 759
    },
    {
      "epoch": 0.1298923260981029,
      "grad_norm": 0.8867720365524292,
      "learning_rate": 8.271734841028553e-06,
      "loss": 1.035,
      "num_input_tokens_seen": 1223424,
      "step": 760
    },
    {
      "epoch": 0.13006323705349512,
      "grad_norm": 0.9483795166015625,
      "learning_rate": 8.206985266925249e-06,
      "loss": 1.2304,
      "num_input_tokens_seen": 1225728,
      "step": 761
    },
    {
      "epoch": 0.13023414800888736,
      "grad_norm": 1.2689613103866577,
      "learning_rate": 8.142440310406924e-06,
      "loss": 1.2076,
      "num_input_tokens_seen": 1227008,
      "step": 762
    },
    {
      "epoch": 0.13040505896427962,
      "grad_norm": 1.5379612445831299,
      "learning_rate": 8.078100757933485e-06,
      "loss": 1.2795,
      "num_input_tokens_seen": 1228800,
      "step": 763
    },
    {
      "epoch": 0.13057596991967185,
      "grad_norm": 0.7210389971733093,
      "learning_rate": 8.013967393462094e-06,
      "loss": 1.181,
      "num_input_tokens_seen": 1231616,
      "step": 764
    },
    {
      "epoch": 0.1307468808750641,
      "grad_norm": 1.3400242328643799,
      "learning_rate": 7.950040998437542e-06,
      "loss": 0.7184,
      "num_input_tokens_seen": 1232640,
      "step": 765
    },
    {
      "epoch": 0.13091779183045632,
      "grad_norm": 1.386775255203247,
      "learning_rate": 7.886322351782783e-06,
      "loss": 1.141,
      "num_input_tokens_seen": 1233792,
      "step": 766
    },
    {
      "epoch": 0.13108870278584858,
      "grad_norm": 1.0909801721572876,
      "learning_rate": 7.822812229889428e-06,
      "loss": 1.2665,
      "num_input_tokens_seen": 1235328,
      "step": 767
    },
    {
      "epoch": 0.13125961374124082,
      "grad_norm": 1.0851480960845947,
      "learning_rate": 7.759511406608255e-06,
      "loss": 0.9639,
      "num_input_tokens_seen": 1236736,
      "step": 768
    },
    {
      "epoch": 0.13143052469663305,
      "grad_norm": 1.1082348823547363,
      "learning_rate": 7.696420653239833e-06,
      "loss": 1.3455,
      "num_input_tokens_seen": 1238272,
      "step": 769
    },
    {
      "epoch": 0.13160143565202528,
      "grad_norm": 1.3094611167907715,
      "learning_rate": 7.633540738525066e-06,
      "loss": 1.2228,
      "num_input_tokens_seen": 1239680,
      "step": 770
    },
    {
      "epoch": 0.13177234660741755,
      "grad_norm": 1.2678766250610352,
      "learning_rate": 7.570872428635889e-06,
      "loss": 1.3585,
      "num_input_tokens_seen": 1241088,
      "step": 771
    },
    {
      "epoch": 0.13194325756280978,
      "grad_norm": 0.9081471562385559,
      "learning_rate": 7.508416487165862e-06,
      "loss": 1.2399,
      "num_input_tokens_seen": 1242496,
      "step": 772
    },
    {
      "epoch": 0.132114168518202,
      "grad_norm": 1.3821220397949219,
      "learning_rate": 7.4461736751209405e-06,
      "loss": 1.0171,
      "num_input_tokens_seen": 1243392,
      "step": 773
    },
    {
      "epoch": 0.13228507947359425,
      "grad_norm": 1.4243266582489014,
      "learning_rate": 7.384144750910133e-06,
      "loss": 1.6419,
      "num_input_tokens_seen": 1244928,
      "step": 774
    },
    {
      "epoch": 0.1324559904289865,
      "grad_norm": 1.3347238302230835,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 1.4084,
      "num_input_tokens_seen": 1246080,
      "step": 775
    },
    {
      "epoch": 0.13262690138437874,
      "grad_norm": 1.892844796180725,
      "learning_rate": 7.260731586586983e-06,
      "loss": 1.222,
      "num_input_tokens_seen": 1247360,
      "step": 776
    },
    {
      "epoch": 0.13279781233977098,
      "grad_norm": 1.1519147157669067,
      "learning_rate": 7.19934885022509e-06,
      "loss": 1.3446,
      "num_input_tokens_seen": 1248768,
      "step": 777
    },
    {
      "epoch": 0.1329687232951632,
      "grad_norm": 1.0032693147659302,
      "learning_rate": 7.138183009179922e-06,
      "loss": 1.4767,
      "num_input_tokens_seen": 1250816,
      "step": 778
    },
    {
      "epoch": 0.13313963425055547,
      "grad_norm": 1.6829365491867065,
      "learning_rate": 7.0772348087379315e-06,
      "loss": 1.276,
      "num_input_tokens_seen": 1252224,
      "step": 779
    },
    {
      "epoch": 0.1333105452059477,
      "grad_norm": 1.2205737829208374,
      "learning_rate": 7.016504991533726e-06,
      "loss": 1.3807,
      "num_input_tokens_seen": 1254400,
      "step": 780
    },
    {
      "epoch": 0.13348145616133994,
      "grad_norm": 1.1474193334579468,
      "learning_rate": 6.9559942975409465e-06,
      "loss": 1.2951,
      "num_input_tokens_seen": 1256192,
      "step": 781
    },
    {
      "epoch": 0.13365236711673217,
      "grad_norm": 1.1373344659805298,
      "learning_rate": 6.895703464063319e-06,
      "loss": 0.9669,
      "num_input_tokens_seen": 1257472,
      "step": 782
    },
    {
      "epoch": 0.13382327807212444,
      "grad_norm": 2.3274779319763184,
      "learning_rate": 6.835633225725605e-06,
      "loss": 0.9802,
      "num_input_tokens_seen": 1258624,
      "step": 783
    },
    {
      "epoch": 0.13399418902751667,
      "grad_norm": 0.8157905340194702,
      "learning_rate": 6.775784314464717e-06,
      "loss": 1.2037,
      "num_input_tokens_seen": 1260416,
      "step": 784
    },
    {
      "epoch": 0.1341650999829089,
      "grad_norm": 1.2313389778137207,
      "learning_rate": 6.716157459520739e-06,
      "loss": 1.4471,
      "num_input_tokens_seen": 1261696,
      "step": 785
    },
    {
      "epoch": 0.13433601093830114,
      "grad_norm": 1.515649437904358,
      "learning_rate": 6.656753387428089e-06,
      "loss": 1.1927,
      "num_input_tokens_seen": 1262976,
      "step": 786
    },
    {
      "epoch": 0.1345069218936934,
      "grad_norm": 1.8788411617279053,
      "learning_rate": 6.5975728220066425e-06,
      "loss": 1.2446,
      "num_input_tokens_seen": 1264000,
      "step": 787
    },
    {
      "epoch": 0.13467783284908563,
      "grad_norm": 1.015358567237854,
      "learning_rate": 6.538616484352902e-06,
      "loss": 0.9728,
      "num_input_tokens_seen": 1265920,
      "step": 788
    },
    {
      "epoch": 0.13484874380447787,
      "grad_norm": 2.1508495807647705,
      "learning_rate": 6.47988509283125e-06,
      "loss": 1.8804,
      "num_input_tokens_seen": 1267072,
      "step": 789
    },
    {
      "epoch": 0.1350196547598701,
      "grad_norm": 1.6655373573303223,
      "learning_rate": 6.421379363065142e-06,
      "loss": 1.0314,
      "num_input_tokens_seen": 1268352,
      "step": 790
    },
    {
      "epoch": 0.13519056571526236,
      "grad_norm": 0.8942627906799316,
      "learning_rate": 6.363100007928446e-06,
      "loss": 1.3335,
      "num_input_tokens_seen": 1270272,
      "step": 791
    },
    {
      "epoch": 0.1353614766706546,
      "grad_norm": 1.1529282331466675,
      "learning_rate": 6.305047737536707e-06,
      "loss": 1.4168,
      "num_input_tokens_seen": 1272704,
      "step": 792
    },
    {
      "epoch": 0.13553238762604683,
      "grad_norm": 1.3043609857559204,
      "learning_rate": 6.247223259238511e-06,
      "loss": 1.5819,
      "num_input_tokens_seen": 1274240,
      "step": 793
    },
    {
      "epoch": 0.13570329858143906,
      "grad_norm": 1.1992034912109375,
      "learning_rate": 6.189627277606894e-06,
      "loss": 1.4734,
      "num_input_tokens_seen": 1276032,
      "step": 794
    },
    {
      "epoch": 0.1358742095368313,
      "grad_norm": 1.2996633052825928,
      "learning_rate": 6.1322604944307e-06,
      "loss": 1.3921,
      "num_input_tokens_seen": 1277696,
      "step": 795
    },
    {
      "epoch": 0.13604512049222356,
      "grad_norm": 1.13285231590271,
      "learning_rate": 6.075123608706093e-06,
      "loss": 1.0373,
      "num_input_tokens_seen": 1278976,
      "step": 796
    },
    {
      "epoch": 0.1362160314476158,
      "grad_norm": 1.2686104774475098,
      "learning_rate": 6.01821731662798e-06,
      "loss": 1.2942,
      "num_input_tokens_seen": 1280384,
      "step": 797
    },
    {
      "epoch": 0.13638694240300803,
      "grad_norm": 1.1396604776382446,
      "learning_rate": 5.961542311581586e-06,
      "loss": 1.3382,
      "num_input_tokens_seen": 1281664,
      "step": 798
    },
    {
      "epoch": 0.13655785335840026,
      "grad_norm": 0.7601625919342041,
      "learning_rate": 5.905099284133952e-06,
      "loss": 0.8818,
      "num_input_tokens_seen": 1285248,
      "step": 799
    },
    {
      "epoch": 0.13672876431379252,
      "grad_norm": 1.1888246536254883,
      "learning_rate": 5.848888922025553e-06,
      "loss": 1.9286,
      "num_input_tokens_seen": 1287168,
      "step": 800
    },
    {
      "epoch": 0.13689967526918476,
      "grad_norm": 1.5452656745910645,
      "learning_rate": 5.792911910161922e-06,
      "loss": 1.4889,
      "num_input_tokens_seen": 1288576,
      "step": 801
    },
    {
      "epoch": 0.137070586224577,
      "grad_norm": 1.860674500465393,
      "learning_rate": 5.737168930605272e-06,
      "loss": 1.192,
      "num_input_tokens_seen": 1289984,
      "step": 802
    },
    {
      "epoch": 0.13724149717996922,
      "grad_norm": 1.034842848777771,
      "learning_rate": 5.681660662566224e-06,
      "loss": 1.3144,
      "num_input_tokens_seen": 1291520,
      "step": 803
    },
    {
      "epoch": 0.13741240813536149,
      "grad_norm": 1.2152104377746582,
      "learning_rate": 5.626387782395512e-06,
      "loss": 1.1976,
      "num_input_tokens_seen": 1293056,
      "step": 804
    },
    {
      "epoch": 0.13758331909075372,
      "grad_norm": 1.2993017435073853,
      "learning_rate": 5.571350963575728e-06,
      "loss": 1.5308,
      "num_input_tokens_seen": 1295232,
      "step": 805
    },
    {
      "epoch": 0.13775423004614595,
      "grad_norm": 1.784773588180542,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 0.909,
      "num_input_tokens_seen": 1296384,
      "step": 806
    },
    {
      "epoch": 0.1379251410015382,
      "grad_norm": 1.0589262247085571,
      "learning_rate": 5.461988189529529e-06,
      "loss": 1.2747,
      "num_input_tokens_seen": 1297792,
      "step": 807
    },
    {
      "epoch": 0.13809605195693045,
      "grad_norm": 1.3898260593414307,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 1.2927,
      "num_input_tokens_seen": 1298944,
      "step": 808
    },
    {
      "epoch": 0.13826696291232268,
      "grad_norm": 2.021599292755127,
      "learning_rate": 5.3535776706149505e-06,
      "loss": 1.2308,
      "num_input_tokens_seen": 1300480,
      "step": 809
    },
    {
      "epoch": 0.13843787386771492,
      "grad_norm": 1.5169730186462402,
      "learning_rate": 5.299731159831953e-06,
      "loss": 1.2924,
      "num_input_tokens_seen": 1301760,
      "step": 810
    },
    {
      "epoch": 0.13860878482310715,
      "grad_norm": 1.3593571186065674,
      "learning_rate": 5.24612469060774e-06,
      "loss": 1.3075,
      "num_input_tokens_seen": 1302784,
      "step": 811
    },
    {
      "epoch": 0.1387796957784994,
      "grad_norm": 1.1218104362487793,
      "learning_rate": 5.192758916120236e-06,
      "loss": 1.1267,
      "num_input_tokens_seen": 1304192,
      "step": 812
    },
    {
      "epoch": 0.13895060673389165,
      "grad_norm": 1.2411898374557495,
      "learning_rate": 5.139634486614544e-06,
      "loss": 1.0783,
      "num_input_tokens_seen": 1305728,
      "step": 813
    },
    {
      "epoch": 0.13912151768928388,
      "grad_norm": 1.1922005414962769,
      "learning_rate": 5.086752049395094e-06,
      "loss": 1.5632,
      "num_input_tokens_seen": 1307520,
      "step": 814
    },
    {
      "epoch": 0.13929242864467611,
      "grad_norm": 1.0882115364074707,
      "learning_rate": 5.034112248817685e-06,
      "loss": 1.4414,
      "num_input_tokens_seen": 1310720,
      "step": 815
    },
    {
      "epoch": 0.13946333960006838,
      "grad_norm": 1.052266240119934,
      "learning_rate": 4.981715726281666e-06,
      "loss": 1.2853,
      "num_input_tokens_seen": 1312000,
      "step": 816
    },
    {
      "epoch": 0.1396342505554606,
      "grad_norm": 1.0469086170196533,
      "learning_rate": 4.929563120222141e-06,
      "loss": 0.9702,
      "num_input_tokens_seen": 1313280,
      "step": 817
    },
    {
      "epoch": 0.13980516151085284,
      "grad_norm": 1.2313703298568726,
      "learning_rate": 4.877655066102149e-06,
      "loss": 1.1455,
      "num_input_tokens_seen": 1314432,
      "step": 818
    },
    {
      "epoch": 0.13997607246624508,
      "grad_norm": 1.452987790107727,
      "learning_rate": 4.825992196404957e-06,
      "loss": 1.4029,
      "num_input_tokens_seen": 1315712,
      "step": 819
    },
    {
      "epoch": 0.14014698342163734,
      "grad_norm": 1.1184760332107544,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 1.5466,
      "num_input_tokens_seen": 1317888,
      "step": 820
    },
    {
      "epoch": 0.14031789437702957,
      "grad_norm": 1.1861913204193115,
      "learning_rate": 4.723404525266839e-06,
      "loss": 1.3677,
      "num_input_tokens_seen": 1319040,
      "step": 821
    },
    {
      "epoch": 0.1404888053324218,
      "grad_norm": 0.8134084939956665,
      "learning_rate": 4.672480973824311e-06,
      "loss": 1.2922,
      "num_input_tokens_seen": 1320704,
      "step": 822
    },
    {
      "epoch": 0.14065971628781404,
      "grad_norm": 1.0305551290512085,
      "learning_rate": 4.621805106786142e-06,
      "loss": 1.3558,
      "num_input_tokens_seen": 1322624,
      "step": 823
    },
    {
      "epoch": 0.1408306272432063,
      "grad_norm": 1.1920245885849,
      "learning_rate": 4.571377541621788e-06,
      "loss": 1.1893,
      "num_input_tokens_seen": 1323776,
      "step": 824
    },
    {
      "epoch": 0.14100153819859854,
      "grad_norm": 2.2472264766693115,
      "learning_rate": 4.521198892775203e-06,
      "loss": 1.4359,
      "num_input_tokens_seen": 1325056,
      "step": 825
    },
    {
      "epoch": 0.14117244915399077,
      "grad_norm": 1.4728949069976807,
      "learning_rate": 4.4712697716574e-06,
      "loss": 1.0496,
      "num_input_tokens_seen": 1326336,
      "step": 826
    },
    {
      "epoch": 0.141343360109383,
      "grad_norm": 0.9130901098251343,
      "learning_rate": 4.421590786638951e-06,
      "loss": 1.2303,
      "num_input_tokens_seen": 1329408,
      "step": 827
    },
    {
      "epoch": 0.14151427106477527,
      "grad_norm": 1.2225362062454224,
      "learning_rate": 4.372162543042624e-06,
      "loss": 1.2908,
      "num_input_tokens_seen": 1330432,
      "step": 828
    },
    {
      "epoch": 0.1416851820201675,
      "grad_norm": 1.0512142181396484,
      "learning_rate": 4.322985643135952e-06,
      "loss": 0.8727,
      "num_input_tokens_seen": 1331968,
      "step": 829
    },
    {
      "epoch": 0.14185609297555973,
      "grad_norm": 0.9386869668960571,
      "learning_rate": 4.274060686123959e-06,
      "loss": 1.0239,
      "num_input_tokens_seen": 1334272,
      "step": 830
    },
    {
      "epoch": 0.14202700393095197,
      "grad_norm": 1.494614601135254,
      "learning_rate": 4.225388268141797e-06,
      "loss": 1.0142,
      "num_input_tokens_seen": 1336064,
      "step": 831
    },
    {
      "epoch": 0.1421979148863442,
      "grad_norm": 1.4117918014526367,
      "learning_rate": 4.176968982247514e-06,
      "loss": 1.3031,
      "num_input_tokens_seen": 1337472,
      "step": 832
    },
    {
      "epoch": 0.14236882584173646,
      "grad_norm": 1.1854019165039062,
      "learning_rate": 4.128803418414839e-06,
      "loss": 1.2582,
      "num_input_tokens_seen": 1338880,
      "step": 833
    },
    {
      "epoch": 0.1425397367971287,
      "grad_norm": 1.1391500234603882,
      "learning_rate": 4.08089216352596e-06,
      "loss": 1.002,
      "num_input_tokens_seen": 1340544,
      "step": 834
    },
    {
      "epoch": 0.14271064775252093,
      "grad_norm": 1.1694997549057007,
      "learning_rate": 4.0332358013644016e-06,
      "loss": 1.1415,
      "num_input_tokens_seen": 1341696,
      "step": 835
    },
    {
      "epoch": 0.14288155870791316,
      "grad_norm": 1.3301547765731812,
      "learning_rate": 3.985834912607894e-06,
      "loss": 0.942,
      "num_input_tokens_seen": 1342976,
      "step": 836
    },
    {
      "epoch": 0.14305246966330543,
      "grad_norm": 1.1023616790771484,
      "learning_rate": 3.938690074821313e-06,
      "loss": 1.2828,
      "num_input_tokens_seen": 1344512,
      "step": 837
    },
    {
      "epoch": 0.14322338061869766,
      "grad_norm": 1.4206124544143677,
      "learning_rate": 3.891801862449629e-06,
      "loss": 1.3126,
      "num_input_tokens_seen": 1345792,
      "step": 838
    },
    {
      "epoch": 0.1433942915740899,
      "grad_norm": 1.0976618528366089,
      "learning_rate": 3.845170846810902e-06,
      "loss": 1.3137,
      "num_input_tokens_seen": 1347072,
      "step": 839
    },
    {
      "epoch": 0.14356520252948213,
      "grad_norm": 1.187723994255066,
      "learning_rate": 3.798797596089351e-06,
      "loss": 0.8729,
      "num_input_tokens_seen": 1348480,
      "step": 840
    },
    {
      "epoch": 0.1437361134848744,
      "grad_norm": 1.1579833030700684,
      "learning_rate": 3.752682675328406e-06,
      "loss": 1.461,
      "num_input_tokens_seen": 1350272,
      "step": 841
    },
    {
      "epoch": 0.14390702444026662,
      "grad_norm": 1.269291877746582,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 0.9909,
      "num_input_tokens_seen": 1351168,
      "step": 842
    },
    {
      "epoch": 0.14407793539565886,
      "grad_norm": 1.87632417678833,
      "learning_rate": 3.661230068116811e-06,
      "loss": 1.2848,
      "num_input_tokens_seen": 1352192,
      "step": 843
    },
    {
      "epoch": 0.1442488463510511,
      "grad_norm": 0.9860010147094727,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 0.9242,
      "num_input_tokens_seen": 1355136,
      "step": 844
    },
    {
      "epoch": 0.14441975730644335,
      "grad_norm": 1.0316380262374878,
      "learning_rate": 3.5708174824471947e-06,
      "loss": 1.4212,
      "num_input_tokens_seen": 1357056,
      "step": 845
    },
    {
      "epoch": 0.1445906682618356,
      "grad_norm": 1.077081322669983,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 1.2989,
      "num_input_tokens_seen": 1358848,
      "step": 846
    },
    {
      "epoch": 0.14476157921722782,
      "grad_norm": 1.5776399374008179,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 1.2986,
      "num_input_tokens_seen": 1360128,
      "step": 847
    },
    {
      "epoch": 0.14493249017262005,
      "grad_norm": 1.116262674331665,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 0.8913,
      "num_input_tokens_seen": 1361792,
      "step": 848
    },
    {
      "epoch": 0.14510340112801232,
      "grad_norm": 1.3267170190811157,
      "learning_rate": 3.393129951157384e-06,
      "loss": 1.4378,
      "num_input_tokens_seen": 1362944,
      "step": 849
    },
    {
      "epoch": 0.14527431208340455,
      "grad_norm": 0.9976586103439331,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.8418,
      "num_input_tokens_seen": 1364608,
      "step": 850
    },
    {
      "epoch": 0.14544522303879678,
      "grad_norm": 1.0878413915634155,
      "learning_rate": 3.305863665776793e-06,
      "loss": 1.3859,
      "num_input_tokens_seen": 1367168,
      "step": 851
    },
    {
      "epoch": 0.14561613399418902,
      "grad_norm": 0.9897251725196838,
      "learning_rate": 3.262626762369525e-06,
      "loss": 1.7746,
      "num_input_tokens_seen": 1369856,
      "step": 852
    },
    {
      "epoch": 0.14578704494958128,
      "grad_norm": 1.1449042558670044,
      "learning_rate": 3.219654721995266e-06,
      "loss": 1.2649,
      "num_input_tokens_seen": 1371520,
      "step": 853
    },
    {
      "epoch": 0.1459579559049735,
      "grad_norm": 1.4321788549423218,
      "learning_rate": 3.176948068254762e-06,
      "loss": 1.3288,
      "num_input_tokens_seen": 1372544,
      "step": 854
    },
    {
      "epoch": 0.14612886686036575,
      "grad_norm": 1.1559323072433472,
      "learning_rate": 3.1345073215151066e-06,
      "loss": 1.4201,
      "num_input_tokens_seen": 1373952,
      "step": 855
    },
    {
      "epoch": 0.14629977781575798,
      "grad_norm": 1.3518966436386108,
      "learning_rate": 3.092332998903416e-06,
      "loss": 1.219,
      "num_input_tokens_seen": 1375232,
      "step": 856
    },
    {
      "epoch": 0.14647068877115024,
      "grad_norm": 0.9331943392753601,
      "learning_rate": 3.0504256143004866e-06,
      "loss": 1.2026,
      "num_input_tokens_seen": 1377536,
      "step": 857
    },
    {
      "epoch": 0.14664159972654248,
      "grad_norm": 1.206374168395996,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 1.37,
      "num_input_tokens_seen": 1379072,
      "step": 858
    },
    {
      "epoch": 0.1468125106819347,
      "grad_norm": 1.3218806982040405,
      "learning_rate": 2.967413698375196e-06,
      "loss": 1.0882,
      "num_input_tokens_seen": 1380608,
      "step": 859
    },
    {
      "epoch": 0.14698342163732694,
      "grad_norm": 1.1909106969833374,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 1.028,
      "num_input_tokens_seen": 1381888,
      "step": 860
    },
    {
      "epoch": 0.1471543325927192,
      "grad_norm": 1.576419472694397,
      "learning_rate": 2.8854756196229016e-06,
      "loss": 1.3133,
      "num_input_tokens_seen": 1383040,
      "step": 861
    },
    {
      "epoch": 0.14732524354811144,
      "grad_norm": 1.4733809232711792,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 0.9629,
      "num_input_tokens_seen": 1383936,
      "step": 862
    },
    {
      "epoch": 0.14749615450350367,
      "grad_norm": 1.2588505744934082,
      "learning_rate": 2.8046153715899692e-06,
      "loss": 1.2192,
      "num_input_tokens_seen": 1385728,
      "step": 863
    },
    {
      "epoch": 0.1476670654588959,
      "grad_norm": 1.3809770345687866,
      "learning_rate": 2.764590667717562e-06,
      "loss": 1.5283,
      "num_input_tokens_seen": 1387392,
      "step": 864
    },
    {
      "epoch": 0.14783797641428817,
      "grad_norm": 1.7464593648910522,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 1.4633,
      "num_input_tokens_seen": 1388544,
      "step": 865
    },
    {
      "epoch": 0.1480088873696804,
      "grad_norm": 1.2380460500717163,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 1.289,
      "num_input_tokens_seen": 1390080,
      "step": 866
    },
    {
      "epoch": 0.14817979832507264,
      "grad_norm": 1.0820006132125854,
      "learning_rate": 2.646144079015797e-06,
      "loss": 1.0689,
      "num_input_tokens_seen": 1391872,
      "step": 867
    },
    {
      "epoch": 0.14835070928046487,
      "grad_norm": 1.4117571115493774,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 1.1782,
      "num_input_tokens_seen": 1393280,
      "step": 868
    },
    {
      "epoch": 0.1485216202358571,
      "grad_norm": 1.7439886331558228,
      "learning_rate": 2.5685407581417907e-06,
      "loss": 1.3284,
      "num_input_tokens_seen": 1394432,
      "step": 869
    },
    {
      "epoch": 0.14869253119124937,
      "grad_norm": 1.5166078805923462,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 1.1636,
      "num_input_tokens_seen": 1395840,
      "step": 870
    },
    {
      "epoch": 0.1488634421466416,
      "grad_norm": 1.124435544013977,
      "learning_rate": 2.492030714945162e-06,
      "loss": 1.1276,
      "num_input_tokens_seen": 1397888,
      "step": 871
    },
    {
      "epoch": 0.14903435310203383,
      "grad_norm": 1.6896885633468628,
      "learning_rate": 2.454186839872158e-06,
      "loss": 1.0896,
      "num_input_tokens_seen": 1398912,
      "step": 872
    },
    {
      "epoch": 0.14920526405742607,
      "grad_norm": 1.5593105554580688,
      "learning_rate": 2.4166176784174795e-06,
      "loss": 1.3285,
      "num_input_tokens_seen": 1400064,
      "step": 873
    },
    {
      "epoch": 0.14937617501281833,
      "grad_norm": 1.4158780574798584,
      "learning_rate": 2.379323688349516e-06,
      "loss": 1.3602,
      "num_input_tokens_seen": 1401728,
      "step": 874
    },
    {
      "epoch": 0.14954708596821056,
      "grad_norm": 1.836614966392517,
      "learning_rate": 2.3423053240837515e-06,
      "loss": 1.1402,
      "num_input_tokens_seen": 1403008,
      "step": 875
    },
    {
      "epoch": 0.1497179969236028,
      "grad_norm": 1.524086594581604,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 1.3828,
      "num_input_tokens_seen": 1404032,
      "step": 876
    },
    {
      "epoch": 0.14988890787899503,
      "grad_norm": 1.3214770555496216,
      "learning_rate": 2.269097273823287e-06,
      "loss": 1.035,
      "num_input_tokens_seen": 1405568,
      "step": 877
    },
    {
      "epoch": 0.1500598188343873,
      "grad_norm": 1.4707361459732056,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 1.2834,
      "num_input_tokens_seen": 1406848,
      "step": 878
    },
    {
      "epoch": 0.15023072978977953,
      "grad_norm": 1.1581166982650757,
      "learning_rate": 2.1969970956931762e-06,
      "loss": 1.055,
      "num_input_tokens_seen": 1408768,
      "step": 879
    },
    {
      "epoch": 0.15040164074517176,
      "grad_norm": 1.1640604734420776,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 0.9085,
      "num_input_tokens_seen": 1410432,
      "step": 880
    },
    {
      "epoch": 0.150572551700564,
      "grad_norm": 1.515830636024475,
      "learning_rate": 2.1260083037543817e-06,
      "loss": 1.0848,
      "num_input_tokens_seen": 1411712,
      "step": 881
    },
    {
      "epoch": 0.15074346265595626,
      "grad_norm": 2.0324416160583496,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 1.3526,
      "num_input_tokens_seen": 1412736,
      "step": 882
    },
    {
      "epoch": 0.1509143736113485,
      "grad_norm": 1.426978349685669,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 1.2092,
      "num_input_tokens_seen": 1413888,
      "step": 883
    },
    {
      "epoch": 0.15108528456674072,
      "grad_norm": 0.9203426837921143,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 1.5626,
      "num_input_tokens_seen": 1417472,
      "step": 884
    },
    {
      "epoch": 0.15125619552213296,
      "grad_norm": 0.9721229672431946,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 1.2204,
      "num_input_tokens_seen": 1420928,
      "step": 885
    },
    {
      "epoch": 0.15142710647752522,
      "grad_norm": 1.1609843969345093,
      "learning_rate": 1.95342121028749e-06,
      "loss": 1.0952,
      "num_input_tokens_seen": 1422720,
      "step": 886
    },
    {
      "epoch": 0.15159801743291745,
      "grad_norm": 1.433011770248413,
      "learning_rate": 1.9197445721754776e-06,
      "loss": 1.3586,
      "num_input_tokens_seen": 1424000,
      "step": 887
    },
    {
      "epoch": 0.1517689283883097,
      "grad_norm": 1.597603678703308,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 1.1095,
      "num_input_tokens_seen": 1425664,
      "step": 888
    },
    {
      "epoch": 0.15193983934370192,
      "grad_norm": 0.9311800003051758,
      "learning_rate": 1.8532353797501318e-06,
      "loss": 1.2704,
      "num_input_tokens_seen": 1427712,
      "step": 889
    },
    {
      "epoch": 0.15211075029909418,
      "grad_norm": 1.432224988937378,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 1.4539,
      "num_input_tokens_seen": 1429888,
      "step": 890
    },
    {
      "epoch": 0.15228166125448642,
      "grad_norm": 1.2645517587661743,
      "learning_rate": 1.787854327977162e-06,
      "loss": 1.3658,
      "num_input_tokens_seen": 1431296,
      "step": 891
    },
    {
      "epoch": 0.15245257220987865,
      "grad_norm": 2.107800006866455,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 1.2844,
      "num_input_tokens_seen": 1432320,
      "step": 892
    },
    {
      "epoch": 0.15262348316527088,
      "grad_norm": 1.3306528329849243,
      "learning_rate": 1.7236046034367958e-06,
      "loss": 1.6368,
      "num_input_tokens_seen": 1434368,
      "step": 893
    },
    {
      "epoch": 0.15279439412066315,
      "grad_norm": 1.5626564025878906,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 1.3363,
      "num_input_tokens_seen": 1435776,
      "step": 894
    },
    {
      "epoch": 0.15296530507605538,
      "grad_norm": 1.439666986465454,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 1.2809,
      "num_input_tokens_seen": 1437824,
      "step": 895
    },
    {
      "epoch": 0.15313621603144761,
      "grad_norm": 1.0919952392578125,
      "learning_rate": 1.629358090099639e-06,
      "loss": 1.4008,
      "num_input_tokens_seen": 1439360,
      "step": 896
    },
    {
      "epoch": 0.15330712698683985,
      "grad_norm": 1.0641480684280396,
      "learning_rate": 1.5985116065256684e-06,
      "loss": 1.0371,
      "num_input_tokens_seen": 1441536,
      "step": 897
    },
    {
      "epoch": 0.1534780379422321,
      "grad_norm": 1.1555286645889282,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 1.4055,
      "num_input_tokens_seen": 1443200,
      "step": 898
    },
    {
      "epoch": 0.15364894889762434,
      "grad_norm": 1.25511634349823,
      "learning_rate": 1.5376744310111019e-06,
      "loss": 0.8945,
      "num_input_tokens_seen": 1445248,
      "step": 899
    },
    {
      "epoch": 0.15381985985301658,
      "grad_norm": 1.616089105606079,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 1.2633,
      "num_input_tokens_seen": 1446144,
      "step": 900
    },
    {
      "epoch": 0.1539907708084088,
      "grad_norm": 1.5981040000915527,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 1.2341,
      "num_input_tokens_seen": 1447424,
      "step": 901
    },
    {
      "epoch": 0.15416168176380107,
      "grad_norm": 1.2001171112060547,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 1.2011,
      "num_input_tokens_seen": 1449856,
      "step": 902
    },
    {
      "epoch": 0.1543325927191933,
      "grad_norm": 1.4192404747009277,
      "learning_rate": 1.4194335513099761e-06,
      "loss": 0.9583,
      "num_input_tokens_seen": 1451136,
      "step": 903
    },
    {
      "epoch": 0.15450350367458554,
      "grad_norm": 1.5193495750427246,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 1.581,
      "num_input_tokens_seen": 1452416,
      "step": 904
    },
    {
      "epoch": 0.15467441462997777,
      "grad_norm": 0.9636537432670593,
      "learning_rate": 1.362035610017079e-06,
      "loss": 1.4089,
      "num_input_tokens_seen": 1454720,
      "step": 905
    },
    {
      "epoch": 0.15484532558537,
      "grad_norm": 1.145270824432373,
      "learning_rate": 1.333768497107593e-06,
      "loss": 1.2912,
      "num_input_tokens_seen": 1455872,
      "step": 906
    },
    {
      "epoch": 0.15501623654076227,
      "grad_norm": 1.1519962549209595,
      "learning_rate": 1.305789749760361e-06,
      "loss": 1.8063,
      "num_input_tokens_seen": 1457280,
      "step": 907
    },
    {
      "epoch": 0.1551871474961545,
      "grad_norm": 1.2856088876724243,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 1.4918,
      "num_input_tokens_seen": 1458944,
      "step": 908
    },
    {
      "epoch": 0.15535805845154674,
      "grad_norm": 1.0581367015838623,
      "learning_rate": 1.250698711883691e-06,
      "loss": 1.2023,
      "num_input_tokens_seen": 1460864,
      "step": 909
    },
    {
      "epoch": 0.15552896940693897,
      "grad_norm": 1.1302508115768433,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 1.1574,
      "num_input_tokens_seen": 1462144,
      "step": 910
    },
    {
      "epoch": 0.15569988036233123,
      "grad_norm": 2.128718376159668,
      "learning_rate": 1.1967651814465354e-06,
      "loss": 1.0081,
      "num_input_tokens_seen": 1463040,
      "step": 911
    },
    {
      "epoch": 0.15587079131772347,
      "grad_norm": 1.2116367816925049,
      "learning_rate": 1.170233305176327e-06,
      "loss": 1.4967,
      "num_input_tokens_seen": 1466240,
      "step": 912
    },
    {
      "epoch": 0.1560417022731157,
      "grad_norm": 1.3587337732315063,
      "learning_rate": 1.1439917870930793e-06,
      "loss": 1.2931,
      "num_input_tokens_seen": 1467392,
      "step": 913
    },
    {
      "epoch": 0.15621261322850793,
      "grad_norm": 1.382611632347107,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 1.1517,
      "num_input_tokens_seen": 1469312,
      "step": 914
    },
    {
      "epoch": 0.1563835241839002,
      "grad_norm": 1.1362651586532593,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 1.2739,
      "num_input_tokens_seen": 1471360,
      "step": 915
    },
    {
      "epoch": 0.15655443513929243,
      "grad_norm": 1.006221055984497,
      "learning_rate": 1.067012561698319e-06,
      "loss": 1.5087,
      "num_input_tokens_seen": 1473152,
      "step": 916
    },
    {
      "epoch": 0.15672534609468466,
      "grad_norm": 2.274958848953247,
      "learning_rate": 1.0419356383716688e-06,
      "loss": 1.2366,
      "num_input_tokens_seen": 1474176,
      "step": 917
    },
    {
      "epoch": 0.1568962570500769,
      "grad_norm": 1.58634352684021,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 1.1047,
      "num_input_tokens_seen": 1476224,
      "step": 918
    },
    {
      "epoch": 0.15706716800546916,
      "grad_norm": 1.7188833951950073,
      "learning_rate": 9.926578580764234e-07,
      "loss": 1.4236,
      "num_input_tokens_seen": 1477632,
      "step": 919
    },
    {
      "epoch": 0.1572380789608614,
      "grad_norm": 1.2681986093521118,
      "learning_rate": 9.684576015420278e-07,
      "loss": 1.3838,
      "num_input_tokens_seen": 1479296,
      "step": 920
    },
    {
      "epoch": 0.15740898991625363,
      "grad_norm": 1.461035132408142,
      "learning_rate": 9.445501617678654e-07,
      "loss": 1.2004,
      "num_input_tokens_seen": 1480448,
      "step": 921
    },
    {
      "epoch": 0.15757990087164586,
      "grad_norm": 1.0013573169708252,
      "learning_rate": 9.209358300585474e-07,
      "loss": 1.2345,
      "num_input_tokens_seen": 1481984,
      "step": 922
    },
    {
      "epoch": 0.15775081182703812,
      "grad_norm": 1.3375686407089233,
      "learning_rate": 8.976148941472501e-07,
      "loss": 1.5232,
      "num_input_tokens_seen": 1483648,
      "step": 923
    },
    {
      "epoch": 0.15792172278243036,
      "grad_norm": 1.1407254934310913,
      "learning_rate": 8.745876381922147e-07,
      "loss": 1.2186,
      "num_input_tokens_seen": 1485184,
      "step": 924
    },
    {
      "epoch": 0.1580926337378226,
      "grad_norm": 1.2354094982147217,
      "learning_rate": 8.51854342773295e-07,
      "loss": 1.1846,
      "num_input_tokens_seen": 1487104,
      "step": 925
    },
    {
      "epoch": 0.15826354469321482,
      "grad_norm": 1.4203088283538818,
      "learning_rate": 8.294152848885157e-07,
      "loss": 0.8531,
      "num_input_tokens_seen": 1489280,
      "step": 926
    },
    {
      "epoch": 0.1584344556486071,
      "grad_norm": 0.829649806022644,
      "learning_rate": 8.072707379507216e-07,
      "loss": 1.2442,
      "num_input_tokens_seen": 1492352,
      "step": 927
    },
    {
      "epoch": 0.15860536660399932,
      "grad_norm": 1.0982556343078613,
      "learning_rate": 7.854209717842231e-07,
      "loss": 1.6088,
      "num_input_tokens_seen": 1494400,
      "step": 928
    },
    {
      "epoch": 0.15877627755939155,
      "grad_norm": 1.4998908042907715,
      "learning_rate": 7.638662526215284e-07,
      "loss": 1.4094,
      "num_input_tokens_seen": 1495808,
      "step": 929
    },
    {
      "epoch": 0.1589471885147838,
      "grad_norm": 1.1602911949157715,
      "learning_rate": 7.426068431000882e-07,
      "loss": 1.5794,
      "num_input_tokens_seen": 1497728,
      "step": 930
    },
    {
      "epoch": 0.15911809947017605,
      "grad_norm": 1.28364896774292,
      "learning_rate": 7.216430022591008e-07,
      "loss": 1.5104,
      "num_input_tokens_seen": 1499264,
      "step": 931
    },
    {
      "epoch": 0.15928901042556828,
      "grad_norm": 1.5680805444717407,
      "learning_rate": 7.009749855363456e-07,
      "loss": 1.2905,
      "num_input_tokens_seen": 1500672,
      "step": 932
    },
    {
      "epoch": 0.15945992138096052,
      "grad_norm": 1.3869675397872925,
      "learning_rate": 6.806030447650879e-07,
      "loss": 1.2259,
      "num_input_tokens_seen": 1501824,
      "step": 933
    },
    {
      "epoch": 0.15963083233635275,
      "grad_norm": 1.1915948390960693,
      "learning_rate": 6.605274281709928e-07,
      "loss": 1.1045,
      "num_input_tokens_seen": 1505408,
      "step": 934
    },
    {
      "epoch": 0.159801743291745,
      "grad_norm": 1.0018951892852783,
      "learning_rate": 6.407483803691216e-07,
      "loss": 1.0814,
      "num_input_tokens_seen": 1507456,
      "step": 935
    },
    {
      "epoch": 0.15997265424713725,
      "grad_norm": 1.230881690979004,
      "learning_rate": 6.212661423609184e-07,
      "loss": 1.7491,
      "num_input_tokens_seen": 1509120,
      "step": 936
    },
    {
      "epoch": 0.16014356520252948,
      "grad_norm": 0.8036161661148071,
      "learning_rate": 6.020809515313142e-07,
      "loss": 1.3342,
      "num_input_tokens_seen": 1511936,
      "step": 937
    },
    {
      "epoch": 0.16031447615792171,
      "grad_norm": 1.5798101425170898,
      "learning_rate": 5.83193041645802e-07,
      "loss": 1.2073,
      "num_input_tokens_seen": 1513472,
      "step": 938
    },
    {
      "epoch": 0.16048538711331398,
      "grad_norm": 0.9174858331680298,
      "learning_rate": 5.646026428476031e-07,
      "loss": 1.246,
      "num_input_tokens_seen": 1516032,
      "step": 939
    },
    {
      "epoch": 0.1606562980687062,
      "grad_norm": 1.1037200689315796,
      "learning_rate": 5.463099816548579e-07,
      "loss": 1.3583,
      "num_input_tokens_seen": 1517440,
      "step": 940
    },
    {
      "epoch": 0.16082720902409844,
      "grad_norm": 1.2390100955963135,
      "learning_rate": 5.283152809578751e-07,
      "loss": 1.2856,
      "num_input_tokens_seen": 1519104,
      "step": 941
    },
    {
      "epoch": 0.16099811997949068,
      "grad_norm": 1.3245415687561035,
      "learning_rate": 5.106187600163987e-07,
      "loss": 1.6292,
      "num_input_tokens_seen": 1521536,
      "step": 942
    },
    {
      "epoch": 0.16116903093488294,
      "grad_norm": 1.2784466743469238,
      "learning_rate": 4.932206344569562e-07,
      "loss": 1.2274,
      "num_input_tokens_seen": 1523200,
      "step": 943
    },
    {
      "epoch": 0.16133994189027517,
      "grad_norm": 1.680444359779358,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 1.0736,
      "num_input_tokens_seen": 1524224,
      "step": 944
    },
    {
      "epoch": 0.1615108528456674,
      "grad_norm": 1.5389809608459473,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 1.4572,
      "num_input_tokens_seen": 1525760,
      "step": 945
    },
    {
      "epoch": 0.16168176380105964,
      "grad_norm": 1.3516191244125366,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 1.5376,
      "num_input_tokens_seen": 1527168,
      "step": 946
    },
    {
      "epoch": 0.16185267475645188,
      "grad_norm": 1.3067411184310913,
      "learning_rate": 4.26616271261146e-07,
      "loss": 1.3427,
      "num_input_tokens_seen": 1528704,
      "step": 947
    },
    {
      "epoch": 0.16202358571184414,
      "grad_norm": 1.7368286848068237,
      "learning_rate": 4.107132296653549e-07,
      "loss": 1.4243,
      "num_input_tokens_seen": 1529984,
      "step": 948
    },
    {
      "epoch": 0.16219449666723637,
      "grad_norm": 1.6182806491851807,
      "learning_rate": 3.95109800768953e-07,
      "loss": 1.0776,
      "num_input_tokens_seen": 1530880,
      "step": 949
    },
    {
      "epoch": 0.1623654076226286,
      "grad_norm": 0.9815467000007629,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 1.4843,
      "num_input_tokens_seen": 1532288,
      "step": 950
    },
    {
      "epoch": 0.16253631857802084,
      "grad_norm": 1.1946020126342773,
      "learning_rate": 3.6480253791274786e-07,
      "loss": 1.6809,
      "num_input_tokens_seen": 1533568,
      "step": 951
    },
    {
      "epoch": 0.1627072295334131,
      "grad_norm": 1.397243857383728,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 1.5857,
      "num_input_tokens_seen": 1535488,
      "step": 952
    },
    {
      "epoch": 0.16287814048880533,
      "grad_norm": 1.1160595417022705,
      "learning_rate": 3.3569595982576583e-07,
      "loss": 1.2309,
      "num_input_tokens_seen": 1536896,
      "step": 953
    },
    {
      "epoch": 0.16304905144419757,
      "grad_norm": 1.660562515258789,
      "learning_rate": 3.215933731753024e-07,
      "loss": 1.505,
      "num_input_tokens_seen": 1538176,
      "step": 954
    },
    {
      "epoch": 0.1632199623995898,
      "grad_norm": 1.9637283086776733,
      "learning_rate": 3.077914851215585e-07,
      "loss": 1.1598,
      "num_input_tokens_seen": 1539456,
      "step": 955
    },
    {
      "epoch": 0.16339087335498206,
      "grad_norm": 1.1801072359085083,
      "learning_rate": 2.942904638361804e-07,
      "loss": 1.5979,
      "num_input_tokens_seen": 1541120,
      "step": 956
    },
    {
      "epoch": 0.1635617843103743,
      "grad_norm": 1.5545284748077393,
      "learning_rate": 2.810904738248549e-07,
      "loss": 1.2662,
      "num_input_tokens_seen": 1542400,
      "step": 957
    },
    {
      "epoch": 0.16373269526576653,
      "grad_norm": 1.340417742729187,
      "learning_rate": 2.681916759252917e-07,
      "loss": 1.1074,
      "num_input_tokens_seen": 1543808,
      "step": 958
    },
    {
      "epoch": 0.16390360622115877,
      "grad_norm": 1.5521413087844849,
      "learning_rate": 2.555942273052753e-07,
      "loss": 1.9079,
      "num_input_tokens_seen": 1545088,
      "step": 959
    },
    {
      "epoch": 0.16407451717655103,
      "grad_norm": 1.5113729238510132,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 1.2645,
      "num_input_tokens_seen": 1546112,
      "step": 960
    },
    {
      "epoch": 0.16424542813194326,
      "grad_norm": 1.1166304349899292,
      "learning_rate": 2.3130398821391007e-07,
      "loss": 1.0971,
      "num_input_tokens_seen": 1548160,
      "step": 961
    },
    {
      "epoch": 0.1644163390873355,
      "grad_norm": 1.140135407447815,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 1.2413,
      "num_input_tokens_seen": 1549824,
      "step": 962
    },
    {
      "epoch": 0.16458725004272773,
      "grad_norm": 1.2093397378921509,
      "learning_rate": 2.0822094042274032e-07,
      "loss": 1.0885,
      "num_input_tokens_seen": 1551360,
      "step": 963
    },
    {
      "epoch": 0.16475816099812,
      "grad_norm": 1.2732384204864502,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 1.0467,
      "num_input_tokens_seen": 1552512,
      "step": 964
    },
    {
      "epoch": 0.16492907195351222,
      "grad_norm": 1.1467697620391846,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 1.5332,
      "num_input_tokens_seen": 1554560,
      "step": 965
    },
    {
      "epoch": 0.16509998290890446,
      "grad_norm": 1.736933946609497,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 1.2919,
      "num_input_tokens_seen": 1555584,
      "step": 966
    },
    {
      "epoch": 0.1652708938642967,
      "grad_norm": 1.532515287399292,
      "learning_rate": 1.6568085999008888e-07,
      "loss": 1.2153,
      "num_input_tokens_seen": 1556992,
      "step": 967
    },
    {
      "epoch": 0.16544180481968895,
      "grad_norm": 1.3020117282867432,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 1.4656,
      "num_input_tokens_seen": 1558400,
      "step": 968
    },
    {
      "epoch": 0.1656127157750812,
      "grad_norm": 0.8880061507225037,
      "learning_rate": 1.4622590069211516e-07,
      "loss": 1.8063,
      "num_input_tokens_seen": 1561984,
      "step": 969
    },
    {
      "epoch": 0.16578362673047342,
      "grad_norm": 1.6443227529525757,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 1.6346,
      "num_input_tokens_seen": 1563136,
      "step": 970
    },
    {
      "epoch": 0.16595453768586566,
      "grad_norm": 1.7352179288864136,
      "learning_rate": 1.2798227928029482e-07,
      "loss": 1.6639,
      "num_input_tokens_seen": 1564416,
      "step": 971
    },
    {
      "epoch": 0.16612544864125792,
      "grad_norm": 1.2134534120559692,
      "learning_rate": 1.193150004542204e-07,
      "loss": 1.2451,
      "num_input_tokens_seen": 1566336,
      "step": 972
    },
    {
      "epoch": 0.16629635959665015,
      "grad_norm": 1.4364734888076782,
      "learning_rate": 1.109508849230001e-07,
      "loss": 1.2667,
      "num_input_tokens_seen": 1567744,
      "step": 973
    },
    {
      "epoch": 0.16646727055204238,
      "grad_norm": 1.1439868211746216,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 1.2834,
      "num_input_tokens_seen": 1568896,
      "step": 974
    },
    {
      "epoch": 0.16663818150743462,
      "grad_norm": 1.6275359392166138,
      "learning_rate": 9.513254770636137e-08,
      "loss": 1.2757,
      "num_input_tokens_seen": 1569920,
      "step": 975
    },
    {
      "epoch": 0.16680909246282688,
      "grad_norm": 1.0780503749847412,
      "learning_rate": 8.767851876239074e-08,
      "loss": 1.2561,
      "num_input_tokens_seen": 1571712,
      "step": 976
    },
    {
      "epoch": 0.16698000341821911,
      "grad_norm": 1.2640010118484497,
      "learning_rate": 8.052803859382174e-08,
      "loss": 1.3559,
      "num_input_tokens_seen": 1573120,
      "step": 977
    },
    {
      "epoch": 0.16715091437361135,
      "grad_norm": 1.6204155683517456,
      "learning_rate": 7.368119432699383e-08,
      "loss": 1.327,
      "num_input_tokens_seen": 1575040,
      "step": 978
    },
    {
      "epoch": 0.16732182532900358,
      "grad_norm": 1.3492166996002197,
      "learning_rate": 6.71380693885476e-08,
      "loss": 1.3955,
      "num_input_tokens_seen": 1576064,
      "step": 979
    },
    {
      "epoch": 0.16749273628439584,
      "grad_norm": 1.0843214988708496,
      "learning_rate": 6.089874350439506e-08,
      "loss": 1.3164,
      "num_input_tokens_seen": 1578112,
      "step": 980
    },
    {
      "epoch": 0.16766364723978808,
      "grad_norm": 1.547484278678894,
      "learning_rate": 5.496329269875089e-08,
      "loss": 1.3673,
      "num_input_tokens_seen": 1579648,
      "step": 981
    },
    {
      "epoch": 0.1678345581951803,
      "grad_norm": 1.3837875127792358,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 1.0416,
      "num_input_tokens_seen": 1580672,
      "step": 982
    },
    {
      "epoch": 0.16800546915057255,
      "grad_norm": 1.1996632814407349,
      "learning_rate": 4.400430190586724e-08,
      "loss": 1.3365,
      "num_input_tokens_seen": 1584512,
      "step": 983
    },
    {
      "epoch": 0.16817638010596478,
      "grad_norm": 1.1210339069366455,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 1.4499,
      "num_input_tokens_seen": 1586048,
      "step": 984
    },
    {
      "epoch": 0.16834729106135704,
      "grad_norm": 1.235082745552063,
      "learning_rate": 3.426163113565417e-08,
      "loss": 1.2893,
      "num_input_tokens_seen": 1587456,
      "step": 985
    },
    {
      "epoch": 0.16851820201674927,
      "grad_norm": 1.0159136056900024,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 1.3162,
      "num_input_tokens_seen": 1589888,
      "step": 986
    },
    {
      "epoch": 0.1686891129721415,
      "grad_norm": 1.5450338125228882,
      "learning_rate": 2.5735755232134118e-08,
      "loss": 1.3872,
      "num_input_tokens_seen": 1591040,
      "step": 987
    },
    {
      "epoch": 0.16886002392753374,
      "grad_norm": 1.2484074831008911,
      "learning_rate": 2.192924752854042e-08,
      "loss": 1.203,
      "num_input_tokens_seen": 1592704,
      "step": 988
    },
    {
      "epoch": 0.169030934882926,
      "grad_norm": 1.500707983970642,
      "learning_rate": 1.842708973447127e-08,
      "loss": 1.5383,
      "num_input_tokens_seen": 1593728,
      "step": 989
    },
    {
      "epoch": 0.16920184583831824,
      "grad_norm": 1.1257606744766235,
      "learning_rate": 1.522932452260595e-08,
      "loss": 1.1761,
      "num_input_tokens_seen": 1595008,
      "step": 990
    },
    {
      "epoch": 0.16937275679371047,
      "grad_norm": 1.4380050897598267,
      "learning_rate": 1.233599085671e-08,
      "loss": 1.5427,
      "num_input_tokens_seen": 1596544,
      "step": 991
    },
    {
      "epoch": 0.1695436677491027,
      "grad_norm": 1.268401861190796,
      "learning_rate": 9.747123991141194e-09,
      "loss": 1.6293,
      "num_input_tokens_seen": 1598848,
      "step": 992
    },
    {
      "epoch": 0.16971457870449497,
      "grad_norm": 1.4034239053726196,
      "learning_rate": 7.462755470422078e-09,
      "loss": 1.1501,
      "num_input_tokens_seen": 1600640,
      "step": 993
    },
    {
      "epoch": 0.1698854896598872,
      "grad_norm": 0.9150272011756897,
      "learning_rate": 5.48291312886251e-09,
      "loss": 1.4968,
      "num_input_tokens_seen": 1603072,
      "step": 994
    },
    {
      "epoch": 0.17005640061527943,
      "grad_norm": 0.9055471420288086,
      "learning_rate": 3.807621090218261e-09,
      "loss": 0.6925,
      "num_input_tokens_seen": 1604992,
      "step": 995
    },
    {
      "epoch": 0.17022731157067167,
      "grad_norm": 1.2360801696777344,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 1.222,
      "num_input_tokens_seen": 1606912,
      "step": 996
    },
    {
      "epoch": 0.17039822252606393,
      "grad_norm": 1.3721458911895752,
      "learning_rate": 1.3707658621964215e-09,
      "loss": 1.8595,
      "num_input_tokens_seen": 1608448,
      "step": 997
    },
    {
      "epoch": 0.17056913348145616,
      "grad_norm": 1.5529732704162598,
      "learning_rate": 6.092323651313292e-10,
      "loss": 1.1192,
      "num_input_tokens_seen": 1609728,
      "step": 998
    },
    {
      "epoch": 0.1707400444368484,
      "grad_norm": 0.9298229217529297,
      "learning_rate": 1.5230855524017708e-10,
      "loss": 1.2113,
      "num_input_tokens_seen": 1611264,
      "step": 999
    },
    {
      "epoch": 0.17091095539224063,
      "grad_norm": 1.3026937246322632,
      "learning_rate": 0.0,
      "loss": 1.5034,
      "num_input_tokens_seen": 1613184,
      "step": 1000
    },
    {
      "epoch": 0.17091095539224063,
      "eval_loss": 1.2649905681610107,
      "eval_runtime": 82.6005,
      "eval_samples_per_second": 62.966,
      "eval_steps_per_second": 7.881,
      "num_input_tokens_seen": 1613184,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 1613184,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.267393155996058e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
