{
  "best_metric": 0.4567340314388275,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_8_bs_4096_seqlen/no_liger_tau_commonsense_qa/checkpoint-1000",
  "epoch": 0.8210180623973727,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008210180623973727,
      "grad_norm": 10.822319030761719,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.1061,
      "num_input_tokens_seen": 768,
      "step": 1
    },
    {
      "epoch": 0.0016420361247947454,
      "grad_norm": 9.114047050476074,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.7517,
      "num_input_tokens_seen": 1536,
      "step": 2
    },
    {
      "epoch": 0.0024630541871921183,
      "grad_norm": 8.099698066711426,
      "learning_rate": 1.5e-06,
      "loss": 1.7819,
      "num_input_tokens_seen": 2304,
      "step": 3
    },
    {
      "epoch": 0.003284072249589491,
      "grad_norm": 5.728412628173828,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7796,
      "num_input_tokens_seen": 3200,
      "step": 4
    },
    {
      "epoch": 0.004105090311986864,
      "grad_norm": 5.3433661460876465,
      "learning_rate": 2.5e-06,
      "loss": 1.5042,
      "num_input_tokens_seen": 3968,
      "step": 5
    },
    {
      "epoch": 0.0049261083743842365,
      "grad_norm": 8.85601806640625,
      "learning_rate": 3e-06,
      "loss": 1.4081,
      "num_input_tokens_seen": 4864,
      "step": 6
    },
    {
      "epoch": 0.005747126436781609,
      "grad_norm": 8.221281051635742,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.6181,
      "num_input_tokens_seen": 5632,
      "step": 7
    },
    {
      "epoch": 0.006568144499178982,
      "grad_norm": 7.26601505279541,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.9134,
      "num_input_tokens_seen": 6528,
      "step": 8
    },
    {
      "epoch": 0.007389162561576354,
      "grad_norm": 10.744441032409668,
      "learning_rate": 4.5e-06,
      "loss": 1.0169,
      "num_input_tokens_seen": 7552,
      "step": 9
    },
    {
      "epoch": 0.008210180623973728,
      "grad_norm": 7.150315284729004,
      "learning_rate": 5e-06,
      "loss": 1.2268,
      "num_input_tokens_seen": 8320,
      "step": 10
    },
    {
      "epoch": 0.0090311986863711,
      "grad_norm": 9.693757057189941,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.3247,
      "num_input_tokens_seen": 9344,
      "step": 11
    },
    {
      "epoch": 0.009852216748768473,
      "grad_norm": 8.788269996643066,
      "learning_rate": 6e-06,
      "loss": 2.0829,
      "num_input_tokens_seen": 10112,
      "step": 12
    },
    {
      "epoch": 0.010673234811165846,
      "grad_norm": 7.374996185302734,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.2831,
      "num_input_tokens_seen": 10880,
      "step": 13
    },
    {
      "epoch": 0.011494252873563218,
      "grad_norm": 7.897115230560303,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2446,
      "num_input_tokens_seen": 11648,
      "step": 14
    },
    {
      "epoch": 0.012315270935960592,
      "grad_norm": 6.07072114944458,
      "learning_rate": 7.5e-06,
      "loss": 1.4147,
      "num_input_tokens_seen": 12416,
      "step": 15
    },
    {
      "epoch": 0.013136288998357963,
      "grad_norm": 8.197989463806152,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.3713,
      "num_input_tokens_seen": 13184,
      "step": 16
    },
    {
      "epoch": 0.013957307060755337,
      "grad_norm": 6.881649971008301,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.9343,
      "num_input_tokens_seen": 14080,
      "step": 17
    },
    {
      "epoch": 0.014778325123152709,
      "grad_norm": 7.111661434173584,
      "learning_rate": 9e-06,
      "loss": 0.8426,
      "num_input_tokens_seen": 14848,
      "step": 18
    },
    {
      "epoch": 0.015599343185550082,
      "grad_norm": 7.684669494628906,
      "learning_rate": 9.5e-06,
      "loss": 1.4406,
      "num_input_tokens_seen": 15616,
      "step": 19
    },
    {
      "epoch": 0.016420361247947456,
      "grad_norm": 10.715392112731934,
      "learning_rate": 1e-05,
      "loss": 2.409,
      "num_input_tokens_seen": 16512,
      "step": 20
    },
    {
      "epoch": 0.017241379310344827,
      "grad_norm": 9.289017677307129,
      "learning_rate": 1.05e-05,
      "loss": 2.4144,
      "num_input_tokens_seen": 17280,
      "step": 21
    },
    {
      "epoch": 0.0180623973727422,
      "grad_norm": 10.380221366882324,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.8088,
      "num_input_tokens_seen": 18048,
      "step": 22
    },
    {
      "epoch": 0.018883415435139574,
      "grad_norm": 9.767763137817383,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 2.023,
      "num_input_tokens_seen": 18816,
      "step": 23
    },
    {
      "epoch": 0.019704433497536946,
      "grad_norm": 4.987578392028809,
      "learning_rate": 1.2e-05,
      "loss": 0.4965,
      "num_input_tokens_seen": 19584,
      "step": 24
    },
    {
      "epoch": 0.020525451559934318,
      "grad_norm": 7.541545391082764,
      "learning_rate": 1.25e-05,
      "loss": 0.8776,
      "num_input_tokens_seen": 20352,
      "step": 25
    },
    {
      "epoch": 0.021346469622331693,
      "grad_norm": 9.69862174987793,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.922,
      "num_input_tokens_seen": 21248,
      "step": 26
    },
    {
      "epoch": 0.022167487684729065,
      "grad_norm": 6.6612935066223145,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.1196,
      "num_input_tokens_seen": 22016,
      "step": 27
    },
    {
      "epoch": 0.022988505747126436,
      "grad_norm": 5.5847368240356445,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.4543,
      "num_input_tokens_seen": 22784,
      "step": 28
    },
    {
      "epoch": 0.023809523809523808,
      "grad_norm": 6.979003429412842,
      "learning_rate": 1.45e-05,
      "loss": 1.1538,
      "num_input_tokens_seen": 23680,
      "step": 29
    },
    {
      "epoch": 0.024630541871921183,
      "grad_norm": 8.853029251098633,
      "learning_rate": 1.5e-05,
      "loss": 1.6087,
      "num_input_tokens_seen": 24448,
      "step": 30
    },
    {
      "epoch": 0.025451559934318555,
      "grad_norm": 12.513370513916016,
      "learning_rate": 1.55e-05,
      "loss": 1.5171,
      "num_input_tokens_seen": 25216,
      "step": 31
    },
    {
      "epoch": 0.026272577996715927,
      "grad_norm": 12.306022644042969,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.8223,
      "num_input_tokens_seen": 25984,
      "step": 32
    },
    {
      "epoch": 0.027093596059113302,
      "grad_norm": 8.845222473144531,
      "learning_rate": 1.65e-05,
      "loss": 1.8104,
      "num_input_tokens_seen": 26752,
      "step": 33
    },
    {
      "epoch": 0.027914614121510674,
      "grad_norm": 12.90123462677002,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.3623,
      "num_input_tokens_seen": 27776,
      "step": 34
    },
    {
      "epoch": 0.028735632183908046,
      "grad_norm": 6.1520490646362305,
      "learning_rate": 1.75e-05,
      "loss": 0.9816,
      "num_input_tokens_seen": 28544,
      "step": 35
    },
    {
      "epoch": 0.029556650246305417,
      "grad_norm": 6.0132527351379395,
      "learning_rate": 1.8e-05,
      "loss": 0.6837,
      "num_input_tokens_seen": 29312,
      "step": 36
    },
    {
      "epoch": 0.030377668308702793,
      "grad_norm": 11.957756042480469,
      "learning_rate": 1.85e-05,
      "loss": 1.4721,
      "num_input_tokens_seen": 30080,
      "step": 37
    },
    {
      "epoch": 0.031198686371100164,
      "grad_norm": 7.99732780456543,
      "learning_rate": 1.9e-05,
      "loss": 1.7478,
      "num_input_tokens_seen": 30848,
      "step": 38
    },
    {
      "epoch": 0.03201970443349754,
      "grad_norm": 4.332387447357178,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.3688,
      "num_input_tokens_seen": 31744,
      "step": 39
    },
    {
      "epoch": 0.03284072249589491,
      "grad_norm": 7.430484294891357,
      "learning_rate": 2e-05,
      "loss": 1.2084,
      "num_input_tokens_seen": 32768,
      "step": 40
    },
    {
      "epoch": 0.03366174055829228,
      "grad_norm": 6.818962097167969,
      "learning_rate": 2.05e-05,
      "loss": 0.4385,
      "num_input_tokens_seen": 33536,
      "step": 41
    },
    {
      "epoch": 0.034482758620689655,
      "grad_norm": 8.218958854675293,
      "learning_rate": 2.1e-05,
      "loss": 0.7972,
      "num_input_tokens_seen": 34304,
      "step": 42
    },
    {
      "epoch": 0.035303776683087026,
      "grad_norm": 6.077156066894531,
      "learning_rate": 2.15e-05,
      "loss": 0.8509,
      "num_input_tokens_seen": 35200,
      "step": 43
    },
    {
      "epoch": 0.0361247947454844,
      "grad_norm": 5.018527984619141,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4812,
      "num_input_tokens_seen": 35968,
      "step": 44
    },
    {
      "epoch": 0.03694581280788178,
      "grad_norm": 1.7204196453094482,
      "learning_rate": 2.25e-05,
      "loss": 0.103,
      "num_input_tokens_seen": 36736,
      "step": 45
    },
    {
      "epoch": 0.03776683087027915,
      "grad_norm": 8.359519958496094,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8966,
      "num_input_tokens_seen": 37504,
      "step": 46
    },
    {
      "epoch": 0.03858784893267652,
      "grad_norm": 1.7597553730010986,
      "learning_rate": 2.35e-05,
      "loss": 0.0984,
      "num_input_tokens_seen": 38272,
      "step": 47
    },
    {
      "epoch": 0.03940886699507389,
      "grad_norm": 4.8352203369140625,
      "learning_rate": 2.4e-05,
      "loss": 0.3419,
      "num_input_tokens_seen": 39040,
      "step": 48
    },
    {
      "epoch": 0.040229885057471264,
      "grad_norm": 5.030703067779541,
      "learning_rate": 2.45e-05,
      "loss": 0.5964,
      "num_input_tokens_seen": 39808,
      "step": 49
    },
    {
      "epoch": 0.041050903119868636,
      "grad_norm": 4.520214080810547,
      "learning_rate": 2.5e-05,
      "loss": 0.622,
      "num_input_tokens_seen": 40704,
      "step": 50
    },
    {
      "epoch": 0.04187192118226601,
      "grad_norm": 8.656591415405273,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.3473,
      "num_input_tokens_seen": 41472,
      "step": 51
    },
    {
      "epoch": 0.042692939244663386,
      "grad_norm": 5.422724723815918,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.5289,
      "num_input_tokens_seen": 42240,
      "step": 52
    },
    {
      "epoch": 0.04351395730706076,
      "grad_norm": 5.838367938995361,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.6436,
      "num_input_tokens_seen": 43008,
      "step": 53
    },
    {
      "epoch": 0.04433497536945813,
      "grad_norm": 12.408374786376953,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.875,
      "num_input_tokens_seen": 43776,
      "step": 54
    },
    {
      "epoch": 0.0451559934318555,
      "grad_norm": 13.085671424865723,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.531,
      "num_input_tokens_seen": 44544,
      "step": 55
    },
    {
      "epoch": 0.04597701149425287,
      "grad_norm": 6.3864545822143555,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.8026,
      "num_input_tokens_seen": 45312,
      "step": 56
    },
    {
      "epoch": 0.046798029556650245,
      "grad_norm": 8.395501136779785,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.6581,
      "num_input_tokens_seen": 46080,
      "step": 57
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.520933747291565,
      "learning_rate": 2.9e-05,
      "loss": 0.0699,
      "num_input_tokens_seen": 46848,
      "step": 58
    },
    {
      "epoch": 0.048440065681444995,
      "grad_norm": 6.808782577514648,
      "learning_rate": 2.95e-05,
      "loss": 0.6582,
      "num_input_tokens_seen": 47616,
      "step": 59
    },
    {
      "epoch": 0.04926108374384237,
      "grad_norm": 9.546724319458008,
      "learning_rate": 3e-05,
      "loss": 1.3404,
      "num_input_tokens_seen": 48512,
      "step": 60
    },
    {
      "epoch": 0.05008210180623974,
      "grad_norm": 11.364409446716309,
      "learning_rate": 3.05e-05,
      "loss": 1.0625,
      "num_input_tokens_seen": 49280,
      "step": 61
    },
    {
      "epoch": 0.05090311986863711,
      "grad_norm": 0.9851962924003601,
      "learning_rate": 3.1e-05,
      "loss": 0.0412,
      "num_input_tokens_seen": 50048,
      "step": 62
    },
    {
      "epoch": 0.05172413793103448,
      "grad_norm": 6.451358318328857,
      "learning_rate": 3.15e-05,
      "loss": 1.1076,
      "num_input_tokens_seen": 50816,
      "step": 63
    },
    {
      "epoch": 0.052545155993431854,
      "grad_norm": 8.158004760742188,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.405,
      "num_input_tokens_seen": 51840,
      "step": 64
    },
    {
      "epoch": 0.053366174055829226,
      "grad_norm": 6.331491470336914,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.2912,
      "num_input_tokens_seen": 52608,
      "step": 65
    },
    {
      "epoch": 0.054187192118226604,
      "grad_norm": 7.537881851196289,
      "learning_rate": 3.3e-05,
      "loss": 0.4659,
      "num_input_tokens_seen": 53376,
      "step": 66
    },
    {
      "epoch": 0.055008210180623976,
      "grad_norm": 6.758022308349609,
      "learning_rate": 3.35e-05,
      "loss": 0.5471,
      "num_input_tokens_seen": 54144,
      "step": 67
    },
    {
      "epoch": 0.05582922824302135,
      "grad_norm": 9.748329162597656,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.8415,
      "num_input_tokens_seen": 54912,
      "step": 68
    },
    {
      "epoch": 0.05665024630541872,
      "grad_norm": 7.867126941680908,
      "learning_rate": 3.45e-05,
      "loss": 0.9923,
      "num_input_tokens_seen": 55680,
      "step": 69
    },
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 13.135683059692383,
      "learning_rate": 3.5e-05,
      "loss": 0.8691,
      "num_input_tokens_seen": 56448,
      "step": 70
    },
    {
      "epoch": 0.05829228243021346,
      "grad_norm": 4.93663215637207,
      "learning_rate": 3.55e-05,
      "loss": 0.4961,
      "num_input_tokens_seen": 57216,
      "step": 71
    },
    {
      "epoch": 0.059113300492610835,
      "grad_norm": 7.967620849609375,
      "learning_rate": 3.6e-05,
      "loss": 0.718,
      "num_input_tokens_seen": 57984,
      "step": 72
    },
    {
      "epoch": 0.05993431855500821,
      "grad_norm": 5.567229270935059,
      "learning_rate": 3.65e-05,
      "loss": 0.4411,
      "num_input_tokens_seen": 58752,
      "step": 73
    },
    {
      "epoch": 0.060755336617405585,
      "grad_norm": 12.82087516784668,
      "learning_rate": 3.7e-05,
      "loss": 1.5394,
      "num_input_tokens_seen": 59520,
      "step": 74
    },
    {
      "epoch": 0.06157635467980296,
      "grad_norm": 9.931857109069824,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.7587,
      "num_input_tokens_seen": 60416,
      "step": 75
    },
    {
      "epoch": 0.06239737274220033,
      "grad_norm": 8.262918472290039,
      "learning_rate": 3.8e-05,
      "loss": 0.8253,
      "num_input_tokens_seen": 61184,
      "step": 76
    },
    {
      "epoch": 0.06321839080459771,
      "grad_norm": 10.414100646972656,
      "learning_rate": 3.85e-05,
      "loss": 1.5461,
      "num_input_tokens_seen": 61952,
      "step": 77
    },
    {
      "epoch": 0.06403940886699508,
      "grad_norm": 8.980581283569336,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.169,
      "num_input_tokens_seen": 62720,
      "step": 78
    },
    {
      "epoch": 0.06486042692939245,
      "grad_norm": 7.964399814605713,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.9413,
      "num_input_tokens_seen": 63488,
      "step": 79
    },
    {
      "epoch": 0.06568144499178982,
      "grad_norm": 8.798985481262207,
      "learning_rate": 4e-05,
      "loss": 1.0192,
      "num_input_tokens_seen": 64256,
      "step": 80
    },
    {
      "epoch": 0.0665024630541872,
      "grad_norm": 4.780389308929443,
      "learning_rate": 4.05e-05,
      "loss": 0.2608,
      "num_input_tokens_seen": 65280,
      "step": 81
    },
    {
      "epoch": 0.06732348111658457,
      "grad_norm": 4.60716438293457,
      "learning_rate": 4.1e-05,
      "loss": 0.3557,
      "num_input_tokens_seen": 66176,
      "step": 82
    },
    {
      "epoch": 0.06814449917898194,
      "grad_norm": 4.336494445800781,
      "learning_rate": 4.15e-05,
      "loss": 0.7472,
      "num_input_tokens_seen": 66944,
      "step": 83
    },
    {
      "epoch": 0.06896551724137931,
      "grad_norm": 6.60369873046875,
      "learning_rate": 4.2e-05,
      "loss": 0.5479,
      "num_input_tokens_seen": 67712,
      "step": 84
    },
    {
      "epoch": 0.06978653530377668,
      "grad_norm": 4.743605136871338,
      "learning_rate": 4.25e-05,
      "loss": 0.4361,
      "num_input_tokens_seen": 68480,
      "step": 85
    },
    {
      "epoch": 0.07060755336617405,
      "grad_norm": 10.691305160522461,
      "learning_rate": 4.3e-05,
      "loss": 1.3109,
      "num_input_tokens_seen": 69376,
      "step": 86
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 3.6169593334198,
      "learning_rate": 4.35e-05,
      "loss": 0.209,
      "num_input_tokens_seen": 70144,
      "step": 87
    },
    {
      "epoch": 0.0722495894909688,
      "grad_norm": 5.430459022521973,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6303,
      "num_input_tokens_seen": 70912,
      "step": 88
    },
    {
      "epoch": 0.07307060755336617,
      "grad_norm": 10.22894287109375,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.0029,
      "num_input_tokens_seen": 71808,
      "step": 89
    },
    {
      "epoch": 0.07389162561576355,
      "grad_norm": 13.595240592956543,
      "learning_rate": 4.5e-05,
      "loss": 0.7511,
      "num_input_tokens_seen": 72576,
      "step": 90
    },
    {
      "epoch": 0.07471264367816093,
      "grad_norm": 7.449072360992432,
      "learning_rate": 4.55e-05,
      "loss": 0.4835,
      "num_input_tokens_seen": 73472,
      "step": 91
    },
    {
      "epoch": 0.0755336617405583,
      "grad_norm": 9.72739315032959,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6801,
      "num_input_tokens_seen": 74240,
      "step": 92
    },
    {
      "epoch": 0.07635467980295567,
      "grad_norm": 11.095735549926758,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.7562,
      "num_input_tokens_seen": 75008,
      "step": 93
    },
    {
      "epoch": 0.07717569786535304,
      "grad_norm": 10.105284690856934,
      "learning_rate": 4.7e-05,
      "loss": 0.8137,
      "num_input_tokens_seen": 75776,
      "step": 94
    },
    {
      "epoch": 0.07799671592775041,
      "grad_norm": 13.135171890258789,
      "learning_rate": 4.75e-05,
      "loss": 1.8624,
      "num_input_tokens_seen": 76544,
      "step": 95
    },
    {
      "epoch": 0.07881773399014778,
      "grad_norm": 11.429040908813477,
      "learning_rate": 4.8e-05,
      "loss": 1.0485,
      "num_input_tokens_seen": 77312,
      "step": 96
    },
    {
      "epoch": 0.07963875205254516,
      "grad_norm": 9.161680221557617,
      "learning_rate": 4.85e-05,
      "loss": 1.0642,
      "num_input_tokens_seen": 78080,
      "step": 97
    },
    {
      "epoch": 0.08045977011494253,
      "grad_norm": 8.031103134155273,
      "learning_rate": 4.9e-05,
      "loss": 0.541,
      "num_input_tokens_seen": 78976,
      "step": 98
    },
    {
      "epoch": 0.0812807881773399,
      "grad_norm": 6.3282551765441895,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.8386,
      "num_input_tokens_seen": 79744,
      "step": 99
    },
    {
      "epoch": 0.08210180623973727,
      "grad_norm": 5.549055576324463,
      "learning_rate": 5e-05,
      "loss": 0.5806,
      "num_input_tokens_seen": 80512,
      "step": 100
    },
    {
      "epoch": 0.08292282430213464,
      "grad_norm": 6.795893669128418,
      "learning_rate": 4.999984769144476e-05,
      "loss": 0.4655,
      "num_input_tokens_seen": 81280,
      "step": 101
    },
    {
      "epoch": 0.08374384236453201,
      "grad_norm": 6.284181118011475,
      "learning_rate": 4.999939076763487e-05,
      "loss": 0.6591,
      "num_input_tokens_seen": 82176,
      "step": 102
    },
    {
      "epoch": 0.08456486042692939,
      "grad_norm": 5.915673732757568,
      "learning_rate": 4.999862923413781e-05,
      "loss": 0.3176,
      "num_input_tokens_seen": 83072,
      "step": 103
    },
    {
      "epoch": 0.08538587848932677,
      "grad_norm": 6.610632419586182,
      "learning_rate": 4.999756310023261e-05,
      "loss": 1.1021,
      "num_input_tokens_seen": 83840,
      "step": 104
    },
    {
      "epoch": 0.08620689655172414,
      "grad_norm": 8.006865501403809,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 0.6319,
      "num_input_tokens_seen": 84736,
      "step": 105
    },
    {
      "epoch": 0.08702791461412152,
      "grad_norm": 6.771460056304932,
      "learning_rate": 4.999451708687114e-05,
      "loss": 0.6894,
      "num_input_tokens_seen": 85504,
      "step": 106
    },
    {
      "epoch": 0.08784893267651889,
      "grad_norm": 4.888860702514648,
      "learning_rate": 4.999253724452958e-05,
      "loss": 0.422,
      "num_input_tokens_seen": 86272,
      "step": 107
    },
    {
      "epoch": 0.08866995073891626,
      "grad_norm": 6.258172512054443,
      "learning_rate": 4.999025287600886e-05,
      "loss": 0.8396,
      "num_input_tokens_seen": 87040,
      "step": 108
    },
    {
      "epoch": 0.08949096880131363,
      "grad_norm": 6.048257350921631,
      "learning_rate": 4.998766400914329e-05,
      "loss": 0.6391,
      "num_input_tokens_seen": 87808,
      "step": 109
    },
    {
      "epoch": 0.090311986863711,
      "grad_norm": 11.41028118133545,
      "learning_rate": 4.99847706754774e-05,
      "loss": 0.6604,
      "num_input_tokens_seen": 88704,
      "step": 110
    },
    {
      "epoch": 0.09113300492610837,
      "grad_norm": 6.564143657684326,
      "learning_rate": 4.998157291026553e-05,
      "loss": 0.4029,
      "num_input_tokens_seen": 89600,
      "step": 111
    },
    {
      "epoch": 0.09195402298850575,
      "grad_norm": 7.358954906463623,
      "learning_rate": 4.997807075247146e-05,
      "loss": 1.0027,
      "num_input_tokens_seen": 90368,
      "step": 112
    },
    {
      "epoch": 0.09277504105090312,
      "grad_norm": 8.255613327026367,
      "learning_rate": 4.997426424476787e-05,
      "loss": 1.1499,
      "num_input_tokens_seen": 91264,
      "step": 113
    },
    {
      "epoch": 0.09359605911330049,
      "grad_norm": 7.939909934997559,
      "learning_rate": 4.997015343353585e-05,
      "loss": 0.362,
      "num_input_tokens_seen": 92032,
      "step": 114
    },
    {
      "epoch": 0.09441707717569786,
      "grad_norm": 5.854660987854004,
      "learning_rate": 4.996573836886435e-05,
      "loss": 0.3012,
      "num_input_tokens_seen": 92800,
      "step": 115
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 10.618416786193848,
      "learning_rate": 4.996101910454953e-05,
      "loss": 0.8717,
      "num_input_tokens_seen": 93568,
      "step": 116
    },
    {
      "epoch": 0.0960591133004926,
      "grad_norm": 2.1371302604675293,
      "learning_rate": 4.995599569809414e-05,
      "loss": 0.0848,
      "num_input_tokens_seen": 94336,
      "step": 117
    },
    {
      "epoch": 0.09688013136288999,
      "grad_norm": 9.141465187072754,
      "learning_rate": 4.995066821070679e-05,
      "loss": 0.8636,
      "num_input_tokens_seen": 95232,
      "step": 118
    },
    {
      "epoch": 0.09770114942528736,
      "grad_norm": 9.411623001098633,
      "learning_rate": 4.994503670730125e-05,
      "loss": 0.6836,
      "num_input_tokens_seen": 96000,
      "step": 119
    },
    {
      "epoch": 0.09852216748768473,
      "grad_norm": 11.481467247009277,
      "learning_rate": 4.993910125649561e-05,
      "loss": 0.5871,
      "num_input_tokens_seen": 96768,
      "step": 120
    },
    {
      "epoch": 0.0993431855500821,
      "grad_norm": 7.404245376586914,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 0.8232,
      "num_input_tokens_seen": 97536,
      "step": 121
    },
    {
      "epoch": 0.10016420361247948,
      "grad_norm": 7.404763698577881,
      "learning_rate": 4.992631880567301e-05,
      "loss": 0.657,
      "num_input_tokens_seen": 98432,
      "step": 122
    },
    {
      "epoch": 0.10098522167487685,
      "grad_norm": 10.72346305847168,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.5971,
      "num_input_tokens_seen": 99200,
      "step": 123
    },
    {
      "epoch": 0.10180623973727422,
      "grad_norm": 8.910880088806152,
      "learning_rate": 4.991232148123761e-05,
      "loss": 1.1054,
      "num_input_tokens_seen": 100096,
      "step": 124
    },
    {
      "epoch": 0.10262725779967159,
      "grad_norm": 12.900805473327637,
      "learning_rate": 4.990486745229364e-05,
      "loss": 1.2864,
      "num_input_tokens_seen": 100864,
      "step": 125
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 8.466120719909668,
      "learning_rate": 4.989710996539926e-05,
      "loss": 0.5613,
      "num_input_tokens_seen": 101632,
      "step": 126
    },
    {
      "epoch": 0.10426929392446634,
      "grad_norm": 12.040327072143555,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 1.1373,
      "num_input_tokens_seen": 102400,
      "step": 127
    },
    {
      "epoch": 0.10509031198686371,
      "grad_norm": 12.899679183959961,
      "learning_rate": 4.988068499954578e-05,
      "loss": 1.8508,
      "num_input_tokens_seen": 103168,
      "step": 128
    },
    {
      "epoch": 0.10591133004926108,
      "grad_norm": 7.428847312927246,
      "learning_rate": 4.987201772071971e-05,
      "loss": 0.3133,
      "num_input_tokens_seen": 103936,
      "step": 129
    },
    {
      "epoch": 0.10673234811165845,
      "grad_norm": 11.36488151550293,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 1.1559,
      "num_input_tokens_seen": 104704,
      "step": 130
    },
    {
      "epoch": 0.10755336617405582,
      "grad_norm": 18.2855167388916,
      "learning_rate": 4.985377409930789e-05,
      "loss": 1.3423,
      "num_input_tokens_seen": 105472,
      "step": 131
    },
    {
      "epoch": 0.10837438423645321,
      "grad_norm": 8.382363319396973,
      "learning_rate": 4.984419797901491e-05,
      "loss": 0.852,
      "num_input_tokens_seen": 106240,
      "step": 132
    },
    {
      "epoch": 0.10919540229885058,
      "grad_norm": 6.405787467956543,
      "learning_rate": 4.983431914000991e-05,
      "loss": 0.3298,
      "num_input_tokens_seen": 107008,
      "step": 133
    },
    {
      "epoch": 0.11001642036124795,
      "grad_norm": 7.177333354949951,
      "learning_rate": 4.982413770266342e-05,
      "loss": 0.5246,
      "num_input_tokens_seen": 107776,
      "step": 134
    },
    {
      "epoch": 0.11083743842364532,
      "grad_norm": 7.4557671546936035,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 0.6182,
      "num_input_tokens_seen": 108544,
      "step": 135
    },
    {
      "epoch": 0.1116584564860427,
      "grad_norm": 9.025479316711426,
      "learning_rate": 4.980286753286195e-05,
      "loss": 0.7354,
      "num_input_tokens_seen": 109312,
      "step": 136
    },
    {
      "epoch": 0.11247947454844007,
      "grad_norm": 11.325533866882324,
      "learning_rate": 4.979177905957726e-05,
      "loss": 0.8845,
      "num_input_tokens_seen": 110208,
      "step": 137
    },
    {
      "epoch": 0.11330049261083744,
      "grad_norm": 14.761451721191406,
      "learning_rate": 4.978038850628854e-05,
      "loss": 1.9276,
      "num_input_tokens_seen": 111104,
      "step": 138
    },
    {
      "epoch": 0.11412151067323481,
      "grad_norm": 7.662156581878662,
      "learning_rate": 4.976869601178609e-05,
      "loss": 0.815,
      "num_input_tokens_seen": 111872,
      "step": 139
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 10.667815208435059,
      "learning_rate": 4.975670171853926e-05,
      "loss": 1.2669,
      "num_input_tokens_seen": 112640,
      "step": 140
    },
    {
      "epoch": 0.11576354679802955,
      "grad_norm": 4.25888729095459,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 0.4983,
      "num_input_tokens_seen": 113408,
      "step": 141
    },
    {
      "epoch": 0.11658456486042693,
      "grad_norm": 6.203714847564697,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 0.8453,
      "num_input_tokens_seen": 114176,
      "step": 142
    },
    {
      "epoch": 0.1174055829228243,
      "grad_norm": 4.0528645515441895,
      "learning_rate": 4.971890952617515e-05,
      "loss": 0.3241,
      "num_input_tokens_seen": 114944,
      "step": 143
    },
    {
      "epoch": 0.11822660098522167,
      "grad_norm": 8.8125638961792,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 1.0283,
      "num_input_tokens_seen": 115712,
      "step": 144
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 8.778026580810547,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.9384,
      "num_input_tokens_seen": 116480,
      "step": 145
    },
    {
      "epoch": 0.11986863711001643,
      "grad_norm": 8.496299743652344,
      "learning_rate": 4.96784066268247e-05,
      "loss": 0.6327,
      "num_input_tokens_seen": 117248,
      "step": 146
    },
    {
      "epoch": 0.1206896551724138,
      "grad_norm": 12.59821891784668,
      "learning_rate": 4.966430404017424e-05,
      "loss": 0.8701,
      "num_input_tokens_seen": 118016,
      "step": 147
    },
    {
      "epoch": 0.12151067323481117,
      "grad_norm": 7.306458473205566,
      "learning_rate": 4.964990092676263e-05,
      "loss": 0.5848,
      "num_input_tokens_seen": 118784,
      "step": 148
    },
    {
      "epoch": 0.12233169129720854,
      "grad_norm": 6.688453197479248,
      "learning_rate": 4.963519746208726e-05,
      "loss": 1.0059,
      "num_input_tokens_seen": 119552,
      "step": 149
    },
    {
      "epoch": 0.12315270935960591,
      "grad_norm": 6.196354389190674,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.8894,
      "num_input_tokens_seen": 120448,
      "step": 150
    },
    {
      "epoch": 0.12397372742200329,
      "grad_norm": 7.516632556915283,
      "learning_rate": 4.960489019923105e-05,
      "loss": 0.8556,
      "num_input_tokens_seen": 121472,
      "step": 151
    },
    {
      "epoch": 0.12479474548440066,
      "grad_norm": 8.181785583496094,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 0.8765,
      "num_input_tokens_seen": 122240,
      "step": 152
    },
    {
      "epoch": 0.12561576354679804,
      "grad_norm": 5.469689846038818,
      "learning_rate": 4.957338372873886e-05,
      "loss": 0.4047,
      "num_input_tokens_seen": 123008,
      "step": 153
    },
    {
      "epoch": 0.12643678160919541,
      "grad_norm": 5.959813117980957,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.888,
      "num_input_tokens_seen": 123776,
      "step": 154
    },
    {
      "epoch": 0.1272577996715928,
      "grad_norm": 8.204755783081055,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 1.0108,
      "num_input_tokens_seen": 124544,
      "step": 155
    },
    {
      "epoch": 0.12807881773399016,
      "grad_norm": 10.95719051361084,
      "learning_rate": 4.952387888372979e-05,
      "loss": 1.3284,
      "num_input_tokens_seen": 125312,
      "step": 156
    },
    {
      "epoch": 0.12889983579638753,
      "grad_norm": 8.205195426940918,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 0.7292,
      "num_input_tokens_seen": 126080,
      "step": 157
    },
    {
      "epoch": 0.1297208538587849,
      "grad_norm": 9.226615905761719,
      "learning_rate": 4.94893812399836e-05,
      "loss": 1.2268,
      "num_input_tokens_seen": 126848,
      "step": 158
    },
    {
      "epoch": 0.13054187192118227,
      "grad_norm": 7.732687950134277,
      "learning_rate": 4.947168471904213e-05,
      "loss": 0.6795,
      "num_input_tokens_seen": 127744,
      "step": 159
    },
    {
      "epoch": 0.13136288998357964,
      "grad_norm": 6.877814292907715,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 0.4999,
      "num_input_tokens_seen": 128512,
      "step": 160
    },
    {
      "epoch": 0.13218390804597702,
      "grad_norm": 5.782126426696777,
      "learning_rate": 4.94353973571524e-05,
      "loss": 0.8762,
      "num_input_tokens_seen": 129280,
      "step": 161
    },
    {
      "epoch": 0.1330049261083744,
      "grad_norm": 6.632837772369385,
      "learning_rate": 4.94168069583542e-05,
      "loss": 0.9143,
      "num_input_tokens_seen": 130176,
      "step": 162
    },
    {
      "epoch": 0.13382594417077176,
      "grad_norm": 5.621506214141846,
      "learning_rate": 4.939791904846869e-05,
      "loss": 0.4034,
      "num_input_tokens_seen": 130944,
      "step": 163
    },
    {
      "epoch": 0.13464696223316913,
      "grad_norm": 8.842842102050781,
      "learning_rate": 4.937873385763908e-05,
      "loss": 0.9833,
      "num_input_tokens_seen": 131712,
      "step": 164
    },
    {
      "epoch": 0.1354679802955665,
      "grad_norm": 6.52510929107666,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 0.5582,
      "num_input_tokens_seen": 132480,
      "step": 165
    },
    {
      "epoch": 0.13628899835796388,
      "grad_norm": 6.094015598297119,
      "learning_rate": 4.933947257182901e-05,
      "loss": 0.5432,
      "num_input_tokens_seen": 133248,
      "step": 166
    },
    {
      "epoch": 0.13711001642036125,
      "grad_norm": 6.642945766448975,
      "learning_rate": 4.931939695523492e-05,
      "loss": 0.7406,
      "num_input_tokens_seen": 134016,
      "step": 167
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 5.910409927368164,
      "learning_rate": 4.929902501446366e-05,
      "loss": 0.5359,
      "num_input_tokens_seen": 134912,
      "step": 168
    },
    {
      "epoch": 0.138752052545156,
      "grad_norm": 5.060255527496338,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 0.3525,
      "num_input_tokens_seen": 135680,
      "step": 169
    },
    {
      "epoch": 0.13957307060755336,
      "grad_norm": 9.139718055725098,
      "learning_rate": 4.925739315689991e-05,
      "loss": 0.5433,
      "num_input_tokens_seen": 136448,
      "step": 170
    },
    {
      "epoch": 0.14039408866995073,
      "grad_norm": 9.030200004577637,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 0.6554,
      "num_input_tokens_seen": 137216,
      "step": 171
    },
    {
      "epoch": 0.1412151067323481,
      "grad_norm": 10.263299942016602,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 0.6117,
      "num_input_tokens_seen": 138112,
      "step": 172
    },
    {
      "epoch": 0.14203612479474548,
      "grad_norm": 8.003140449523926,
      "learning_rate": 4.919272926204929e-05,
      "loss": 0.7438,
      "num_input_tokens_seen": 138880,
      "step": 173
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 12.316474914550781,
      "learning_rate": 4.917058471511149e-05,
      "loss": 0.9287,
      "num_input_tokens_seen": 139648,
      "step": 174
    },
    {
      "epoch": 0.14367816091954022,
      "grad_norm": 8.505125045776367,
      "learning_rate": 4.914814565722671e-05,
      "loss": 0.364,
      "num_input_tokens_seen": 140416,
      "step": 175
    },
    {
      "epoch": 0.1444991789819376,
      "grad_norm": 9.100848197937012,
      "learning_rate": 4.912541236180779e-05,
      "loss": 0.4573,
      "num_input_tokens_seen": 141312,
      "step": 176
    },
    {
      "epoch": 0.14532019704433496,
      "grad_norm": 5.612690448760986,
      "learning_rate": 4.910238510585276e-05,
      "loss": 0.6184,
      "num_input_tokens_seen": 142208,
      "step": 177
    },
    {
      "epoch": 0.14614121510673234,
      "grad_norm": 15.097236633300781,
      "learning_rate": 4.907906416994146e-05,
      "loss": 1.1773,
      "num_input_tokens_seen": 142976,
      "step": 178
    },
    {
      "epoch": 0.1469622331691297,
      "grad_norm": 9.492777824401855,
      "learning_rate": 4.905544983823214e-05,
      "loss": 1.1647,
      "num_input_tokens_seen": 143744,
      "step": 179
    },
    {
      "epoch": 0.1477832512315271,
      "grad_norm": 8.594952583312988,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 0.5554,
      "num_input_tokens_seen": 144640,
      "step": 180
    },
    {
      "epoch": 0.14860426929392448,
      "grad_norm": 11.242301940917969,
      "learning_rate": 4.900734214192358e-05,
      "loss": 0.5709,
      "num_input_tokens_seen": 145408,
      "step": 181
    },
    {
      "epoch": 0.14942528735632185,
      "grad_norm": 9.162640571594238,
      "learning_rate": 4.898284936350144e-05,
      "loss": 1.15,
      "num_input_tokens_seen": 146176,
      "step": 182
    },
    {
      "epoch": 0.15024630541871922,
      "grad_norm": 3.611511468887329,
      "learning_rate": 4.895806436162833e-05,
      "loss": 0.1115,
      "num_input_tokens_seen": 146944,
      "step": 183
    },
    {
      "epoch": 0.1510673234811166,
      "grad_norm": 14.162703514099121,
      "learning_rate": 4.893298743830168e-05,
      "loss": 1.4844,
      "num_input_tokens_seen": 147712,
      "step": 184
    },
    {
      "epoch": 0.15188834154351397,
      "grad_norm": 7.746850490570068,
      "learning_rate": 4.890761889907589e-05,
      "loss": 0.622,
      "num_input_tokens_seen": 148480,
      "step": 185
    },
    {
      "epoch": 0.15270935960591134,
      "grad_norm": 6.298117637634277,
      "learning_rate": 4.888195905305859e-05,
      "loss": 0.3603,
      "num_input_tokens_seen": 149376,
      "step": 186
    },
    {
      "epoch": 0.1535303776683087,
      "grad_norm": 6.0370659828186035,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 0.4034,
      "num_input_tokens_seen": 150272,
      "step": 187
    },
    {
      "epoch": 0.15435139573070608,
      "grad_norm": 3.1151883602142334,
      "learning_rate": 4.882976669482367e-05,
      "loss": 0.0844,
      "num_input_tokens_seen": 151040,
      "step": 188
    },
    {
      "epoch": 0.15517241379310345,
      "grad_norm": 6.112973213195801,
      "learning_rate": 4.880323481855347e-05,
      "loss": 0.4887,
      "num_input_tokens_seen": 151936,
      "step": 189
    },
    {
      "epoch": 0.15599343185550082,
      "grad_norm": 13.345136642456055,
      "learning_rate": 4.877641290737884e-05,
      "loss": 1.0798,
      "num_input_tokens_seen": 152704,
      "step": 190
    },
    {
      "epoch": 0.1568144499178982,
      "grad_norm": 8.685068130493164,
      "learning_rate": 4.874930128811631e-05,
      "loss": 0.6751,
      "num_input_tokens_seen": 153472,
      "step": 191
    },
    {
      "epoch": 0.15763546798029557,
      "grad_norm": 11.428149223327637,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 0.5688,
      "num_input_tokens_seen": 154240,
      "step": 192
    },
    {
      "epoch": 0.15845648604269294,
      "grad_norm": 10.85833740234375,
      "learning_rate": 4.869421025023965e-05,
      "loss": 1.046,
      "num_input_tokens_seen": 155264,
      "step": 193
    },
    {
      "epoch": 0.1592775041050903,
      "grad_norm": 16.53873062133789,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 1.3288,
      "num_input_tokens_seen": 156032,
      "step": 194
    },
    {
      "epoch": 0.16009852216748768,
      "grad_norm": 5.8890461921691895,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 0.3694,
      "num_input_tokens_seen": 156800,
      "step": 195
    },
    {
      "epoch": 0.16091954022988506,
      "grad_norm": 8.811628341674805,
      "learning_rate": 4.860940925593703e-05,
      "loss": 0.6394,
      "num_input_tokens_seen": 157568,
      "step": 196
    },
    {
      "epoch": 0.16174055829228243,
      "grad_norm": 6.202238082885742,
      "learning_rate": 4.858056644869002e-05,
      "loss": 0.5357,
      "num_input_tokens_seen": 158336,
      "step": 197
    },
    {
      "epoch": 0.1625615763546798,
      "grad_norm": 9.945191383361816,
      "learning_rate": 4.855143631968242e-05,
      "loss": 0.7819,
      "num_input_tokens_seen": 159104,
      "step": 198
    },
    {
      "epoch": 0.16338259441707717,
      "grad_norm": 9.572493553161621,
      "learning_rate": 4.852201922385564e-05,
      "loss": 0.5016,
      "num_input_tokens_seen": 159872,
      "step": 199
    },
    {
      "epoch": 0.16420361247947454,
      "grad_norm": 10.420982360839844,
      "learning_rate": 4.849231551964771e-05,
      "loss": 0.7964,
      "num_input_tokens_seen": 160640,
      "step": 200
    },
    {
      "epoch": 0.16502463054187191,
      "grad_norm": 16.23244285583496,
      "learning_rate": 4.84623255689889e-05,
      "loss": 1.5671,
      "num_input_tokens_seen": 161408,
      "step": 201
    },
    {
      "epoch": 0.16584564860426929,
      "grad_norm": 15.528718948364258,
      "learning_rate": 4.843204973729729e-05,
      "loss": 0.7844,
      "num_input_tokens_seen": 162176,
      "step": 202
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 11.876667976379395,
      "learning_rate": 4.840148839347434e-05,
      "loss": 0.7547,
      "num_input_tokens_seen": 163072,
      "step": 203
    },
    {
      "epoch": 0.16748768472906403,
      "grad_norm": 9.382770538330078,
      "learning_rate": 4.837064190990036e-05,
      "loss": 0.5017,
      "num_input_tokens_seen": 163968,
      "step": 204
    },
    {
      "epoch": 0.1683087027914614,
      "grad_norm": 8.889184951782227,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 0.4438,
      "num_input_tokens_seen": 164864,
      "step": 205
    },
    {
      "epoch": 0.16912972085385877,
      "grad_norm": 15.726595878601074,
      "learning_rate": 4.830809503038781e-05,
      "loss": 0.5999,
      "num_input_tokens_seen": 165632,
      "step": 206
    },
    {
      "epoch": 0.16995073891625614,
      "grad_norm": 9.992613792419434,
      "learning_rate": 4.827639539656321e-05,
      "loss": 0.7173,
      "num_input_tokens_seen": 166528,
      "step": 207
    },
    {
      "epoch": 0.17077175697865354,
      "grad_norm": 7.401379108428955,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 0.4142,
      "num_input_tokens_seen": 167296,
      "step": 208
    },
    {
      "epoch": 0.17159277504105092,
      "grad_norm": 7.254805564880371,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 0.4201,
      "num_input_tokens_seen": 168064,
      "step": 209
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 10.448773384094238,
      "learning_rate": 4.817959636416969e-05,
      "loss": 0.8516,
      "num_input_tokens_seen": 168832,
      "step": 210
    },
    {
      "epoch": 0.17323481116584566,
      "grad_norm": 9.942131996154785,
      "learning_rate": 4.814676462024988e-05,
      "loss": 0.533,
      "num_input_tokens_seen": 169728,
      "step": 211
    },
    {
      "epoch": 0.17405582922824303,
      "grad_norm": 13.451558113098145,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 1.1343,
      "num_input_tokens_seen": 170496,
      "step": 212
    },
    {
      "epoch": 0.1748768472906404,
      "grad_norm": 14.862506866455078,
      "learning_rate": 4.808025542782453e-05,
      "loss": 0.3096,
      "num_input_tokens_seen": 171264,
      "step": 213
    },
    {
      "epoch": 0.17569786535303777,
      "grad_norm": 6.813963890075684,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 0.3159,
      "num_input_tokens_seen": 172032,
      "step": 214
    },
    {
      "epoch": 0.17651888341543515,
      "grad_norm": 8.483039855957031,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 0.492,
      "num_input_tokens_seen": 172800,
      "step": 215
    },
    {
      "epoch": 0.17733990147783252,
      "grad_norm": 10.094818115234375,
      "learning_rate": 4.797838348138086e-05,
      "loss": 1.1016,
      "num_input_tokens_seen": 173696,
      "step": 216
    },
    {
      "epoch": 0.1781609195402299,
      "grad_norm": 11.748194694519043,
      "learning_rate": 4.794386564209953e-05,
      "loss": 1.0202,
      "num_input_tokens_seen": 174464,
      "step": 217
    },
    {
      "epoch": 0.17898193760262726,
      "grad_norm": 16.209672927856445,
      "learning_rate": 4.790906823905599e-05,
      "loss": 1.156,
      "num_input_tokens_seen": 175360,
      "step": 218
    },
    {
      "epoch": 0.17980295566502463,
      "grad_norm": 9.303634643554688,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 0.6909,
      "num_input_tokens_seen": 176128,
      "step": 219
    },
    {
      "epoch": 0.180623973727422,
      "grad_norm": 14.082103729248047,
      "learning_rate": 4.783863644106502e-05,
      "loss": 1.321,
      "num_input_tokens_seen": 176896,
      "step": 220
    },
    {
      "epoch": 0.18144499178981938,
      "grad_norm": 2.5644288063049316,
      "learning_rate": 4.780300290430682e-05,
      "loss": 0.0787,
      "num_input_tokens_seen": 177664,
      "step": 221
    },
    {
      "epoch": 0.18226600985221675,
      "grad_norm": 8.334208488464355,
      "learning_rate": 4.776709152015443e-05,
      "loss": 0.6735,
      "num_input_tokens_seen": 178560,
      "step": 222
    },
    {
      "epoch": 0.18308702791461412,
      "grad_norm": 11.09506893157959,
      "learning_rate": 4.773090272617672e-05,
      "loss": 0.6136,
      "num_input_tokens_seen": 179456,
      "step": 223
    },
    {
      "epoch": 0.1839080459770115,
      "grad_norm": 10.374229431152344,
      "learning_rate": 4.769443696332272e-05,
      "loss": 0.7891,
      "num_input_tokens_seen": 180352,
      "step": 224
    },
    {
      "epoch": 0.18472906403940886,
      "grad_norm": 10.459357261657715,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.7322,
      "num_input_tokens_seen": 181120,
      "step": 225
    },
    {
      "epoch": 0.18555008210180624,
      "grad_norm": 13.170193672180176,
      "learning_rate": 4.762067631165049e-05,
      "loss": 0.9784,
      "num_input_tokens_seen": 181888,
      "step": 226
    },
    {
      "epoch": 0.1863711001642036,
      "grad_norm": 10.548151969909668,
      "learning_rate": 4.758338232158252e-05,
      "loss": 0.7318,
      "num_input_tokens_seen": 182912,
      "step": 227
    },
    {
      "epoch": 0.18719211822660098,
      "grad_norm": 9.352437019348145,
      "learning_rate": 4.754581316012785e-05,
      "loss": 0.8898,
      "num_input_tokens_seen": 183680,
      "step": 228
    },
    {
      "epoch": 0.18801313628899835,
      "grad_norm": 5.892105579376221,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 0.2679,
      "num_input_tokens_seen": 184320,
      "step": 229
    },
    {
      "epoch": 0.18883415435139572,
      "grad_norm": 9.989742279052734,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 0.722,
      "num_input_tokens_seen": 185088,
      "step": 230
    },
    {
      "epoch": 0.1896551724137931,
      "grad_norm": 12.271588325500488,
      "learning_rate": 4.743145924185821e-05,
      "loss": 0.5043,
      "num_input_tokens_seen": 185856,
      "step": 231
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.888286828994751,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 0.1365,
      "num_input_tokens_seen": 186624,
      "step": 232
    },
    {
      "epoch": 0.19129720853858784,
      "grad_norm": 6.2825026512146,
      "learning_rate": 4.73538559209842e-05,
      "loss": 0.5228,
      "num_input_tokens_seen": 187392,
      "step": 233
    },
    {
      "epoch": 0.1921182266009852,
      "grad_norm": 11.104076385498047,
      "learning_rate": 4.731464546130314e-05,
      "loss": 0.6275,
      "num_input_tokens_seen": 188160,
      "step": 234
    },
    {
      "epoch": 0.19293924466338258,
      "grad_norm": 14.355345726013184,
      "learning_rate": 4.72751631047092e-05,
      "loss": 0.6235,
      "num_input_tokens_seen": 189056,
      "step": 235
    },
    {
      "epoch": 0.19376026272577998,
      "grad_norm": 11.069282531738281,
      "learning_rate": 4.723540933228244e-05,
      "loss": 0.4608,
      "num_input_tokens_seen": 189824,
      "step": 236
    },
    {
      "epoch": 0.19458128078817735,
      "grad_norm": 15.906550407409668,
      "learning_rate": 4.719538462841003e-05,
      "loss": 1.4489,
      "num_input_tokens_seen": 190592,
      "step": 237
    },
    {
      "epoch": 0.19540229885057472,
      "grad_norm": 12.730509757995605,
      "learning_rate": 4.715508948078037e-05,
      "loss": 0.9438,
      "num_input_tokens_seen": 191360,
      "step": 238
    },
    {
      "epoch": 0.1962233169129721,
      "grad_norm": 8.927741050720215,
      "learning_rate": 4.71145243803771e-05,
      "loss": 0.5244,
      "num_input_tokens_seen": 192128,
      "step": 239
    },
    {
      "epoch": 0.19704433497536947,
      "grad_norm": 5.996366500854492,
      "learning_rate": 4.707368982147318e-05,
      "loss": 0.2682,
      "num_input_tokens_seen": 192896,
      "step": 240
    },
    {
      "epoch": 0.19786535303776684,
      "grad_norm": 10.633506774902344,
      "learning_rate": 4.70325863016248e-05,
      "loss": 0.6967,
      "num_input_tokens_seen": 193664,
      "step": 241
    },
    {
      "epoch": 0.1986863711001642,
      "grad_norm": 6.869167327880859,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 0.1668,
      "num_input_tokens_seen": 194560,
      "step": 242
    },
    {
      "epoch": 0.19950738916256158,
      "grad_norm": 4.750509262084961,
      "learning_rate": 4.694957438569951e-05,
      "loss": 0.1529,
      "num_input_tokens_seen": 195328,
      "step": 243
    },
    {
      "epoch": 0.20032840722495895,
      "grad_norm": 8.339273452758789,
      "learning_rate": 4.690766700109659e-05,
      "loss": 0.5429,
      "num_input_tokens_seen": 196224,
      "step": 244
    },
    {
      "epoch": 0.20114942528735633,
      "grad_norm": 10.006525993347168,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 0.7861,
      "num_input_tokens_seen": 196992,
      "step": 245
    },
    {
      "epoch": 0.2019704433497537,
      "grad_norm": 14.864776611328125,
      "learning_rate": 4.682305193174524e-05,
      "loss": 0.499,
      "num_input_tokens_seen": 198016,
      "step": 246
    },
    {
      "epoch": 0.20279146141215107,
      "grad_norm": 16.616607666015625,
      "learning_rate": 4.678034527800474e-05,
      "loss": 1.0513,
      "num_input_tokens_seen": 198784,
      "step": 247
    },
    {
      "epoch": 0.20361247947454844,
      "grad_norm": 8.593324661254883,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 0.3441,
      "num_input_tokens_seen": 199552,
      "step": 248
    },
    {
      "epoch": 0.2044334975369458,
      "grad_norm": 11.528533935546875,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.5042,
      "num_input_tokens_seen": 200320,
      "step": 249
    },
    {
      "epoch": 0.20525451559934318,
      "grad_norm": 15.016440391540527,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.3259,
      "num_input_tokens_seen": 201088,
      "step": 250
    },
    {
      "epoch": 0.20607553366174056,
      "grad_norm": 8.225470542907715,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 0.2253,
      "num_input_tokens_seen": 201856,
      "step": 251
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 17.283279418945312,
      "learning_rate": 4.656284173018144e-05,
      "loss": 0.3748,
      "num_input_tokens_seen": 202624,
      "step": 252
    },
    {
      "epoch": 0.2077175697865353,
      "grad_norm": 14.351219177246094,
      "learning_rate": 4.65185506750986e-05,
      "loss": 0.9245,
      "num_input_tokens_seen": 203520,
      "step": 253
    },
    {
      "epoch": 0.20853858784893267,
      "grad_norm": 13.64018726348877,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 0.4199,
      "num_input_tokens_seen": 204288,
      "step": 254
    },
    {
      "epoch": 0.20935960591133004,
      "grad_norm": 20.357099533081055,
      "learning_rate": 4.642918251755281e-05,
      "loss": 1.4388,
      "num_input_tokens_seen": 205056,
      "step": 255
    },
    {
      "epoch": 0.21018062397372742,
      "grad_norm": 14.833854675292969,
      "learning_rate": 4.638410650401267e-05,
      "loss": 0.3692,
      "num_input_tokens_seen": 205824,
      "step": 256
    },
    {
      "epoch": 0.2110016420361248,
      "grad_norm": 13.791114807128906,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 0.6461,
      "num_input_tokens_seen": 206592,
      "step": 257
    },
    {
      "epoch": 0.21182266009852216,
      "grad_norm": 5.351015567779541,
      "learning_rate": 4.629317335357619e-05,
      "loss": 0.1382,
      "num_input_tokens_seen": 207488,
      "step": 258
    },
    {
      "epoch": 0.21264367816091953,
      "grad_norm": 2.986788034439087,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 0.0604,
      "num_input_tokens_seen": 208384,
      "step": 259
    },
    {
      "epoch": 0.2134646962233169,
      "grad_norm": 21.321683883666992,
      "learning_rate": 4.620120240391065e-05,
      "loss": 1.4445,
      "num_input_tokens_seen": 209280,
      "step": 260
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 12.13432502746582,
      "learning_rate": 4.615482915318911e-05,
      "loss": 0.608,
      "num_input_tokens_seen": 210048,
      "step": 261
    },
    {
      "epoch": 0.21510673234811165,
      "grad_norm": 15.580814361572266,
      "learning_rate": 4.610819813755038e-05,
      "loss": 0.7048,
      "num_input_tokens_seen": 210816,
      "step": 262
    },
    {
      "epoch": 0.21592775041050905,
      "grad_norm": 26.336915969848633,
      "learning_rate": 4.606130992517869e-05,
      "loss": 1.2827,
      "num_input_tokens_seen": 211584,
      "step": 263
    },
    {
      "epoch": 0.21674876847290642,
      "grad_norm": 5.777958869934082,
      "learning_rate": 4.601416508739211e-05,
      "loss": 0.1238,
      "num_input_tokens_seen": 212352,
      "step": 264
    },
    {
      "epoch": 0.2175697865353038,
      "grad_norm": 13.910466194152832,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 0.5978,
      "num_input_tokens_seen": 213120,
      "step": 265
    },
    {
      "epoch": 0.21839080459770116,
      "grad_norm": 8.325839042663574,
      "learning_rate": 4.591910783647404e-05,
      "loss": 0.6608,
      "num_input_tokens_seen": 214016,
      "step": 266
    },
    {
      "epoch": 0.21921182266009853,
      "grad_norm": 9.725253105163574,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 0.4187,
      "num_input_tokens_seen": 214784,
      "step": 267
    },
    {
      "epoch": 0.2200328407224959,
      "grad_norm": 14.917620658874512,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 1.0047,
      "num_input_tokens_seen": 215552,
      "step": 268
    },
    {
      "epoch": 0.22085385878489328,
      "grad_norm": 4.110219478607178,
      "learning_rate": 4.577461173185821e-05,
      "loss": 0.083,
      "num_input_tokens_seen": 216320,
      "step": 269
    },
    {
      "epoch": 0.22167487684729065,
      "grad_norm": 22.214258193969727,
      "learning_rate": 4.572593931387604e-05,
      "loss": 1.2835,
      "num_input_tokens_seen": 217216,
      "step": 270
    },
    {
      "epoch": 0.22249589490968802,
      "grad_norm": 15.02878475189209,
      "learning_rate": 4.567701435686404e-05,
      "loss": 0.2783,
      "num_input_tokens_seen": 217984,
      "step": 271
    },
    {
      "epoch": 0.2233169129720854,
      "grad_norm": 7.395138740539551,
      "learning_rate": 4.562783745695738e-05,
      "loss": 0.7647,
      "num_input_tokens_seen": 218752,
      "step": 272
    },
    {
      "epoch": 0.22413793103448276,
      "grad_norm": 15.196319580078125,
      "learning_rate": 4.557840921336105e-05,
      "loss": 1.1884,
      "num_input_tokens_seen": 219520,
      "step": 273
    },
    {
      "epoch": 0.22495894909688013,
      "grad_norm": 3.0958034992218018,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 0.1089,
      "num_input_tokens_seen": 220288,
      "step": 274
    },
    {
      "epoch": 0.2257799671592775,
      "grad_norm": 10.153539657592773,
      "learning_rate": 4.54788011072248e-05,
      "loss": 0.6845,
      "num_input_tokens_seen": 221056,
      "step": 275
    },
    {
      "epoch": 0.22660098522167488,
      "grad_norm": 16.114791870117188,
      "learning_rate": 4.542862245837821e-05,
      "loss": 0.7849,
      "num_input_tokens_seen": 221824,
      "step": 276
    },
    {
      "epoch": 0.22742200328407225,
      "grad_norm": 20.09825325012207,
      "learning_rate": 4.537819489321386e-05,
      "loss": 0.9347,
      "num_input_tokens_seen": 222592,
      "step": 277
    },
    {
      "epoch": 0.22824302134646962,
      "grad_norm": 11.50346565246582,
      "learning_rate": 4.532751902617569e-05,
      "loss": 0.566,
      "num_input_tokens_seen": 223360,
      "step": 278
    },
    {
      "epoch": 0.229064039408867,
      "grad_norm": 13.383136749267578,
      "learning_rate": 4.527659547473317e-05,
      "loss": 0.5657,
      "num_input_tokens_seen": 224256,
      "step": 279
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 17.973108291625977,
      "learning_rate": 4.522542485937369e-05,
      "loss": 1.073,
      "num_input_tokens_seen": 225024,
      "step": 280
    },
    {
      "epoch": 0.23070607553366174,
      "grad_norm": 4.085208892822266,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 0.1599,
      "num_input_tokens_seen": 225792,
      "step": 281
    },
    {
      "epoch": 0.2315270935960591,
      "grad_norm": 14.46140193939209,
      "learning_rate": 4.512234493389785e-05,
      "loss": 0.8581,
      "num_input_tokens_seen": 226688,
      "step": 282
    },
    {
      "epoch": 0.23234811165845648,
      "grad_norm": 10.349175453186035,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 0.3793,
      "num_input_tokens_seen": 227456,
      "step": 283
    },
    {
      "epoch": 0.23316912972085385,
      "grad_norm": 7.6240234375,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 0.215,
      "num_input_tokens_seen": 228224,
      "step": 284
    },
    {
      "epoch": 0.23399014778325122,
      "grad_norm": 14.16389274597168,
      "learning_rate": 4.496588775118232e-05,
      "loss": 0.93,
      "num_input_tokens_seen": 228992,
      "step": 285
    },
    {
      "epoch": 0.2348111658456486,
      "grad_norm": 16.663625717163086,
      "learning_rate": 4.491324795060491e-05,
      "loss": 0.9021,
      "num_input_tokens_seen": 229760,
      "step": 286
    },
    {
      "epoch": 0.23563218390804597,
      "grad_norm": 7.374619960784912,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.2922,
      "num_input_tokens_seen": 230656,
      "step": 287
    },
    {
      "epoch": 0.23645320197044334,
      "grad_norm": 15.627580642700195,
      "learning_rate": 4.480724108387977e-05,
      "loss": 1.0694,
      "num_input_tokens_seen": 231424,
      "step": 288
    },
    {
      "epoch": 0.2372742200328407,
      "grad_norm": 11.38653564453125,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 0.6602,
      "num_input_tokens_seen": 232192,
      "step": 289
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 10.951635360717773,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 1.0483,
      "num_input_tokens_seen": 232960,
      "step": 290
    },
    {
      "epoch": 0.23891625615763548,
      "grad_norm": 4.903123378753662,
      "learning_rate": 4.464642232938505e-05,
      "loss": 0.1052,
      "num_input_tokens_seen": 233728,
      "step": 291
    },
    {
      "epoch": 0.23973727422003285,
      "grad_norm": 15.125466346740723,
      "learning_rate": 4.4592336433146e-05,
      "loss": 0.4632,
      "num_input_tokens_seen": 234624,
      "step": 292
    },
    {
      "epoch": 0.24055829228243022,
      "grad_norm": 17.204240798950195,
      "learning_rate": 4.453801181047047e-05,
      "loss": 0.7858,
      "num_input_tokens_seen": 235392,
      "step": 293
    },
    {
      "epoch": 0.2413793103448276,
      "grad_norm": 8.982355117797852,
      "learning_rate": 4.448344912328686e-05,
      "loss": 0.4238,
      "num_input_tokens_seen": 236288,
      "step": 294
    },
    {
      "epoch": 0.24220032840722497,
      "grad_norm": 11.861900329589844,
      "learning_rate": 4.442864903642428e-05,
      "loss": 0.5634,
      "num_input_tokens_seen": 237056,
      "step": 295
    },
    {
      "epoch": 0.24302134646962234,
      "grad_norm": 14.05526351928711,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 0.7268,
      "num_input_tokens_seen": 237824,
      "step": 296
    },
    {
      "epoch": 0.2438423645320197,
      "grad_norm": 20.56101417541504,
      "learning_rate": 4.431833933743378e-05,
      "loss": 1.4515,
      "num_input_tokens_seen": 238592,
      "step": 297
    },
    {
      "epoch": 0.24466338259441708,
      "grad_norm": 13.916065216064453,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.7095,
      "num_input_tokens_seen": 239488,
      "step": 298
    },
    {
      "epoch": 0.24548440065681446,
      "grad_norm": 2.8632333278656006,
      "learning_rate": 4.420708808983809e-05,
      "loss": 0.0794,
      "num_input_tokens_seen": 240384,
      "step": 299
    },
    {
      "epoch": 0.24630541871921183,
      "grad_norm": 12.868613243103027,
      "learning_rate": 4.415111107797445e-05,
      "loss": 0.9735,
      "num_input_tokens_seen": 241152,
      "step": 300
    },
    {
      "epoch": 0.2471264367816092,
      "grad_norm": 9.154862403869629,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.5326,
      "num_input_tokens_seen": 241920,
      "step": 301
    },
    {
      "epoch": 0.24794745484400657,
      "grad_norm": 6.137587547302246,
      "learning_rate": 4.403845768841842e-05,
      "loss": 0.3801,
      "num_input_tokens_seen": 242688,
      "step": 302
    },
    {
      "epoch": 0.24876847290640394,
      "grad_norm": 5.967226982116699,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 0.1605,
      "num_input_tokens_seen": 243456,
      "step": 303
    },
    {
      "epoch": 0.24958949096880131,
      "grad_norm": 4.701290130615234,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 0.1799,
      "num_input_tokens_seen": 244224,
      "step": 304
    },
    {
      "epoch": 0.2504105090311987,
      "grad_norm": 4.751568794250488,
      "learning_rate": 4.386773950556931e-05,
      "loss": 0.1198,
      "num_input_tokens_seen": 244992,
      "step": 305
    },
    {
      "epoch": 0.2512315270935961,
      "grad_norm": 20.5931339263916,
      "learning_rate": 4.381037272239311e-05,
      "loss": 0.9484,
      "num_input_tokens_seen": 245760,
      "step": 306
    },
    {
      "epoch": 0.25205254515599346,
      "grad_norm": 4.035682678222656,
      "learning_rate": 4.375277674076149e-05,
      "loss": 0.1941,
      "num_input_tokens_seen": 246528,
      "step": 307
    },
    {
      "epoch": 0.25287356321839083,
      "grad_norm": 8.58834457397461,
      "learning_rate": 4.36949522624633e-05,
      "loss": 0.1772,
      "num_input_tokens_seen": 247296,
      "step": 308
    },
    {
      "epoch": 0.2536945812807882,
      "grad_norm": 3.8128662109375,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.0929,
      "num_input_tokens_seen": 248064,
      "step": 309
    },
    {
      "epoch": 0.2545155993431856,
      "grad_norm": 14.457817077636719,
      "learning_rate": 4.357862063693486e-05,
      "loss": 0.3656,
      "num_input_tokens_seen": 248832,
      "step": 310
    },
    {
      "epoch": 0.25533661740558294,
      "grad_norm": 16.362276077270508,
      "learning_rate": 4.352011490716875e-05,
      "loss": 0.4988,
      "num_input_tokens_seen": 249600,
      "step": 311
    },
    {
      "epoch": 0.2561576354679803,
      "grad_norm": 8.295536994934082,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 0.214,
      "num_input_tokens_seen": 250368,
      "step": 312
    },
    {
      "epoch": 0.2569786535303777,
      "grad_norm": 38.52027893066406,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 1.8041,
      "num_input_tokens_seen": 251136,
      "step": 313
    },
    {
      "epoch": 0.25779967159277506,
      "grad_norm": 31.95012092590332,
      "learning_rate": 4.334324661257191e-05,
      "loss": 1.1064,
      "num_input_tokens_seen": 252032,
      "step": 314
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 8.980833053588867,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 0.1435,
      "num_input_tokens_seen": 252800,
      "step": 315
    },
    {
      "epoch": 0.2594417077175698,
      "grad_norm": 26.23211097717285,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 1.6013,
      "num_input_tokens_seen": 253568,
      "step": 316
    },
    {
      "epoch": 0.2602627257799672,
      "grad_norm": 4.517701625823975,
      "learning_rate": 4.31643667742744e-05,
      "loss": 0.0617,
      "num_input_tokens_seen": 254336,
      "step": 317
    },
    {
      "epoch": 0.26108374384236455,
      "grad_norm": 10.295964241027832,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.3568,
      "num_input_tokens_seen": 255104,
      "step": 318
    },
    {
      "epoch": 0.2619047619047619,
      "grad_norm": 3.9237074851989746,
      "learning_rate": 4.304400570245906e-05,
      "loss": 0.0539,
      "num_input_tokens_seen": 255872,
      "step": 319
    },
    {
      "epoch": 0.2627257799671593,
      "grad_norm": 19.261085510253906,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 0.729,
      "num_input_tokens_seen": 256768,
      "step": 320
    },
    {
      "epoch": 0.26354679802955666,
      "grad_norm": 16.45861053466797,
      "learning_rate": 4.292276519126207e-05,
      "loss": 0.6795,
      "num_input_tokens_seen": 257536,
      "step": 321
    },
    {
      "epoch": 0.26436781609195403,
      "grad_norm": 29.715885162353516,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 2.0807,
      "num_input_tokens_seen": 258432,
      "step": 322
    },
    {
      "epoch": 0.2651888341543514,
      "grad_norm": 30.9678955078125,
      "learning_rate": 4.280065114977492e-05,
      "loss": 1.0949,
      "num_input_tokens_seen": 259328,
      "step": 323
    },
    {
      "epoch": 0.2660098522167488,
      "grad_norm": 15.780445098876953,
      "learning_rate": 4.273926841341302e-05,
      "loss": 1.3447,
      "num_input_tokens_seen": 260096,
      "step": 324
    },
    {
      "epoch": 0.26683087027914615,
      "grad_norm": 10.944400787353516,
      "learning_rate": 4.267766952966369e-05,
      "loss": 0.1661,
      "num_input_tokens_seen": 260864,
      "step": 325
    },
    {
      "epoch": 0.2676518883415435,
      "grad_norm": 16.731908798217773,
      "learning_rate": 4.261585524908987e-05,
      "loss": 0.3436,
      "num_input_tokens_seen": 261632,
      "step": 326
    },
    {
      "epoch": 0.2684729064039409,
      "grad_norm": 16.03744125366211,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 0.9867,
      "num_input_tokens_seen": 262400,
      "step": 327
    },
    {
      "epoch": 0.26929392446633826,
      "grad_norm": 26.55290412902832,
      "learning_rate": 4.249158351283414e-05,
      "loss": 1.0828,
      "num_input_tokens_seen": 263168,
      "step": 328
    },
    {
      "epoch": 0.27011494252873564,
      "grad_norm": 9.46867847442627,
      "learning_rate": 4.242912757136412e-05,
      "loss": 0.2691,
      "num_input_tokens_seen": 263936,
      "step": 329
    },
    {
      "epoch": 0.270935960591133,
      "grad_norm": 16.773967742919922,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 0.651,
      "num_input_tokens_seen": 264704,
      "step": 330
    },
    {
      "epoch": 0.2717569786535304,
      "grad_norm": 9.245512008666992,
      "learning_rate": 4.230357934676017e-05,
      "loss": 0.5518,
      "num_input_tokens_seen": 265472,
      "step": 331
    },
    {
      "epoch": 0.27257799671592775,
      "grad_norm": 9.606402397155762,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.3002,
      "num_input_tokens_seen": 266240,
      "step": 332
    },
    {
      "epoch": 0.2733990147783251,
      "grad_norm": 7.121413230895996,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.3476,
      "num_input_tokens_seen": 267008,
      "step": 333
    },
    {
      "epoch": 0.2742200328407225,
      "grad_norm": 4.991968154907227,
      "learning_rate": 4.211367764821722e-05,
      "loss": 0.1552,
      "num_input_tokens_seen": 267904,
      "step": 334
    },
    {
      "epoch": 0.27504105090311987,
      "grad_norm": 18.909568786621094,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 0.9661,
      "num_input_tokens_seen": 268672,
      "step": 335
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 17.11032485961914,
      "learning_rate": 4.198603260653792e-05,
      "loss": 0.5532,
      "num_input_tokens_seen": 269568,
      "step": 336
    },
    {
      "epoch": 0.2766830870279146,
      "grad_norm": 14.468923568725586,
      "learning_rate": 4.192189924206652e-05,
      "loss": 0.4952,
      "num_input_tokens_seen": 270464,
      "step": 337
    },
    {
      "epoch": 0.277504105090312,
      "grad_norm": 16.879928588867188,
      "learning_rate": 4.185755968959308e-05,
      "loss": 0.6606,
      "num_input_tokens_seen": 271232,
      "step": 338
    },
    {
      "epoch": 0.27832512315270935,
      "grad_norm": 6.700069904327393,
      "learning_rate": 4.179301473307476e-05,
      "loss": 0.2168,
      "num_input_tokens_seen": 272000,
      "step": 339
    },
    {
      "epoch": 0.2791461412151067,
      "grad_norm": 20.15970230102539,
      "learning_rate": 4.172826515897146e-05,
      "loss": 0.8203,
      "num_input_tokens_seen": 272768,
      "step": 340
    },
    {
      "epoch": 0.2799671592775041,
      "grad_norm": 10.28686237335205,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.3948,
      "num_input_tokens_seen": 273664,
      "step": 341
    },
    {
      "epoch": 0.28078817733990147,
      "grad_norm": 6.312470436096191,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 0.203,
      "num_input_tokens_seen": 274560,
      "step": 342
    },
    {
      "epoch": 0.28160919540229884,
      "grad_norm": 15.98139476776123,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 0.94,
      "num_input_tokens_seen": 275328,
      "step": 343
    },
    {
      "epoch": 0.2824302134646962,
      "grad_norm": 5.866513729095459,
      "learning_rate": 4.146723650296701e-05,
      "loss": 0.1932,
      "num_input_tokens_seen": 276096,
      "step": 344
    },
    {
      "epoch": 0.2832512315270936,
      "grad_norm": 10.436144828796387,
      "learning_rate": 4.140147572476268e-05,
      "loss": 0.3342,
      "num_input_tokens_seen": 276992,
      "step": 345
    },
    {
      "epoch": 0.28407224958949095,
      "grad_norm": 18.218639373779297,
      "learning_rate": 4.133551509975264e-05,
      "loss": 0.9076,
      "num_input_tokens_seen": 277760,
      "step": 346
    },
    {
      "epoch": 0.2848932676518883,
      "grad_norm": 7.809820652008057,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 0.2186,
      "num_input_tokens_seen": 278656,
      "step": 347
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 7.391127109527588,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 0.1938,
      "num_input_tokens_seen": 279424,
      "step": 348
    },
    {
      "epoch": 0.28653530377668307,
      "grad_norm": 13.538605690002441,
      "learning_rate": 4.113644219309877e-05,
      "loss": 0.7108,
      "num_input_tokens_seen": 280192,
      "step": 349
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 15.202836990356445,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 0.7841,
      "num_input_tokens_seen": 281088,
      "step": 350
    },
    {
      "epoch": 0.2881773399014778,
      "grad_norm": 13.179884910583496,
      "learning_rate": 4.100274248712389e-05,
      "loss": 0.8361,
      "num_input_tokens_seen": 281984,
      "step": 351
    },
    {
      "epoch": 0.2889983579638752,
      "grad_norm": 16.02565574645996,
      "learning_rate": 4.093559974371725e-05,
      "loss": 1.0792,
      "num_input_tokens_seen": 282880,
      "step": 352
    },
    {
      "epoch": 0.28981937602627256,
      "grad_norm": 10.9913911819458,
      "learning_rate": 4.086826283005669e-05,
      "loss": 0.6985,
      "num_input_tokens_seen": 283648,
      "step": 353
    },
    {
      "epoch": 0.29064039408866993,
      "grad_norm": 13.329235076904297,
      "learning_rate": 4.080073256662127e-05,
      "loss": 0.7577,
      "num_input_tokens_seen": 284416,
      "step": 354
    },
    {
      "epoch": 0.2914614121510673,
      "grad_norm": 12.2229585647583,
      "learning_rate": 4.073300977624594e-05,
      "loss": 0.4498,
      "num_input_tokens_seen": 285184,
      "step": 355
    },
    {
      "epoch": 0.2922824302134647,
      "grad_norm": 45.941932678222656,
      "learning_rate": 4.066509528411152e-05,
      "loss": 1.1128,
      "num_input_tokens_seen": 286080,
      "step": 356
    },
    {
      "epoch": 0.29310344827586204,
      "grad_norm": 22.539140701293945,
      "learning_rate": 4.059698991773466e-05,
      "loss": 1.4152,
      "num_input_tokens_seen": 286848,
      "step": 357
    },
    {
      "epoch": 0.2939244663382594,
      "grad_norm": 7.726198196411133,
      "learning_rate": 4.052869450695776e-05,
      "loss": 0.3052,
      "num_input_tokens_seen": 287616,
      "step": 358
    },
    {
      "epoch": 0.2947454844006568,
      "grad_norm": 19.158769607543945,
      "learning_rate": 4.046020988393885e-05,
      "loss": 0.9933,
      "num_input_tokens_seen": 288384,
      "step": 359
    },
    {
      "epoch": 0.2955665024630542,
      "grad_norm": 18.13735008239746,
      "learning_rate": 4.039153688314145e-05,
      "loss": 0.6808,
      "num_input_tokens_seen": 289152,
      "step": 360
    },
    {
      "epoch": 0.2963875205254516,
      "grad_norm": 13.536850929260254,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 0.6009,
      "num_input_tokens_seen": 289920,
      "step": 361
    },
    {
      "epoch": 0.29720853858784896,
      "grad_norm": 14.854897499084473,
      "learning_rate": 4.02536290975317e-05,
      "loss": 1.4105,
      "num_input_tokens_seen": 290688,
      "step": 362
    },
    {
      "epoch": 0.29802955665024633,
      "grad_norm": 9.893046379089355,
      "learning_rate": 4.018439599308217e-05,
      "loss": 0.4429,
      "num_input_tokens_seen": 291456,
      "step": 363
    },
    {
      "epoch": 0.2988505747126437,
      "grad_norm": 8.66922378540039,
      "learning_rate": 4.011497787155938e-05,
      "loss": 0.6252,
      "num_input_tokens_seen": 292608,
      "step": 364
    },
    {
      "epoch": 0.2996715927750411,
      "grad_norm": 21.358951568603516,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 0.9495,
      "num_input_tokens_seen": 293376,
      "step": 365
    },
    {
      "epoch": 0.30049261083743845,
      "grad_norm": 11.662996292114258,
      "learning_rate": 3.997558996288965e-05,
      "loss": 0.4156,
      "num_input_tokens_seen": 294144,
      "step": 366
    },
    {
      "epoch": 0.3013136288998358,
      "grad_norm": 15.117457389831543,
      "learning_rate": 3.99056218741404e-05,
      "loss": 1.0915,
      "num_input_tokens_seen": 294912,
      "step": 367
    },
    {
      "epoch": 0.3021346469622332,
      "grad_norm": 12.502971649169922,
      "learning_rate": 3.983547216509254e-05,
      "loss": 0.5195,
      "num_input_tokens_seen": 295680,
      "step": 368
    },
    {
      "epoch": 0.30295566502463056,
      "grad_norm": 6.037707328796387,
      "learning_rate": 3.976514169049814e-05,
      "loss": 0.2528,
      "num_input_tokens_seen": 296448,
      "step": 369
    },
    {
      "epoch": 0.30377668308702793,
      "grad_norm": 8.704992294311523,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.6004,
      "num_input_tokens_seen": 297216,
      "step": 370
    },
    {
      "epoch": 0.3045977011494253,
      "grad_norm": 6.852180480957031,
      "learning_rate": 3.962394187468039e-05,
      "loss": 0.3066,
      "num_input_tokens_seen": 297984,
      "step": 371
    },
    {
      "epoch": 0.3054187192118227,
      "grad_norm": 10.302886009216309,
      "learning_rate": 3.955307425393224e-05,
      "loss": 0.3415,
      "num_input_tokens_seen": 298752,
      "step": 372
    },
    {
      "epoch": 0.30623973727422005,
      "grad_norm": 24.25398826599121,
      "learning_rate": 3.948202930856697e-05,
      "loss": 1.0647,
      "num_input_tokens_seen": 299520,
      "step": 373
    },
    {
      "epoch": 0.3070607553366174,
      "grad_norm": 18.462806701660156,
      "learning_rate": 3.941080790424484e-05,
      "loss": 0.5585,
      "num_input_tokens_seen": 300288,
      "step": 374
    },
    {
      "epoch": 0.3078817733990148,
      "grad_norm": 2.6054084300994873,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.0695,
      "num_input_tokens_seen": 301056,
      "step": 375
    },
    {
      "epoch": 0.30870279146141216,
      "grad_norm": 9.050853729248047,
      "learning_rate": 3.92678391921108e-05,
      "loss": 0.4389,
      "num_input_tokens_seen": 301824,
      "step": 376
    },
    {
      "epoch": 0.30952380952380953,
      "grad_norm": 9.40036678314209,
      "learning_rate": 3.919609362632753e-05,
      "loss": 0.5903,
      "num_input_tokens_seen": 302592,
      "step": 377
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 7.592607498168945,
      "learning_rate": 3.912417508562345e-05,
      "loss": 0.6878,
      "num_input_tokens_seen": 303360,
      "step": 378
    },
    {
      "epoch": 0.3111658456486043,
      "grad_norm": 22.92536163330078,
      "learning_rate": 3.905208444630327e-05,
      "loss": 0.4797,
      "num_input_tokens_seen": 304128,
      "step": 379
    },
    {
      "epoch": 0.31198686371100165,
      "grad_norm": 13.348407745361328,
      "learning_rate": 3.897982258676867e-05,
      "loss": 0.9776,
      "num_input_tokens_seen": 304896,
      "step": 380
    },
    {
      "epoch": 0.312807881773399,
      "grad_norm": 13.493221282958984,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 0.7428,
      "num_input_tokens_seen": 305792,
      "step": 381
    },
    {
      "epoch": 0.3136288998357964,
      "grad_norm": 9.689300537109375,
      "learning_rate": 3.883478873108361e-05,
      "loss": 0.572,
      "num_input_tokens_seen": 306688,
      "step": 382
    },
    {
      "epoch": 0.31444991789819376,
      "grad_norm": 9.421388626098633,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 0.5977,
      "num_input_tokens_seen": 307456,
      "step": 383
    },
    {
      "epoch": 0.31527093596059114,
      "grad_norm": 9.222434997558594,
      "learning_rate": 3.868908058731376e-05,
      "loss": 0.2877,
      "num_input_tokens_seen": 308224,
      "step": 384
    },
    {
      "epoch": 0.3160919540229885,
      "grad_norm": 17.818233489990234,
      "learning_rate": 3.861597587537568e-05,
      "loss": 0.9838,
      "num_input_tokens_seen": 308992,
      "step": 385
    },
    {
      "epoch": 0.3169129720853859,
      "grad_norm": 6.142002105712891,
      "learning_rate": 3.85427052570685e-05,
      "loss": 0.281,
      "num_input_tokens_seen": 309760,
      "step": 386
    },
    {
      "epoch": 0.31773399014778325,
      "grad_norm": 11.594097137451172,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 0.4024,
      "num_input_tokens_seen": 310656,
      "step": 387
    },
    {
      "epoch": 0.3185550082101806,
      "grad_norm": 11.71949577331543,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 0.3904,
      "num_input_tokens_seen": 311424,
      "step": 388
    },
    {
      "epoch": 0.319376026272578,
      "grad_norm": 4.171940803527832,
      "learning_rate": 3.832190690176825e-05,
      "loss": 0.1481,
      "num_input_tokens_seen": 312192,
      "step": 389
    },
    {
      "epoch": 0.32019704433497537,
      "grad_norm": 4.929272174835205,
      "learning_rate": 3.824798160583012e-05,
      "loss": 0.1482,
      "num_input_tokens_seen": 312960,
      "step": 390
    },
    {
      "epoch": 0.32101806239737274,
      "grad_norm": 5.230946063995361,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 0.1148,
      "num_input_tokens_seen": 313728,
      "step": 391
    },
    {
      "epoch": 0.3218390804597701,
      "grad_norm": 8.56067180633545,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 0.2591,
      "num_input_tokens_seen": 314496,
      "step": 392
    },
    {
      "epoch": 0.3226600985221675,
      "grad_norm": 24.548593521118164,
      "learning_rate": 3.802524079601442e-05,
      "loss": 0.3998,
      "num_input_tokens_seen": 315264,
      "step": 393
    },
    {
      "epoch": 0.32348111658456485,
      "grad_norm": 11.301931381225586,
      "learning_rate": 3.795067523432826e-05,
      "loss": 0.3838,
      "num_input_tokens_seen": 316032,
      "step": 394
    },
    {
      "epoch": 0.3243021346469622,
      "grad_norm": 16.85011863708496,
      "learning_rate": 3.787595187275136e-05,
      "loss": 0.3394,
      "num_input_tokens_seen": 316800,
      "step": 395
    },
    {
      "epoch": 0.3251231527093596,
      "grad_norm": 21.90738296508789,
      "learning_rate": 3.780107162176429e-05,
      "loss": 0.5568,
      "num_input_tokens_seen": 317568,
      "step": 396
    },
    {
      "epoch": 0.32594417077175697,
      "grad_norm": 10.847840309143066,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 0.4243,
      "num_input_tokens_seen": 318336,
      "step": 397
    },
    {
      "epoch": 0.32676518883415434,
      "grad_norm": 8.687518119812012,
      "learning_rate": 3.765084410302909e-05,
      "loss": 0.265,
      "num_input_tokens_seen": 319104,
      "step": 398
    },
    {
      "epoch": 0.3275862068965517,
      "grad_norm": 16.380720138549805,
      "learning_rate": 3.757549866575588e-05,
      "loss": 0.4735,
      "num_input_tokens_seen": 319872,
      "step": 399
    },
    {
      "epoch": 0.3284072249589491,
      "grad_norm": 14.235166549682617,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.7137,
      "num_input_tokens_seen": 320640,
      "step": 400
    },
    {
      "epoch": 0.32922824302134646,
      "grad_norm": 14.533111572265625,
      "learning_rate": 3.742434902568889e-05,
      "loss": 0.4913,
      "num_input_tokens_seen": 321536,
      "step": 401
    },
    {
      "epoch": 0.33004926108374383,
      "grad_norm": 20.96527671813965,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 0.4539,
      "num_input_tokens_seen": 322304,
      "step": 402
    },
    {
      "epoch": 0.3308702791461412,
      "grad_norm": 13.42003059387207,
      "learning_rate": 3.727259384037852e-05,
      "loss": 0.5482,
      "num_input_tokens_seen": 323072,
      "step": 403
    },
    {
      "epoch": 0.33169129720853857,
      "grad_norm": 15.135832786560059,
      "learning_rate": 3.719649147846832e-05,
      "loss": 0.6583,
      "num_input_tokens_seen": 323840,
      "step": 404
    },
    {
      "epoch": 0.33251231527093594,
      "grad_norm": 10.200703620910645,
      "learning_rate": 3.712024050615843e-05,
      "loss": 0.111,
      "num_input_tokens_seen": 324608,
      "step": 405
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 19.792503356933594,
      "learning_rate": 3.704384185254288e-05,
      "loss": 1.2459,
      "num_input_tokens_seen": 325376,
      "step": 406
    },
    {
      "epoch": 0.3341543513957307,
      "grad_norm": 17.43242073059082,
      "learning_rate": 3.696729644851518e-05,
      "loss": 0.7056,
      "num_input_tokens_seen": 326144,
      "step": 407
    },
    {
      "epoch": 0.33497536945812806,
      "grad_norm": 12.844938278198242,
      "learning_rate": 3.689060522675689e-05,
      "loss": 1.386,
      "num_input_tokens_seen": 326912,
      "step": 408
    },
    {
      "epoch": 0.33579638752052543,
      "grad_norm": 16.548095703125,
      "learning_rate": 3.681376912172636e-05,
      "loss": 0.3867,
      "num_input_tokens_seen": 327808,
      "step": 409
    },
    {
      "epoch": 0.3366174055829228,
      "grad_norm": 15.318784713745117,
      "learning_rate": 3.673678906964727e-05,
      "loss": 0.4543,
      "num_input_tokens_seen": 328704,
      "step": 410
    },
    {
      "epoch": 0.3374384236453202,
      "grad_norm": 17.83009147644043,
      "learning_rate": 3.665966600849728e-05,
      "loss": 0.2949,
      "num_input_tokens_seen": 329600,
      "step": 411
    },
    {
      "epoch": 0.33825944170771755,
      "grad_norm": 10.118967056274414,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 0.407,
      "num_input_tokens_seen": 330368,
      "step": 412
    },
    {
      "epoch": 0.3390804597701149,
      "grad_norm": 13.021985054016113,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 0.3717,
      "num_input_tokens_seen": 331136,
      "step": 413
    },
    {
      "epoch": 0.3399014778325123,
      "grad_norm": 7.771170616149902,
      "learning_rate": 3.642744817646736e-05,
      "loss": 0.0746,
      "num_input_tokens_seen": 331904,
      "step": 414
    },
    {
      "epoch": 0.34072249589490966,
      "grad_norm": 29.868051528930664,
      "learning_rate": 3.634976249348867e-05,
      "loss": 1.883,
      "num_input_tokens_seen": 332672,
      "step": 415
    },
    {
      "epoch": 0.3415435139573071,
      "grad_norm": 14.447011947631836,
      "learning_rate": 3.627193851723577e-05,
      "loss": 0.7126,
      "num_input_tokens_seen": 333440,
      "step": 416
    },
    {
      "epoch": 0.34236453201970446,
      "grad_norm": 3.3036131858825684,
      "learning_rate": 3.619397719596924e-05,
      "loss": 0.0814,
      "num_input_tokens_seen": 334208,
      "step": 417
    },
    {
      "epoch": 0.34318555008210183,
      "grad_norm": 11.742940902709961,
      "learning_rate": 3.611587947962319e-05,
      "loss": 0.43,
      "num_input_tokens_seen": 334976,
      "step": 418
    },
    {
      "epoch": 0.3440065681444992,
      "grad_norm": 10.004315376281738,
      "learning_rate": 3.603764631979363e-05,
      "loss": 0.2892,
      "num_input_tokens_seen": 335872,
      "step": 419
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 9.416264533996582,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.2882,
      "num_input_tokens_seen": 336640,
      "step": 420
    },
    {
      "epoch": 0.34564860426929395,
      "grad_norm": 3.224182367324829,
      "learning_rate": 3.588077748430819e-05,
      "loss": 0.0754,
      "num_input_tokens_seen": 337408,
      "step": 421
    },
    {
      "epoch": 0.3464696223316913,
      "grad_norm": 14.65664291381836,
      "learning_rate": 3.580214372004956e-05,
      "loss": 1.2955,
      "num_input_tokens_seen": 338176,
      "step": 422
    },
    {
      "epoch": 0.3472906403940887,
      "grad_norm": 14.251296043395996,
      "learning_rate": 3.572337833507865e-05,
      "loss": 0.6631,
      "num_input_tokens_seen": 338944,
      "step": 423
    },
    {
      "epoch": 0.34811165845648606,
      "grad_norm": 10.87939739227295,
      "learning_rate": 3.564448228912682e-05,
      "loss": 0.5788,
      "num_input_tokens_seen": 339712,
      "step": 424
    },
    {
      "epoch": 0.34893267651888343,
      "grad_norm": 11.291116714477539,
      "learning_rate": 3.556545654351749e-05,
      "loss": 0.4749,
      "num_input_tokens_seen": 340608,
      "step": 425
    },
    {
      "epoch": 0.3497536945812808,
      "grad_norm": 13.711198806762695,
      "learning_rate": 3.548630206115443e-05,
      "loss": 0.5286,
      "num_input_tokens_seen": 341376,
      "step": 426
    },
    {
      "epoch": 0.3505747126436782,
      "grad_norm": 19.350360870361328,
      "learning_rate": 3.540701980651003e-05,
      "loss": 0.6936,
      "num_input_tokens_seen": 342144,
      "step": 427
    },
    {
      "epoch": 0.35139573070607555,
      "grad_norm": 20.970104217529297,
      "learning_rate": 3.532761074561355e-05,
      "loss": 1.6032,
      "num_input_tokens_seen": 342912,
      "step": 428
    },
    {
      "epoch": 0.3522167487684729,
      "grad_norm": 12.454900741577148,
      "learning_rate": 3.524807584603932e-05,
      "loss": 0.5368,
      "num_input_tokens_seen": 343680,
      "step": 429
    },
    {
      "epoch": 0.3530377668308703,
      "grad_norm": 13.215832710266113,
      "learning_rate": 3.516841607689501e-05,
      "loss": 0.2023,
      "num_input_tokens_seen": 344448,
      "step": 430
    },
    {
      "epoch": 0.35385878489326766,
      "grad_norm": 16.502578735351562,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 0.9771,
      "num_input_tokens_seen": 345216,
      "step": 431
    },
    {
      "epoch": 0.35467980295566504,
      "grad_norm": 19.433183670043945,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.3946,
      "num_input_tokens_seen": 345984,
      "step": 432
    },
    {
      "epoch": 0.3555008210180624,
      "grad_norm": 25.398670196533203,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 0.7674,
      "num_input_tokens_seen": 346880,
      "step": 433
    },
    {
      "epoch": 0.3563218390804598,
      "grad_norm": 5.712023735046387,
      "learning_rate": 3.484854773977378e-05,
      "loss": 0.1498,
      "num_input_tokens_seen": 347648,
      "step": 434
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 10.511703491210938,
      "learning_rate": 3.476827821223184e-05,
      "loss": 0.4399,
      "num_input_tokens_seen": 348416,
      "step": 435
    },
    {
      "epoch": 0.3579638752052545,
      "grad_norm": 7.398787498474121,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 0.4963,
      "num_input_tokens_seen": 349184,
      "step": 436
    },
    {
      "epoch": 0.3587848932676519,
      "grad_norm": 14.959653854370117,
      "learning_rate": 3.460738306649509e-05,
      "loss": 0.5107,
      "num_input_tokens_seen": 349952,
      "step": 437
    },
    {
      "epoch": 0.35960591133004927,
      "grad_norm": 23.947519302368164,
      "learning_rate": 3.452675940875686e-05,
      "loss": 0.7663,
      "num_input_tokens_seen": 350720,
      "step": 438
    },
    {
      "epoch": 0.36042692939244664,
      "grad_norm": 9.131391525268555,
      "learning_rate": 3.444601967046168e-05,
      "loss": 0.3147,
      "num_input_tokens_seen": 351488,
      "step": 439
    },
    {
      "epoch": 0.361247947454844,
      "grad_norm": 10.089783668518066,
      "learning_rate": 3.436516483539781e-05,
      "loss": 0.2127,
      "num_input_tokens_seen": 352384,
      "step": 440
    },
    {
      "epoch": 0.3620689655172414,
      "grad_norm": 8.5430269241333,
      "learning_rate": 3.428419588875588e-05,
      "loss": 0.2145,
      "num_input_tokens_seen": 353152,
      "step": 441
    },
    {
      "epoch": 0.36288998357963875,
      "grad_norm": 9.036399841308594,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 0.3916,
      "num_input_tokens_seen": 353920,
      "step": 442
    },
    {
      "epoch": 0.3637110016420361,
      "grad_norm": 8.846455574035645,
      "learning_rate": 3.412191960844049e-05,
      "loss": 0.4603,
      "num_input_tokens_seen": 354688,
      "step": 443
    },
    {
      "epoch": 0.3645320197044335,
      "grad_norm": 13.036741256713867,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 0.546,
      "num_input_tokens_seen": 355584,
      "step": 444
    },
    {
      "epoch": 0.36535303776683087,
      "grad_norm": 15.479804039001465,
      "learning_rate": 3.39591987386325e-05,
      "loss": 1.3454,
      "num_input_tokens_seen": 356480,
      "step": 445
    },
    {
      "epoch": 0.36617405582922824,
      "grad_norm": 11.876087188720703,
      "learning_rate": 3.387767406020343e-05,
      "loss": 0.5322,
      "num_input_tokens_seen": 357248,
      "step": 446
    },
    {
      "epoch": 0.3669950738916256,
      "grad_norm": 18.00340461730957,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 0.6874,
      "num_input_tokens_seen": 358016,
      "step": 447
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 9.421649932861328,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 0.6217,
      "num_input_tokens_seen": 358784,
      "step": 448
    },
    {
      "epoch": 0.36863711001642036,
      "grad_norm": 8.267794609069824,
      "learning_rate": 3.363245497496337e-05,
      "loss": 0.2541,
      "num_input_tokens_seen": 359552,
      "step": 449
    },
    {
      "epoch": 0.3694581280788177,
      "grad_norm": 6.14361047744751,
      "learning_rate": 3.355050358314172e-05,
      "loss": 0.196,
      "num_input_tokens_seen": 360320,
      "step": 450
    },
    {
      "epoch": 0.3702791461412151,
      "grad_norm": 7.911662578582764,
      "learning_rate": 3.346844800613229e-05,
      "loss": 0.25,
      "num_input_tokens_seen": 361088,
      "step": 451
    },
    {
      "epoch": 0.37110016420361247,
      "grad_norm": 7.805177211761475,
      "learning_rate": 3.338628924375638e-05,
      "loss": 0.2786,
      "num_input_tokens_seen": 361856,
      "step": 452
    },
    {
      "epoch": 0.37192118226600984,
      "grad_norm": 3.698157787322998,
      "learning_rate": 3.330402829709258e-05,
      "loss": 0.0894,
      "num_input_tokens_seen": 362624,
      "step": 453
    },
    {
      "epoch": 0.3727422003284072,
      "grad_norm": 12.390257835388184,
      "learning_rate": 3.322166616846458e-05,
      "loss": 0.5445,
      "num_input_tokens_seen": 363392,
      "step": 454
    },
    {
      "epoch": 0.3735632183908046,
      "grad_norm": 16.264930725097656,
      "learning_rate": 3.313920386142892e-05,
      "loss": 0.9269,
      "num_input_tokens_seen": 364160,
      "step": 455
    },
    {
      "epoch": 0.37438423645320196,
      "grad_norm": 20.10418128967285,
      "learning_rate": 3.305664238076278e-05,
      "loss": 0.8141,
      "num_input_tokens_seen": 364928,
      "step": 456
    },
    {
      "epoch": 0.37520525451559933,
      "grad_norm": 14.50174331665039,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 0.789,
      "num_input_tokens_seen": 365824,
      "step": 457
    },
    {
      "epoch": 0.3760262725779967,
      "grad_norm": 14.979418754577637,
      "learning_rate": 3.289122592367757e-05,
      "loss": 0.8718,
      "num_input_tokens_seen": 366592,
      "step": 458
    },
    {
      "epoch": 0.3768472906403941,
      "grad_norm": 13.19316577911377,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 0.5201,
      "num_input_tokens_seen": 367488,
      "step": 459
    },
    {
      "epoch": 0.37766830870279144,
      "grad_norm": 2.6812491416931152,
      "learning_rate": 3.272542485937369e-05,
      "loss": 0.0612,
      "num_input_tokens_seen": 368256,
      "step": 460
    },
    {
      "epoch": 0.3784893267651888,
      "grad_norm": 16.32787322998047,
      "learning_rate": 3.264238262407764e-05,
      "loss": 0.916,
      "num_input_tokens_seen": 369024,
      "step": 461
    },
    {
      "epoch": 0.3793103448275862,
      "grad_norm": 21.025405883789062,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 1.349,
      "num_input_tokens_seen": 369792,
      "step": 462
    },
    {
      "epoch": 0.38013136288998356,
      "grad_norm": 15.554962158203125,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.786,
      "num_input_tokens_seen": 370560,
      "step": 463
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 21.642902374267578,
      "learning_rate": 3.239270125110117e-05,
      "loss": 1.005,
      "num_input_tokens_seen": 371456,
      "step": 464
    },
    {
      "epoch": 0.3817733990147783,
      "grad_norm": 6.209118843078613,
      "learning_rate": 3.230929261806842e-05,
      "loss": 0.3367,
      "num_input_tokens_seen": 372224,
      "step": 465
    },
    {
      "epoch": 0.3825944170771757,
      "grad_norm": 12.294232368469238,
      "learning_rate": 3.222579492361179e-05,
      "loss": 0.6235,
      "num_input_tokens_seen": 372992,
      "step": 466
    },
    {
      "epoch": 0.38341543513957305,
      "grad_norm": 16.9058895111084,
      "learning_rate": 3.214220918512434e-05,
      "loss": 1.1454,
      "num_input_tokens_seen": 373760,
      "step": 467
    },
    {
      "epoch": 0.3842364532019704,
      "grad_norm": 21.40216064453125,
      "learning_rate": 3.205853642107192e-05,
      "loss": 1.2778,
      "num_input_tokens_seen": 374528,
      "step": 468
    },
    {
      "epoch": 0.3850574712643678,
      "grad_norm": 14.3012056350708,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 1.4709,
      "num_input_tokens_seen": 375296,
      "step": 469
    },
    {
      "epoch": 0.38587848932676516,
      "grad_norm": 3.7412052154541016,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 0.1217,
      "num_input_tokens_seen": 376064,
      "step": 470
    },
    {
      "epoch": 0.3866995073891626,
      "grad_norm": 14.614006042480469,
      "learning_rate": 3.180700617601436e-05,
      "loss": 0.6941,
      "num_input_tokens_seen": 376832,
      "step": 471
    },
    {
      "epoch": 0.38752052545155996,
      "grad_norm": 20.414113998413086,
      "learning_rate": 3.172299551538164e-05,
      "loss": 0.961,
      "num_input_tokens_seen": 377600,
      "step": 472
    },
    {
      "epoch": 0.38834154351395733,
      "grad_norm": 8.263985633850098,
      "learning_rate": 3.163890293717022e-05,
      "loss": 0.1646,
      "num_input_tokens_seen": 378368,
      "step": 473
    },
    {
      "epoch": 0.3891625615763547,
      "grad_norm": 6.419250011444092,
      "learning_rate": 3.155472946602162e-05,
      "loss": 0.2292,
      "num_input_tokens_seen": 379136,
      "step": 474
    },
    {
      "epoch": 0.3899835796387521,
      "grad_norm": 14.547205924987793,
      "learning_rate": 3.147047612756302e-05,
      "loss": 1.0749,
      "num_input_tokens_seen": 379904,
      "step": 475
    },
    {
      "epoch": 0.39080459770114945,
      "grad_norm": 13.365476608276367,
      "learning_rate": 3.138614394839476e-05,
      "loss": 0.6387,
      "num_input_tokens_seen": 380672,
      "step": 476
    },
    {
      "epoch": 0.3916256157635468,
      "grad_norm": 4.973348617553711,
      "learning_rate": 3.130173395607785e-05,
      "loss": 0.1539,
      "num_input_tokens_seen": 381440,
      "step": 477
    },
    {
      "epoch": 0.3924466338259442,
      "grad_norm": 11.80234432220459,
      "learning_rate": 3.121724717912138e-05,
      "loss": 0.415,
      "num_input_tokens_seen": 382208,
      "step": 478
    },
    {
      "epoch": 0.39326765188834156,
      "grad_norm": 10.186508178710938,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 0.3904,
      "num_input_tokens_seen": 382976,
      "step": 479
    },
    {
      "epoch": 0.39408866995073893,
      "grad_norm": 9.338964462280273,
      "learning_rate": 3.104804738999169e-05,
      "loss": 0.7718,
      "num_input_tokens_seen": 383744,
      "step": 480
    },
    {
      "epoch": 0.3949096880131363,
      "grad_norm": 7.4882941246032715,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 0.2856,
      "num_input_tokens_seen": 384512,
      "step": 481
    },
    {
      "epoch": 0.3957307060755337,
      "grad_norm": 10.473526000976562,
      "learning_rate": 3.087855282756475e-05,
      "loss": 0.361,
      "num_input_tokens_seen": 385280,
      "step": 482
    },
    {
      "epoch": 0.39655172413793105,
      "grad_norm": 21.437484741210938,
      "learning_rate": 3.079369758735393e-05,
      "loss": 1.2402,
      "num_input_tokens_seen": 386048,
      "step": 483
    },
    {
      "epoch": 0.3973727422003284,
      "grad_norm": 21.49000358581543,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 0.9374,
      "num_input_tokens_seen": 386816,
      "step": 484
    },
    {
      "epoch": 0.3981937602627258,
      "grad_norm": 5.187122821807861,
      "learning_rate": 3.062377635859663e-05,
      "loss": 0.1375,
      "num_input_tokens_seen": 387584,
      "step": 485
    },
    {
      "epoch": 0.39901477832512317,
      "grad_norm": 3.782320499420166,
      "learning_rate": 3.053871244048669e-05,
      "loss": 0.0906,
      "num_input_tokens_seen": 388352,
      "step": 486
    },
    {
      "epoch": 0.39983579638752054,
      "grad_norm": 18.951810836791992,
      "learning_rate": 3.045358103491357e-05,
      "loss": 1.1913,
      "num_input_tokens_seen": 389120,
      "step": 487
    },
    {
      "epoch": 0.4006568144499179,
      "grad_norm": 6.017267227172852,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 0.1791,
      "num_input_tokens_seen": 389888,
      "step": 488
    },
    {
      "epoch": 0.4014778325123153,
      "grad_norm": 19.06535530090332,
      "learning_rate": 3.028311991138472e-05,
      "loss": 0.6584,
      "num_input_tokens_seen": 390656,
      "step": 489
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 14.00788402557373,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 0.5291,
      "num_input_tokens_seen": 391424,
      "step": 490
    },
    {
      "epoch": 0.40311986863711,
      "grad_norm": 14.273772239685059,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 0.9131,
      "num_input_tokens_seen": 392320,
      "step": 491
    },
    {
      "epoch": 0.4039408866995074,
      "grad_norm": 11.821431159973145,
      "learning_rate": 3.002694802864912e-05,
      "loss": 0.74,
      "num_input_tokens_seen": 393216,
      "step": 492
    },
    {
      "epoch": 0.40476190476190477,
      "grad_norm": 7.707892417907715,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 0.2357,
      "num_input_tokens_seen": 393984,
      "step": 493
    },
    {
      "epoch": 0.40558292282430214,
      "grad_norm": 18.021345138549805,
      "learning_rate": 2.98558587804993e-05,
      "loss": 0.8599,
      "num_input_tokens_seen": 394752,
      "step": 494
    },
    {
      "epoch": 0.4064039408866995,
      "grad_norm": 7.472280025482178,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 0.3091,
      "num_input_tokens_seen": 395520,
      "step": 495
    },
    {
      "epoch": 0.4072249589490969,
      "grad_norm": 10.614762306213379,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 0.6016,
      "num_input_tokens_seen": 396416,
      "step": 496
    },
    {
      "epoch": 0.40804597701149425,
      "grad_norm": 10.9567232131958,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.5677,
      "num_input_tokens_seen": 397312,
      "step": 497
    },
    {
      "epoch": 0.4088669950738916,
      "grad_norm": 12.353440284729004,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 0.407,
      "num_input_tokens_seen": 398080,
      "step": 498
    },
    {
      "epoch": 0.409688013136289,
      "grad_norm": 17.06678581237793,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.523,
      "num_input_tokens_seen": 398848,
      "step": 499
    },
    {
      "epoch": 0.41050903119868637,
      "grad_norm": 6.602445602416992,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 0.3396,
      "num_input_tokens_seen": 399616,
      "step": 500
    },
    {
      "epoch": 0.41050903119868637,
      "eval_loss": 0.4703861474990845,
      "eval_runtime": 12.0393,
      "eval_samples_per_second": 101.418,
      "eval_steps_per_second": 12.708,
      "num_input_tokens_seen": 399616,
      "step": 500
    },
    {
      "epoch": 0.41133004926108374,
      "grad_norm": 19.35688591003418,
      "learning_rate": 2.9255237479150816e-05,
      "loss": 0.7444,
      "num_input_tokens_seen": 400384,
      "step": 501
    },
    {
      "epoch": 0.4121510673234811,
      "grad_norm": 6.320526599884033,
      "learning_rate": 2.916921866790256e-05,
      "loss": 0.1701,
      "num_input_tokens_seen": 401152,
      "step": 502
    },
    {
      "epoch": 0.4129720853858785,
      "grad_norm": 11.100852012634277,
      "learning_rate": 2.908314905604056e-05,
      "loss": 0.5142,
      "num_input_tokens_seen": 402048,
      "step": 503
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 10.4762544631958,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 0.5485,
      "num_input_tokens_seen": 402816,
      "step": 504
    },
    {
      "epoch": 0.41461412151067323,
      "grad_norm": 17.30388069152832,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 0.9949,
      "num_input_tokens_seen": 403712,
      "step": 505
    },
    {
      "epoch": 0.4154351395730706,
      "grad_norm": 11.333624839782715,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 0.7342,
      "num_input_tokens_seen": 404480,
      "step": 506
    },
    {
      "epoch": 0.41625615763546797,
      "grad_norm": 18.3604793548584,
      "learning_rate": 2.8738383586092745e-05,
      "loss": 1.186,
      "num_input_tokens_seen": 405248,
      "step": 507
    },
    {
      "epoch": 0.41707717569786534,
      "grad_norm": 15.542903900146484,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 1.0823,
      "num_input_tokens_seen": 406144,
      "step": 508
    },
    {
      "epoch": 0.4178981937602627,
      "grad_norm": 6.940396308898926,
      "learning_rate": 2.8565723342637796e-05,
      "loss": 0.2374,
      "num_input_tokens_seen": 407040,
      "step": 509
    },
    {
      "epoch": 0.4187192118226601,
      "grad_norm": 18.78009605407715,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 1.3166,
      "num_input_tokens_seen": 407936,
      "step": 510
    },
    {
      "epoch": 0.41954022988505746,
      "grad_norm": 11.321510314941406,
      "learning_rate": 2.8392889310857612e-05,
      "loss": 0.3548,
      "num_input_tokens_seen": 408704,
      "step": 511
    },
    {
      "epoch": 0.42036124794745483,
      "grad_norm": 10.15684986114502,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 0.2327,
      "num_input_tokens_seen": 409472,
      "step": 512
    },
    {
      "epoch": 0.4211822660098522,
      "grad_norm": 22.416574478149414,
      "learning_rate": 2.8219889914439074e-05,
      "loss": 0.7843,
      "num_input_tokens_seen": 410240,
      "step": 513
    },
    {
      "epoch": 0.4220032840722496,
      "grad_norm": 7.699347019195557,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 0.2594,
      "num_input_tokens_seen": 411008,
      "step": 514
    },
    {
      "epoch": 0.42282430213464695,
      "grad_norm": 6.438681125640869,
      "learning_rate": 2.8046733585128687e-05,
      "loss": 0.1121,
      "num_input_tokens_seen": 411776,
      "step": 515
    },
    {
      "epoch": 0.4236453201970443,
      "grad_norm": 7.748869895935059,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 0.7395,
      "num_input_tokens_seen": 412544,
      "step": 516
    },
    {
      "epoch": 0.4244663382594417,
      "grad_norm": 11.81357479095459,
      "learning_rate": 2.787342876232167e-05,
      "loss": 0.4163,
      "num_input_tokens_seen": 413440,
      "step": 517
    },
    {
      "epoch": 0.42528735632183906,
      "grad_norm": 14.376007080078125,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 0.3822,
      "num_input_tokens_seen": 414208,
      "step": 518
    },
    {
      "epoch": 0.42610837438423643,
      "grad_norm": 19.75806427001953,
      "learning_rate": 2.7699983892650573e-05,
      "loss": 1.0284,
      "num_input_tokens_seen": 415104,
      "step": 519
    },
    {
      "epoch": 0.4269293924466338,
      "grad_norm": 2.552100896835327,
      "learning_rate": 2.761321158169134e-05,
      "loss": 0.0711,
      "num_input_tokens_seen": 415872,
      "step": 520
    },
    {
      "epoch": 0.4277504105090312,
      "grad_norm": 13.803062438964844,
      "learning_rate": 2.7526407429573657e-05,
      "loss": 0.8436,
      "num_input_tokens_seen": 416768,
      "step": 521
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 13.035656929016113,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 0.5827,
      "num_input_tokens_seen": 417536,
      "step": 522
    },
    {
      "epoch": 0.4293924466338259,
      "grad_norm": 9.609467506408691,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 0.3965,
      "num_input_tokens_seen": 418304,
      "step": 523
    },
    {
      "epoch": 0.4302134646962233,
      "grad_norm": 14.586166381835938,
      "learning_rate": 2.726581450494451e-05,
      "loss": 0.8649,
      "num_input_tokens_seen": 419200,
      "step": 524
    },
    {
      "epoch": 0.43103448275862066,
      "grad_norm": 4.094796657562256,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.1265,
      "num_input_tokens_seen": 419968,
      "step": 525
    },
    {
      "epoch": 0.4318555008210181,
      "grad_norm": 13.132792472839355,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 0.8573,
      "num_input_tokens_seen": 420736,
      "step": 526
    },
    {
      "epoch": 0.43267651888341546,
      "grad_norm": 14.50829029083252,
      "learning_rate": 2.7004973108221472e-05,
      "loss": 0.5539,
      "num_input_tokens_seen": 421632,
      "step": 527
    },
    {
      "epoch": 0.43349753694581283,
      "grad_norm": 5.015863418579102,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 0.1216,
      "num_input_tokens_seen": 422528,
      "step": 528
    },
    {
      "epoch": 0.4343185550082102,
      "grad_norm": 18.96428108215332,
      "learning_rate": 2.6830954928190794e-05,
      "loss": 0.7517,
      "num_input_tokens_seen": 423296,
      "step": 529
    },
    {
      "epoch": 0.4351395730706076,
      "grad_norm": 16.336559295654297,
      "learning_rate": 2.674391184360313e-05,
      "loss": 1.0467,
      "num_input_tokens_seen": 424064,
      "step": 530
    },
    {
      "epoch": 0.43596059113300495,
      "grad_norm": 18.71722984313965,
      "learning_rate": 2.6656847510000012e-05,
      "loss": 0.818,
      "num_input_tokens_seen": 424832,
      "step": 531
    },
    {
      "epoch": 0.4367816091954023,
      "grad_norm": 6.732291221618652,
      "learning_rate": 2.656976298823284e-05,
      "loss": 0.1301,
      "num_input_tokens_seen": 425728,
      "step": 532
    },
    {
      "epoch": 0.4376026272577997,
      "grad_norm": 21.847675323486328,
      "learning_rate": 2.6482659339399045e-05,
      "loss": 0.8876,
      "num_input_tokens_seen": 426496,
      "step": 533
    },
    {
      "epoch": 0.43842364532019706,
      "grad_norm": 15.367324829101562,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 0.6217,
      "num_input_tokens_seen": 427392,
      "step": 534
    },
    {
      "epoch": 0.43924466338259444,
      "grad_norm": 7.815330505371094,
      "learning_rate": 2.63083989060736e-05,
      "loss": 0.2941,
      "num_input_tokens_seen": 428288,
      "step": 535
    },
    {
      "epoch": 0.4400656814449918,
      "grad_norm": 8.286810874938965,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 0.5416,
      "num_input_tokens_seen": 429056,
      "step": 536
    },
    {
      "epoch": 0.4408866995073892,
      "grad_norm": 12.175736427307129,
      "learning_rate": 2.6134074703231344e-05,
      "loss": 0.3181,
      "num_input_tokens_seen": 429824,
      "step": 537
    },
    {
      "epoch": 0.44170771756978655,
      "grad_norm": 16.803516387939453,
      "learning_rate": 2.604689134322999e-05,
      "loss": 1.0115,
      "num_input_tokens_seen": 430592,
      "step": 538
    },
    {
      "epoch": 0.4425287356321839,
      "grad_norm": 13.31485366821289,
      "learning_rate": 2.5959695227188004e-05,
      "loss": 1.0615,
      "num_input_tokens_seen": 431616,
      "step": 539
    },
    {
      "epoch": 0.4433497536945813,
      "grad_norm": 11.350493431091309,
      "learning_rate": 2.587248741756253e-05,
      "loss": 0.4275,
      "num_input_tokens_seen": 432512,
      "step": 540
    },
    {
      "epoch": 0.44417077175697867,
      "grad_norm": 18.95566749572754,
      "learning_rate": 2.578526897695321e-05,
      "loss": 1.0078,
      "num_input_tokens_seen": 433280,
      "step": 541
    },
    {
      "epoch": 0.44499178981937604,
      "grad_norm": 16.058502197265625,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 1.0594,
      "num_input_tokens_seen": 434048,
      "step": 542
    },
    {
      "epoch": 0.4458128078817734,
      "grad_norm": 7.835014820098877,
      "learning_rate": 2.5610804453816333e-05,
      "loss": 0.375,
      "num_input_tokens_seen": 434816,
      "step": 543
    },
    {
      "epoch": 0.4466338259441708,
      "grad_norm": 14.603649139404297,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 0.3724,
      "num_input_tokens_seen": 435584,
      "step": 544
    },
    {
      "epoch": 0.44745484400656815,
      "grad_norm": 9.284316062927246,
      "learning_rate": 2.5436310160932092e-05,
      "loss": 0.4532,
      "num_input_tokens_seen": 436352,
      "step": 545
    },
    {
      "epoch": 0.4482758620689655,
      "grad_norm": 12.097857475280762,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 0.5078,
      "num_input_tokens_seen": 437120,
      "step": 546
    },
    {
      "epoch": 0.4490968801313629,
      "grad_norm": 9.680200576782227,
      "learning_rate": 2.5261794602906145e-05,
      "loss": 0.4196,
      "num_input_tokens_seen": 437888,
      "step": 547
    },
    {
      "epoch": 0.44991789819376027,
      "grad_norm": 18.630325317382812,
      "learning_rate": 2.517453150744904e-05,
      "loss": 0.6532,
      "num_input_tokens_seen": 438656,
      "step": 548
    },
    {
      "epoch": 0.45073891625615764,
      "grad_norm": 10.801551818847656,
      "learning_rate": 2.5087266285380596e-05,
      "loss": 0.8854,
      "num_input_tokens_seen": 439424,
      "step": 549
    },
    {
      "epoch": 0.451559934318555,
      "grad_norm": 8.628301620483398,
      "learning_rate": 2.5e-05,
      "loss": 0.3407,
      "num_input_tokens_seen": 440192,
      "step": 550
    },
    {
      "epoch": 0.4523809523809524,
      "grad_norm": 14.457107543945312,
      "learning_rate": 2.4912733714619417e-05,
      "loss": 0.9979,
      "num_input_tokens_seen": 441088,
      "step": 551
    },
    {
      "epoch": 0.45320197044334976,
      "grad_norm": 10.942357063293457,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 0.3734,
      "num_input_tokens_seen": 441984,
      "step": 552
    },
    {
      "epoch": 0.4540229885057471,
      "grad_norm": 9.507813453674316,
      "learning_rate": 2.4738205397093864e-05,
      "loss": 0.403,
      "num_input_tokens_seen": 442752,
      "step": 553
    },
    {
      "epoch": 0.4548440065681445,
      "grad_norm": 9.419300079345703,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 0.3911,
      "num_input_tokens_seen": 443520,
      "step": 554
    },
    {
      "epoch": 0.45566502463054187,
      "grad_norm": 14.117770195007324,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.7296,
      "num_input_tokens_seen": 444288,
      "step": 555
    },
    {
      "epoch": 0.45648604269293924,
      "grad_norm": 7.1792683601379395,
      "learning_rate": 2.447643950291608e-05,
      "loss": 0.2255,
      "num_input_tokens_seen": 445056,
      "step": 556
    },
    {
      "epoch": 0.4573070607553366,
      "grad_norm": 14.34988021850586,
      "learning_rate": 2.4389195546183673e-05,
      "loss": 0.6366,
      "num_input_tokens_seen": 445824,
      "step": 557
    },
    {
      "epoch": 0.458128078817734,
      "grad_norm": 11.970149993896484,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 0.7305,
      "num_input_tokens_seen": 446592,
      "step": 558
    },
    {
      "epoch": 0.45894909688013136,
      "grad_norm": 8.12315845489502,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 0.2821,
      "num_input_tokens_seen": 447360,
      "step": 559
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 13.028664588928223,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 0.5584,
      "num_input_tokens_seen": 448256,
      "step": 560
    },
    {
      "epoch": 0.4605911330049261,
      "grad_norm": 7.477100372314453,
      "learning_rate": 2.4040304772812002e-05,
      "loss": 0.2034,
      "num_input_tokens_seen": 449024,
      "step": 561
    },
    {
      "epoch": 0.4614121510673235,
      "grad_norm": 8.181623458862305,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 0.2255,
      "num_input_tokens_seen": 449792,
      "step": 562
    },
    {
      "epoch": 0.46223316912972084,
      "grad_norm": 6.77685022354126,
      "learning_rate": 2.386592529676866e-05,
      "loss": 0.2205,
      "num_input_tokens_seen": 450688,
      "step": 563
    },
    {
      "epoch": 0.4630541871921182,
      "grad_norm": 21.343393325805664,
      "learning_rate": 2.377875575510967e-05,
      "loss": 0.7064,
      "num_input_tokens_seen": 451456,
      "step": 564
    },
    {
      "epoch": 0.4638752052545156,
      "grad_norm": 14.89813232421875,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 1.1803,
      "num_input_tokens_seen": 452224,
      "step": 565
    },
    {
      "epoch": 0.46469622331691296,
      "grad_norm": 8.084806442260742,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 0.2586,
      "num_input_tokens_seen": 452992,
      "step": 566
    },
    {
      "epoch": 0.46551724137931033,
      "grad_norm": 22.555458068847656,
      "learning_rate": 2.3517340660600964e-05,
      "loss": 1.3179,
      "num_input_tokens_seen": 453760,
      "step": 567
    },
    {
      "epoch": 0.4663382594417077,
      "grad_norm": 7.943397521972656,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 0.308,
      "num_input_tokens_seen": 454528,
      "step": 568
    },
    {
      "epoch": 0.4671592775041051,
      "grad_norm": 7.5424089431762695,
      "learning_rate": 2.3343152490000004e-05,
      "loss": 0.1873,
      "num_input_tokens_seen": 455296,
      "step": 569
    },
    {
      "epoch": 0.46798029556650245,
      "grad_norm": 15.884626388549805,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 0.4796,
      "num_input_tokens_seen": 456064,
      "step": 570
    },
    {
      "epoch": 0.4688013136288998,
      "grad_norm": 6.477445125579834,
      "learning_rate": 2.3169045071809215e-05,
      "loss": 0.1815,
      "num_input_tokens_seen": 456832,
      "step": 571
    },
    {
      "epoch": 0.4696223316912972,
      "grad_norm": 14.05634880065918,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 0.3685,
      "num_input_tokens_seen": 457600,
      "step": 572
    },
    {
      "epoch": 0.47044334975369456,
      "grad_norm": 20.57737159729004,
      "learning_rate": 2.299502689177853e-05,
      "loss": 0.6233,
      "num_input_tokens_seen": 458368,
      "step": 573
    },
    {
      "epoch": 0.47126436781609193,
      "grad_norm": 18.201343536376953,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 1.0164,
      "num_input_tokens_seen": 459136,
      "step": 574
    },
    {
      "epoch": 0.4720853858784893,
      "grad_norm": 12.112269401550293,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 0.6952,
      "num_input_tokens_seen": 460032,
      "step": 575
    },
    {
      "epoch": 0.4729064039408867,
      "grad_norm": 11.225955963134766,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 0.4436,
      "num_input_tokens_seen": 460800,
      "step": 576
    },
    {
      "epoch": 0.47372742200328405,
      "grad_norm": 3.5927422046661377,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 0.0744,
      "num_input_tokens_seen": 461568,
      "step": 577
    },
    {
      "epoch": 0.4745484400656814,
      "grad_norm": 5.147429466247559,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 0.1125,
      "num_input_tokens_seen": 462336,
      "step": 578
    },
    {
      "epoch": 0.4753694581280788,
      "grad_norm": 16.346776962280273,
      "learning_rate": 2.247359257042634e-05,
      "loss": 1.5534,
      "num_input_tokens_seen": 463360,
      "step": 579
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 4.279769420623779,
      "learning_rate": 2.238678841830867e-05,
      "loss": 0.1282,
      "num_input_tokens_seen": 464128,
      "step": 580
    },
    {
      "epoch": 0.47701149425287354,
      "grad_norm": 7.396267890930176,
      "learning_rate": 2.230001610734943e-05,
      "loss": 0.2548,
      "num_input_tokens_seen": 464896,
      "step": 581
    },
    {
      "epoch": 0.47783251231527096,
      "grad_norm": 2.1474671363830566,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 0.046,
      "num_input_tokens_seen": 465664,
      "step": 582
    },
    {
      "epoch": 0.47865353037766833,
      "grad_norm": 16.87557601928711,
      "learning_rate": 2.212657123767834e-05,
      "loss": 0.9426,
      "num_input_tokens_seen": 466432,
      "step": 583
    },
    {
      "epoch": 0.4794745484400657,
      "grad_norm": 12.806222915649414,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 0.5071,
      "num_input_tokens_seen": 467200,
      "step": 584
    },
    {
      "epoch": 0.4802955665024631,
      "grad_norm": 16.756982803344727,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.5275,
      "num_input_tokens_seen": 467968,
      "step": 585
    },
    {
      "epoch": 0.48111658456486045,
      "grad_norm": 9.12282943725586,
      "learning_rate": 2.186666916089239e-05,
      "loss": 0.2926,
      "num_input_tokens_seen": 468864,
      "step": 586
    },
    {
      "epoch": 0.4819376026272578,
      "grad_norm": 8.454472541809082,
      "learning_rate": 2.1780110085560935e-05,
      "loss": 0.1631,
      "num_input_tokens_seen": 469632,
      "step": 587
    },
    {
      "epoch": 0.4827586206896552,
      "grad_norm": 14.974968910217285,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 0.8331,
      "num_input_tokens_seen": 470400,
      "step": 588
    },
    {
      "epoch": 0.48357963875205257,
      "grad_norm": 17.060686111450195,
      "learning_rate": 2.1607110689142393e-05,
      "loss": 0.3156,
      "num_input_tokens_seen": 471168,
      "step": 589
    },
    {
      "epoch": 0.48440065681444994,
      "grad_norm": 16.482643127441406,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 0.9606,
      "num_input_tokens_seen": 471936,
      "step": 590
    },
    {
      "epoch": 0.4852216748768473,
      "grad_norm": 12.047331809997559,
      "learning_rate": 2.1434276657362213e-05,
      "loss": 0.7229,
      "num_input_tokens_seen": 472832,
      "step": 591
    },
    {
      "epoch": 0.4860426929392447,
      "grad_norm": 16.2746639251709,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 0.931,
      "num_input_tokens_seen": 473600,
      "step": 592
    },
    {
      "epoch": 0.48686371100164205,
      "grad_norm": 22.64579200744629,
      "learning_rate": 2.1261616413907265e-05,
      "loss": 1.2682,
      "num_input_tokens_seen": 474368,
      "step": 593
    },
    {
      "epoch": 0.4876847290640394,
      "grad_norm": 13.522481918334961,
      "learning_rate": 2.117535409289905e-05,
      "loss": 0.4283,
      "num_input_tokens_seen": 475136,
      "step": 594
    },
    {
      "epoch": 0.4885057471264368,
      "grad_norm": 22.673816680908203,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 1.5862,
      "num_input_tokens_seen": 476032,
      "step": 595
    },
    {
      "epoch": 0.48932676518883417,
      "grad_norm": 9.340457916259766,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 0.2809,
      "num_input_tokens_seen": 476800,
      "step": 596
    },
    {
      "epoch": 0.49014778325123154,
      "grad_norm": 27.659793853759766,
      "learning_rate": 2.0916850943959452e-05,
      "loss": 0.9237,
      "num_input_tokens_seen": 477568,
      "step": 597
    },
    {
      "epoch": 0.4909688013136289,
      "grad_norm": 24.992687225341797,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 1.3924,
      "num_input_tokens_seen": 478464,
      "step": 598
    },
    {
      "epoch": 0.4917898193760263,
      "grad_norm": 19.81684684753418,
      "learning_rate": 2.0744762520849193e-05,
      "loss": 0.8118,
      "num_input_tokens_seen": 479232,
      "step": 599
    },
    {
      "epoch": 0.49261083743842365,
      "grad_norm": 22.296777725219727,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 0.5491,
      "num_input_tokens_seen": 480000,
      "step": 600
    },
    {
      "epoch": 0.493431855500821,
      "grad_norm": 5.045540809631348,
      "learning_rate": 2.057288149201042e-05,
      "loss": 0.1854,
      "num_input_tokens_seen": 480768,
      "step": 601
    },
    {
      "epoch": 0.4942528735632184,
      "grad_norm": 13.759693145751953,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 0.3219,
      "num_input_tokens_seen": 481536,
      "step": 602
    },
    {
      "epoch": 0.49507389162561577,
      "grad_norm": 11.659880638122559,
      "learning_rate": 2.0401216234681995e-05,
      "loss": 0.9676,
      "num_input_tokens_seen": 482304,
      "step": 603
    },
    {
      "epoch": 0.49589490968801314,
      "grad_norm": 13.373348236083984,
      "learning_rate": 2.031546713535688e-05,
      "loss": 0.4908,
      "num_input_tokens_seen": 483072,
      "step": 604
    },
    {
      "epoch": 0.4967159277504105,
      "grad_norm": 10.185110092163086,
      "learning_rate": 2.022977511558638e-05,
      "loss": 0.4932,
      "num_input_tokens_seen": 483840,
      "step": 605
    },
    {
      "epoch": 0.4975369458128079,
      "grad_norm": 12.345698356628418,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 0.6644,
      "num_input_tokens_seen": 484608,
      "step": 606
    },
    {
      "epoch": 0.49835796387520526,
      "grad_norm": 12.99691104888916,
      "learning_rate": 2.0058566490521847e-05,
      "loss": 0.5531,
      "num_input_tokens_seen": 485376,
      "step": 607
    },
    {
      "epoch": 0.49917898193760263,
      "grad_norm": 9.46725082397461,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 0.5381,
      "num_input_tokens_seen": 486144,
      "step": 608
    },
    {
      "epoch": 0.5,
      "grad_norm": 12.862040519714355,
      "learning_rate": 1.9887598703955242e-05,
      "loss": 0.4372,
      "num_input_tokens_seen": 486912,
      "step": 609
    },
    {
      "epoch": 0.5008210180623974,
      "grad_norm": 40.46846389770508,
      "learning_rate": 1.980220772955602e-05,
      "loss": 1.3931,
      "num_input_tokens_seen": 487680,
      "step": 610
    },
    {
      "epoch": 0.5016420361247947,
      "grad_norm": 18.09882354736328,
      "learning_rate": 1.9716880088615285e-05,
      "loss": 0.9708,
      "num_input_tokens_seen": 488448,
      "step": 611
    },
    {
      "epoch": 0.5024630541871922,
      "grad_norm": 16.49165916442871,
      "learning_rate": 1.963161682082342e-05,
      "loss": 0.6574,
      "num_input_tokens_seen": 489216,
      "step": 612
    },
    {
      "epoch": 0.5032840722495895,
      "grad_norm": 19.508169174194336,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 0.9639,
      "num_input_tokens_seen": 489984,
      "step": 613
    },
    {
      "epoch": 0.5041050903119869,
      "grad_norm": 21.472043991088867,
      "learning_rate": 1.946128755951332e-05,
      "loss": 1.6692,
      "num_input_tokens_seen": 490752,
      "step": 614
    },
    {
      "epoch": 0.5049261083743842,
      "grad_norm": 8.758853912353516,
      "learning_rate": 1.937622364140338e-05,
      "loss": 0.5791,
      "num_input_tokens_seen": 491520,
      "step": 615
    },
    {
      "epoch": 0.5057471264367817,
      "grad_norm": 8.41061782836914,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 0.2097,
      "num_input_tokens_seen": 492288,
      "step": 616
    },
    {
      "epoch": 0.506568144499179,
      "grad_norm": 16.618436813354492,
      "learning_rate": 1.920630241264607e-05,
      "loss": 0.9031,
      "num_input_tokens_seen": 493056,
      "step": 617
    },
    {
      "epoch": 0.5073891625615764,
      "grad_norm": 21.85169219970703,
      "learning_rate": 1.912144717243525e-05,
      "loss": 1.2967,
      "num_input_tokens_seen": 493824,
      "step": 618
    },
    {
      "epoch": 0.5082101806239737,
      "grad_norm": 9.353875160217285,
      "learning_rate": 1.9036663560535483e-05,
      "loss": 0.2469,
      "num_input_tokens_seen": 494592,
      "step": 619
    },
    {
      "epoch": 0.5090311986863711,
      "grad_norm": 17.885488510131836,
      "learning_rate": 1.895195261000831e-05,
      "loss": 0.8571,
      "num_input_tokens_seen": 495360,
      "step": 620
    },
    {
      "epoch": 0.5098522167487685,
      "grad_norm": 17.208372116088867,
      "learning_rate": 1.8867315353029935e-05,
      "loss": 1.4538,
      "num_input_tokens_seen": 496384,
      "step": 621
    },
    {
      "epoch": 0.5106732348111659,
      "grad_norm": 6.30324649810791,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 0.2421,
      "num_input_tokens_seen": 497152,
      "step": 622
    },
    {
      "epoch": 0.5114942528735632,
      "grad_norm": 15.563956260681152,
      "learning_rate": 1.869826604392216e-05,
      "loss": 0.7201,
      "num_input_tokens_seen": 497920,
      "step": 623
    },
    {
      "epoch": 0.5123152709359606,
      "grad_norm": 7.875722408294678,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 0.2583,
      "num_input_tokens_seen": 498816,
      "step": 624
    },
    {
      "epoch": 0.513136288998358,
      "grad_norm": 25.690170288085938,
      "learning_rate": 1.852952387243698e-05,
      "loss": 1.0111,
      "num_input_tokens_seen": 499584,
      "step": 625
    },
    {
      "epoch": 0.5139573070607554,
      "grad_norm": 22.306533813476562,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 0.7313,
      "num_input_tokens_seen": 500352,
      "step": 626
    },
    {
      "epoch": 0.5147783251231527,
      "grad_norm": 10.485552787780762,
      "learning_rate": 1.8361097062829778e-05,
      "loss": 1.0302,
      "num_input_tokens_seen": 501120,
      "step": 627
    },
    {
      "epoch": 0.5155993431855501,
      "grad_norm": 15.560280799865723,
      "learning_rate": 1.827700448461836e-05,
      "loss": 1.0196,
      "num_input_tokens_seen": 501888,
      "step": 628
    },
    {
      "epoch": 0.5164203612479474,
      "grad_norm": 11.604126930236816,
      "learning_rate": 1.8192993823985643e-05,
      "loss": 0.6785,
      "num_input_tokens_seen": 502656,
      "step": 629
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 19.722013473510742,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 0.8462,
      "num_input_tokens_seen": 503424,
      "step": 630
    },
    {
      "epoch": 0.5180623973727422,
      "grad_norm": 5.896917343139648,
      "learning_rate": 1.802522234901927e-05,
      "loss": 0.1336,
      "num_input_tokens_seen": 504192,
      "step": 631
    },
    {
      "epoch": 0.5188834154351396,
      "grad_norm": 5.2399163246154785,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 0.1366,
      "num_input_tokens_seen": 504960,
      "step": 632
    },
    {
      "epoch": 0.5197044334975369,
      "grad_norm": 15.046367645263672,
      "learning_rate": 1.7857790814875663e-05,
      "loss": 0.5747,
      "num_input_tokens_seen": 505728,
      "step": 633
    },
    {
      "epoch": 0.5205254515599343,
      "grad_norm": 13.186470985412598,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 0.6358,
      "num_input_tokens_seen": 506496,
      "step": 634
    },
    {
      "epoch": 0.5213464696223317,
      "grad_norm": 17.453603744506836,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 1.1278,
      "num_input_tokens_seen": 507264,
      "step": 635
    },
    {
      "epoch": 0.5221674876847291,
      "grad_norm": 7.910717487335205,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 0.3638,
      "num_input_tokens_seen": 508032,
      "step": 636
    },
    {
      "epoch": 0.5229885057471264,
      "grad_norm": 11.442970275878906,
      "learning_rate": 1.7523980193597836e-05,
      "loss": 0.5164,
      "num_input_tokens_seen": 508928,
      "step": 637
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 10.953275680541992,
      "learning_rate": 1.744075273123889e-05,
      "loss": 0.521,
      "num_input_tokens_seen": 509696,
      "step": 638
    },
    {
      "epoch": 0.5246305418719212,
      "grad_norm": 15.269137382507324,
      "learning_rate": 1.735761737592236e-05,
      "loss": 0.5973,
      "num_input_tokens_seen": 510720,
      "step": 639
    },
    {
      "epoch": 0.5254515599343186,
      "grad_norm": 9.791973114013672,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 0.585,
      "num_input_tokens_seen": 511488,
      "step": 640
    },
    {
      "epoch": 0.5262725779967159,
      "grad_norm": 13.213040351867676,
      "learning_rate": 1.7191627037194186e-05,
      "loss": 0.7251,
      "num_input_tokens_seen": 512256,
      "step": 641
    },
    {
      "epoch": 0.5270935960591133,
      "grad_norm": 5.0776190757751465,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 0.0695,
      "num_input_tokens_seen": 513024,
      "step": 642
    },
    {
      "epoch": 0.5279146141215106,
      "grad_norm": 9.579024314880371,
      "learning_rate": 1.702601726754825e-05,
      "loss": 0.4458,
      "num_input_tokens_seen": 513792,
      "step": 643
    },
    {
      "epoch": 0.5287356321839081,
      "grad_norm": 8.566150665283203,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 0.6782,
      "num_input_tokens_seen": 514560,
      "step": 644
    },
    {
      "epoch": 0.5295566502463054,
      "grad_norm": 13.493000984191895,
      "learning_rate": 1.686079613857109e-05,
      "loss": 0.845,
      "num_input_tokens_seen": 515328,
      "step": 645
    },
    {
      "epoch": 0.5303776683087028,
      "grad_norm": 13.403664588928223,
      "learning_rate": 1.677833383153542e-05,
      "loss": 0.6383,
      "num_input_tokens_seen": 516224,
      "step": 646
    },
    {
      "epoch": 0.5311986863711001,
      "grad_norm": 10.834308624267578,
      "learning_rate": 1.6695971702907426e-05,
      "loss": 0.6644,
      "num_input_tokens_seen": 516992,
      "step": 647
    },
    {
      "epoch": 0.5320197044334976,
      "grad_norm": 7.104128837585449,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 0.3891,
      "num_input_tokens_seen": 517760,
      "step": 648
    },
    {
      "epoch": 0.5328407224958949,
      "grad_norm": 13.889813423156738,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 0.4565,
      "num_input_tokens_seen": 518528,
      "step": 649
    },
    {
      "epoch": 0.5336617405582923,
      "grad_norm": 15.822524070739746,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 0.7697,
      "num_input_tokens_seen": 519296,
      "step": 650
    },
    {
      "epoch": 0.5344827586206896,
      "grad_norm": 3.774747133255005,
      "learning_rate": 1.6367545025036636e-05,
      "loss": 0.1126,
      "num_input_tokens_seen": 520064,
      "step": 651
    },
    {
      "epoch": 0.535303776683087,
      "grad_norm": 11.168807983398438,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 0.6325,
      "num_input_tokens_seen": 520832,
      "step": 652
    },
    {
      "epoch": 0.5361247947454844,
      "grad_norm": 7.08775520324707,
      "learning_rate": 1.6203958789882456e-05,
      "loss": 0.3553,
      "num_input_tokens_seen": 521600,
      "step": 653
    },
    {
      "epoch": 0.5369458128078818,
      "grad_norm": 11.789356231689453,
      "learning_rate": 1.612232593979658e-05,
      "loss": 1.0804,
      "num_input_tokens_seen": 522496,
      "step": 654
    },
    {
      "epoch": 0.5377668308702791,
      "grad_norm": 17.59056854248047,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 1.0661,
      "num_input_tokens_seen": 523264,
      "step": 655
    },
    {
      "epoch": 0.5385878489326765,
      "grad_norm": 6.367064476013184,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 0.3568,
      "num_input_tokens_seen": 524032,
      "step": 656
    },
    {
      "epoch": 0.5394088669950738,
      "grad_norm": 18.409990310668945,
      "learning_rate": 1.5878080391559508e-05,
      "loss": 1.055,
      "num_input_tokens_seen": 524800,
      "step": 657
    },
    {
      "epoch": 0.5402298850574713,
      "grad_norm": 19.968725204467773,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 0.8925,
      "num_input_tokens_seen": 525696,
      "step": 658
    },
    {
      "epoch": 0.5410509031198686,
      "grad_norm": 5.877447605133057,
      "learning_rate": 1.5715804111244137e-05,
      "loss": 0.182,
      "num_input_tokens_seen": 526592,
      "step": 659
    },
    {
      "epoch": 0.541871921182266,
      "grad_norm": 10.890539169311523,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.4415,
      "num_input_tokens_seen": 527360,
      "step": 660
    },
    {
      "epoch": 0.5426929392446633,
      "grad_norm": 11.548998832702637,
      "learning_rate": 1.5553980329538326e-05,
      "loss": 0.4455,
      "num_input_tokens_seen": 528256,
      "step": 661
    },
    {
      "epoch": 0.5435139573070608,
      "grad_norm": 12.781478881835938,
      "learning_rate": 1.547324059124315e-05,
      "loss": 0.4193,
      "num_input_tokens_seen": 529024,
      "step": 662
    },
    {
      "epoch": 0.5443349753694581,
      "grad_norm": 13.306100845336914,
      "learning_rate": 1.539261693350491e-05,
      "loss": 0.5813,
      "num_input_tokens_seen": 529792,
      "step": 663
    },
    {
      "epoch": 0.5451559934318555,
      "grad_norm": 11.942788124084473,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 0.7395,
      "num_input_tokens_seen": 530560,
      "step": 664
    },
    {
      "epoch": 0.5459770114942529,
      "grad_norm": 9.144354820251465,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.3357,
      "num_input_tokens_seen": 531328,
      "step": 665
    },
    {
      "epoch": 0.5467980295566502,
      "grad_norm": 16.988628387451172,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 1.3283,
      "num_input_tokens_seen": 532096,
      "step": 666
    },
    {
      "epoch": 0.5476190476190477,
      "grad_norm": 7.268435478210449,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 0.2024,
      "num_input_tokens_seen": 532864,
      "step": 667
    },
    {
      "epoch": 0.548440065681445,
      "grad_norm": 22.14447784423828,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 0.7858,
      "num_input_tokens_seen": 533632,
      "step": 668
    },
    {
      "epoch": 0.5492610837438424,
      "grad_norm": 10.406183242797852,
      "learning_rate": 1.4911367591190248e-05,
      "loss": 0.4735,
      "num_input_tokens_seen": 534400,
      "step": 669
    },
    {
      "epoch": 0.5500821018062397,
      "grad_norm": 7.8767547607421875,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 0.3468,
      "num_input_tokens_seen": 535168,
      "step": 670
    },
    {
      "epoch": 0.5509031198686372,
      "grad_norm": 14.436066627502441,
      "learning_rate": 1.475192415396068e-05,
      "loss": 0.5233,
      "num_input_tokens_seen": 536064,
      "step": 671
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 19.841331481933594,
      "learning_rate": 1.467238925438646e-05,
      "loss": 0.7072,
      "num_input_tokens_seen": 536832,
      "step": 672
    },
    {
      "epoch": 0.5525451559934319,
      "grad_norm": 10.822736740112305,
      "learning_rate": 1.4592980193489975e-05,
      "loss": 0.5504,
      "num_input_tokens_seen": 537600,
      "step": 673
    },
    {
      "epoch": 0.5533661740558292,
      "grad_norm": 11.308760643005371,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 0.4687,
      "num_input_tokens_seen": 538496,
      "step": 674
    },
    {
      "epoch": 0.5541871921182266,
      "grad_norm": 6.667705059051514,
      "learning_rate": 1.443454345648252e-05,
      "loss": 0.1936,
      "num_input_tokens_seen": 539264,
      "step": 675
    },
    {
      "epoch": 0.555008210180624,
      "grad_norm": 11.915234565734863,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 0.8265,
      "num_input_tokens_seen": 540160,
      "step": 676
    },
    {
      "epoch": 0.5558292282430214,
      "grad_norm": 12.398269653320312,
      "learning_rate": 1.4276621664921357e-05,
      "loss": 0.2358,
      "num_input_tokens_seen": 540928,
      "step": 677
    },
    {
      "epoch": 0.5566502463054187,
      "grad_norm": 12.447604179382324,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 0.7679,
      "num_input_tokens_seen": 541824,
      "step": 678
    },
    {
      "epoch": 0.5574712643678161,
      "grad_norm": 4.760353088378906,
      "learning_rate": 1.4119222515691816e-05,
      "loss": 0.2096,
      "num_input_tokens_seen": 542592,
      "step": 679
    },
    {
      "epoch": 0.5582922824302134,
      "grad_norm": 6.501363277435303,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.1662,
      "num_input_tokens_seen": 543360,
      "step": 680
    },
    {
      "epoch": 0.5591133004926109,
      "grad_norm": 5.6684746742248535,
      "learning_rate": 1.3962353680206373e-05,
      "loss": 0.2293,
      "num_input_tokens_seen": 544128,
      "step": 681
    },
    {
      "epoch": 0.5599343185550082,
      "grad_norm": 15.700751304626465,
      "learning_rate": 1.388412052037682e-05,
      "loss": 0.5777,
      "num_input_tokens_seen": 545024,
      "step": 682
    },
    {
      "epoch": 0.5607553366174056,
      "grad_norm": 12.240194320678711,
      "learning_rate": 1.380602280403076e-05,
      "loss": 0.4525,
      "num_input_tokens_seen": 545792,
      "step": 683
    },
    {
      "epoch": 0.5615763546798029,
      "grad_norm": 6.948907852172852,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 0.1119,
      "num_input_tokens_seen": 546560,
      "step": 684
    },
    {
      "epoch": 0.5623973727422004,
      "grad_norm": 12.202445030212402,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 0.4784,
      "num_input_tokens_seen": 547328,
      "step": 685
    },
    {
      "epoch": 0.5632183908045977,
      "grad_norm": 4.695610523223877,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 0.112,
      "num_input_tokens_seen": 548096,
      "step": 686
    },
    {
      "epoch": 0.5640394088669951,
      "grad_norm": 11.025063514709473,
      "learning_rate": 1.349500538040371e-05,
      "loss": 0.5148,
      "num_input_tokens_seen": 548864,
      "step": 687
    },
    {
      "epoch": 0.5648604269293924,
      "grad_norm": 8.755101203918457,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 0.1374,
      "num_input_tokens_seen": 549760,
      "step": 688
    },
    {
      "epoch": 0.5656814449917899,
      "grad_norm": 12.3419189453125,
      "learning_rate": 1.3340333991502724e-05,
      "loss": 0.2482,
      "num_input_tokens_seen": 550528,
      "step": 689
    },
    {
      "epoch": 0.5665024630541872,
      "grad_norm": 4.776583194732666,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 0.1085,
      "num_input_tokens_seen": 551296,
      "step": 690
    },
    {
      "epoch": 0.5673234811165846,
      "grad_norm": 20.36983871459961,
      "learning_rate": 1.3186230878273653e-05,
      "loss": 0.6412,
      "num_input_tokens_seen": 552064,
      "step": 691
    },
    {
      "epoch": 0.5681444991789819,
      "grad_norm": 19.001792907714844,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 0.7814,
      "num_input_tokens_seen": 552960,
      "step": 692
    },
    {
      "epoch": 0.5689655172413793,
      "grad_norm": 14.094453811645508,
      "learning_rate": 1.3032703551484832e-05,
      "loss": 0.7397,
      "num_input_tokens_seen": 553728,
      "step": 693
    },
    {
      "epoch": 0.5697865353037767,
      "grad_norm": 33.212581634521484,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 1.6096,
      "num_input_tokens_seen": 554752,
      "step": 694
    },
    {
      "epoch": 0.5706075533661741,
      "grad_norm": 6.139265060424805,
      "learning_rate": 1.2879759493841575e-05,
      "loss": 0.1474,
      "num_input_tokens_seen": 555648,
      "step": 695
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 9.256012916564941,
      "learning_rate": 1.280350852153168e-05,
      "loss": 0.3005,
      "num_input_tokens_seen": 556416,
      "step": 696
    },
    {
      "epoch": 0.5722495894909688,
      "grad_norm": 16.619873046875,
      "learning_rate": 1.272740615962148e-05,
      "loss": 0.6142,
      "num_input_tokens_seen": 557312,
      "step": 697
    },
    {
      "epoch": 0.5730706075533661,
      "grad_norm": 9.116068840026855,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 0.4505,
      "num_input_tokens_seen": 558208,
      "step": 698
    },
    {
      "epoch": 0.5738916256157636,
      "grad_norm": 21.045791625976562,
      "learning_rate": 1.2575650974311119e-05,
      "loss": 1.0634,
      "num_input_tokens_seen": 559104,
      "step": 699
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 12.657514572143555,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 0.5357,
      "num_input_tokens_seen": 560000,
      "step": 700
    },
    {
      "epoch": 0.5755336617405583,
      "grad_norm": 13.635184288024902,
      "learning_rate": 1.2424501334244123e-05,
      "loss": 0.6381,
      "num_input_tokens_seen": 560896,
      "step": 701
    },
    {
      "epoch": 0.5763546798029556,
      "grad_norm": 13.654897689819336,
      "learning_rate": 1.234915589697091e-05,
      "loss": 0.6326,
      "num_input_tokens_seen": 561664,
      "step": 702
    },
    {
      "epoch": 0.577175697865353,
      "grad_norm": 16.510208129882812,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 0.3837,
      "num_input_tokens_seen": 562560,
      "step": 703
    },
    {
      "epoch": 0.5779967159277504,
      "grad_norm": 5.649799346923828,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 0.1066,
      "num_input_tokens_seen": 563456,
      "step": 704
    },
    {
      "epoch": 0.5788177339901478,
      "grad_norm": 14.378872871398926,
      "learning_rate": 1.2124048127248644e-05,
      "loss": 0.5922,
      "num_input_tokens_seen": 564224,
      "step": 705
    },
    {
      "epoch": 0.5796387520525451,
      "grad_norm": 11.059441566467285,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 0.3323,
      "num_input_tokens_seen": 565120,
      "step": 706
    },
    {
      "epoch": 0.5804597701149425,
      "grad_norm": 10.863810539245605,
      "learning_rate": 1.19747592039856e-05,
      "loss": 0.2632,
      "num_input_tokens_seen": 565888,
      "step": 707
    },
    {
      "epoch": 0.5812807881773399,
      "grad_norm": 24.143218994140625,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 1.6234,
      "num_input_tokens_seen": 566656,
      "step": 708
    },
    {
      "epoch": 0.5821018062397373,
      "grad_norm": 15.994134902954102,
      "learning_rate": 1.1826105112583061e-05,
      "loss": 0.8009,
      "num_input_tokens_seen": 567424,
      "step": 709
    },
    {
      "epoch": 0.5829228243021346,
      "grad_norm": 17.13608741760254,
      "learning_rate": 1.175201839416988e-05,
      "loss": 0.4017,
      "num_input_tokens_seen": 568192,
      "step": 710
    },
    {
      "epoch": 0.583743842364532,
      "grad_norm": 20.677505493164062,
      "learning_rate": 1.167809309823175e-05,
      "loss": 0.776,
      "num_input_tokens_seen": 569088,
      "step": 711
    },
    {
      "epoch": 0.5845648604269293,
      "grad_norm": 19.234638214111328,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 0.8957,
      "num_input_tokens_seen": 569984,
      "step": 712
    },
    {
      "epoch": 0.5853858784893268,
      "grad_norm": 19.043882369995117,
      "learning_rate": 1.1530730374828422e-05,
      "loss": 1.0353,
      "num_input_tokens_seen": 570752,
      "step": 713
    },
    {
      "epoch": 0.5862068965517241,
      "grad_norm": 13.766763687133789,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 0.3277,
      "num_input_tokens_seen": 571520,
      "step": 714
    },
    {
      "epoch": 0.5870279146141215,
      "grad_norm": 6.753744125366211,
      "learning_rate": 1.1384024124624324e-05,
      "loss": 0.218,
      "num_input_tokens_seen": 572288,
      "step": 715
    },
    {
      "epoch": 0.5878489326765188,
      "grad_norm": 10.108755111694336,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 0.1397,
      "num_input_tokens_seen": 573056,
      "step": 716
    },
    {
      "epoch": 0.5886699507389163,
      "grad_norm": 9.03653621673584,
      "learning_rate": 1.123798149787511e-05,
      "loss": 0.3395,
      "num_input_tokens_seen": 573824,
      "step": 717
    },
    {
      "epoch": 0.5894909688013136,
      "grad_norm": 13.248714447021484,
      "learning_rate": 1.11652112689164e-05,
      "loss": 1.0425,
      "num_input_tokens_seen": 574720,
      "step": 718
    },
    {
      "epoch": 0.590311986863711,
      "grad_norm": 10.94626522064209,
      "learning_rate": 1.109260961249238e-05,
      "loss": 0.419,
      "num_input_tokens_seen": 575488,
      "step": 719
    },
    {
      "epoch": 0.5911330049261084,
      "grad_norm": 15.023051261901855,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 0.3612,
      "num_input_tokens_seen": 576384,
      "step": 720
    },
    {
      "epoch": 0.5919540229885057,
      "grad_norm": 9.8744535446167,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 0.2138,
      "num_input_tokens_seen": 577152,
      "step": 721
    },
    {
      "epoch": 0.5927750410509032,
      "grad_norm": 11.52042293548584,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 0.3782,
      "num_input_tokens_seen": 577920,
      "step": 722
    },
    {
      "epoch": 0.5935960591133005,
      "grad_norm": 18.073863983154297,
      "learning_rate": 1.0803906373672476e-05,
      "loss": 0.9192,
      "num_input_tokens_seen": 578688,
      "step": 723
    },
    {
      "epoch": 0.5944170771756979,
      "grad_norm": 9.397892951965332,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 0.6327,
      "num_input_tokens_seen": 579456,
      "step": 724
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 25.664960861206055,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 1.76,
      "num_input_tokens_seen": 580224,
      "step": 725
    },
    {
      "epoch": 0.5960591133004927,
      "grad_norm": 18.81505012512207,
      "learning_rate": 1.058919209575517e-05,
      "loss": 0.5487,
      "num_input_tokens_seen": 580992,
      "step": 726
    },
    {
      "epoch": 0.59688013136289,
      "grad_norm": 4.503079414367676,
      "learning_rate": 1.0517970691433035e-05,
      "loss": 0.0568,
      "num_input_tokens_seen": 581760,
      "step": 727
    },
    {
      "epoch": 0.5977011494252874,
      "grad_norm": 7.583625793457031,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 0.2576,
      "num_input_tokens_seen": 582528,
      "step": 728
    },
    {
      "epoch": 0.5985221674876847,
      "grad_norm": 18.939842224121094,
      "learning_rate": 1.0376058125319613e-05,
      "loss": 0.5381,
      "num_input_tokens_seen": 583296,
      "step": 729
    },
    {
      "epoch": 0.5993431855500821,
      "grad_norm": 8.853614807128906,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 0.5492,
      "num_input_tokens_seen": 584064,
      "step": 730
    },
    {
      "epoch": 0.6001642036124795,
      "grad_norm": 12.092466354370117,
      "learning_rate": 1.0234858309501862e-05,
      "loss": 0.6871,
      "num_input_tokens_seen": 584832,
      "step": 731
    },
    {
      "epoch": 0.6009852216748769,
      "grad_norm": 8.781827926635742,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 0.2995,
      "num_input_tokens_seen": 585856,
      "step": 732
    },
    {
      "epoch": 0.6018062397372742,
      "grad_norm": 11.234268188476562,
      "learning_rate": 1.0094378125859602e-05,
      "loss": 0.2738,
      "num_input_tokens_seen": 586624,
      "step": 733
    },
    {
      "epoch": 0.6026272577996716,
      "grad_norm": 15.854637145996094,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 0.4022,
      "num_input_tokens_seen": 587392,
      "step": 734
    },
    {
      "epoch": 0.603448275862069,
      "grad_norm": 17.966306686401367,
      "learning_rate": 9.954624421198792e-06,
      "loss": 1.095,
      "num_input_tokens_seen": 588160,
      "step": 735
    },
    {
      "epoch": 0.6042692939244664,
      "grad_norm": 5.156599044799805,
      "learning_rate": 9.88502212844063e-06,
      "loss": 0.2209,
      "num_input_tokens_seen": 588928,
      "step": 736
    },
    {
      "epoch": 0.6050903119868637,
      "grad_norm": 10.76752758026123,
      "learning_rate": 9.815604006917839e-06,
      "loss": 0.5935,
      "num_input_tokens_seen": 589696,
      "step": 737
    },
    {
      "epoch": 0.6059113300492611,
      "grad_norm": 17.721330642700195,
      "learning_rate": 9.746370902468311e-06,
      "loss": 0.3864,
      "num_input_tokens_seen": 590464,
      "step": 738
    },
    {
      "epoch": 0.6067323481116584,
      "grad_norm": 6.223318099975586,
      "learning_rate": 9.677323658675594e-06,
      "loss": 0.1773,
      "num_input_tokens_seen": 591232,
      "step": 739
    },
    {
      "epoch": 0.6075533661740559,
      "grad_norm": 13.24976634979248,
      "learning_rate": 9.608463116858542e-06,
      "loss": 0.8204,
      "num_input_tokens_seen": 592000,
      "step": 740
    },
    {
      "epoch": 0.6083743842364532,
      "grad_norm": 11.552679061889648,
      "learning_rate": 9.539790116061151e-06,
      "loss": 0.8112,
      "num_input_tokens_seen": 592768,
      "step": 741
    },
    {
      "epoch": 0.6091954022988506,
      "grad_norm": 16.80919647216797,
      "learning_rate": 9.471305493042243e-06,
      "loss": 1.0678,
      "num_input_tokens_seen": 593664,
      "step": 742
    },
    {
      "epoch": 0.6100164203612479,
      "grad_norm": 11.493758201599121,
      "learning_rate": 9.403010082265351e-06,
      "loss": 0.3999,
      "num_input_tokens_seen": 594432,
      "step": 743
    },
    {
      "epoch": 0.6108374384236454,
      "grad_norm": 18.033809661865234,
      "learning_rate": 9.334904715888495e-06,
      "loss": 0.4392,
      "num_input_tokens_seen": 595200,
      "step": 744
    },
    {
      "epoch": 0.6116584564860427,
      "grad_norm": 20.468229293823242,
      "learning_rate": 9.266990223754069e-06,
      "loss": 1.1675,
      "num_input_tokens_seen": 595968,
      "step": 745
    },
    {
      "epoch": 0.6124794745484401,
      "grad_norm": 7.539775371551514,
      "learning_rate": 9.199267433378727e-06,
      "loss": 0.484,
      "num_input_tokens_seen": 596736,
      "step": 746
    },
    {
      "epoch": 0.6133004926108374,
      "grad_norm": 13.308883666992188,
      "learning_rate": 9.131737169943314e-06,
      "loss": 0.5512,
      "num_input_tokens_seen": 597504,
      "step": 747
    },
    {
      "epoch": 0.6141215106732348,
      "grad_norm": 6.740693092346191,
      "learning_rate": 9.064400256282757e-06,
      "loss": 0.2318,
      "num_input_tokens_seen": 598144,
      "step": 748
    },
    {
      "epoch": 0.6149425287356322,
      "grad_norm": 8.502015113830566,
      "learning_rate": 8.997257512876108e-06,
      "loss": 0.2436,
      "num_input_tokens_seen": 598912,
      "step": 749
    },
    {
      "epoch": 0.6157635467980296,
      "grad_norm": 6.02455997467041,
      "learning_rate": 8.930309757836517e-06,
      "loss": 0.217,
      "num_input_tokens_seen": 599680,
      "step": 750
    },
    {
      "epoch": 0.6165845648604269,
      "grad_norm": 10.970537185668945,
      "learning_rate": 8.863557806901233e-06,
      "loss": 0.4584,
      "num_input_tokens_seen": 600448,
      "step": 751
    },
    {
      "epoch": 0.6174055829228243,
      "grad_norm": 12.054771423339844,
      "learning_rate": 8.797002473421728e-06,
      "loss": 0.4943,
      "num_input_tokens_seen": 601216,
      "step": 752
    },
    {
      "epoch": 0.6182266009852216,
      "grad_norm": 12.44386100769043,
      "learning_rate": 8.73064456835373e-06,
      "loss": 0.3053,
      "num_input_tokens_seen": 602112,
      "step": 753
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 8.519200325012207,
      "learning_rate": 8.664484900247363e-06,
      "loss": 0.4327,
      "num_input_tokens_seen": 602880,
      "step": 754
    },
    {
      "epoch": 0.6198686371100164,
      "grad_norm": 17.479578018188477,
      "learning_rate": 8.598524275237322e-06,
      "loss": 0.6469,
      "num_input_tokens_seen": 603648,
      "step": 755
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 8.598607063293457,
      "learning_rate": 8.532763497032987e-06,
      "loss": 0.2228,
      "num_input_tokens_seen": 604544,
      "step": 756
    },
    {
      "epoch": 0.6215106732348111,
      "grad_norm": 22.684249877929688,
      "learning_rate": 8.467203366908707e-06,
      "loss": 1.3339,
      "num_input_tokens_seen": 605440,
      "step": 757
    },
    {
      "epoch": 0.6223316912972086,
      "grad_norm": 16.02072525024414,
      "learning_rate": 8.40184468369396e-06,
      "loss": 0.6865,
      "num_input_tokens_seen": 606208,
      "step": 758
    },
    {
      "epoch": 0.6231527093596059,
      "grad_norm": 10.03339672088623,
      "learning_rate": 8.33668824376369e-06,
      "loss": 0.3554,
      "num_input_tokens_seen": 606976,
      "step": 759
    },
    {
      "epoch": 0.6239737274220033,
      "grad_norm": 5.5323686599731445,
      "learning_rate": 8.271734841028553e-06,
      "loss": 0.1214,
      "num_input_tokens_seen": 607744,
      "step": 760
    },
    {
      "epoch": 0.6247947454844006,
      "grad_norm": 19.206205368041992,
      "learning_rate": 8.206985266925249e-06,
      "loss": 1.0092,
      "num_input_tokens_seen": 608512,
      "step": 761
    },
    {
      "epoch": 0.625615763546798,
      "grad_norm": 4.793747901916504,
      "learning_rate": 8.142440310406924e-06,
      "loss": 0.0806,
      "num_input_tokens_seen": 609280,
      "step": 762
    },
    {
      "epoch": 0.6264367816091954,
      "grad_norm": 11.033464431762695,
      "learning_rate": 8.078100757933485e-06,
      "loss": 0.3062,
      "num_input_tokens_seen": 610176,
      "step": 763
    },
    {
      "epoch": 0.6272577996715928,
      "grad_norm": 14.205730438232422,
      "learning_rate": 8.013967393462094e-06,
      "loss": 0.2503,
      "num_input_tokens_seen": 610944,
      "step": 764
    },
    {
      "epoch": 0.6280788177339901,
      "grad_norm": 21.00545883178711,
      "learning_rate": 7.950040998437542e-06,
      "loss": 1.2594,
      "num_input_tokens_seen": 611712,
      "step": 765
    },
    {
      "epoch": 0.6288998357963875,
      "grad_norm": 11.024002075195312,
      "learning_rate": 7.886322351782783e-06,
      "loss": 0.3173,
      "num_input_tokens_seen": 612480,
      "step": 766
    },
    {
      "epoch": 0.6297208538587848,
      "grad_norm": 11.98373031616211,
      "learning_rate": 7.822812229889428e-06,
      "loss": 0.6302,
      "num_input_tokens_seen": 613248,
      "step": 767
    },
    {
      "epoch": 0.6305418719211823,
      "grad_norm": 17.05611228942871,
      "learning_rate": 7.759511406608255e-06,
      "loss": 0.6517,
      "num_input_tokens_seen": 614016,
      "step": 768
    },
    {
      "epoch": 0.6313628899835796,
      "grad_norm": 15.588370323181152,
      "learning_rate": 7.696420653239833e-06,
      "loss": 0.8073,
      "num_input_tokens_seen": 614784,
      "step": 769
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 13.69023609161377,
      "learning_rate": 7.633540738525066e-06,
      "loss": 0.4607,
      "num_input_tokens_seen": 615552,
      "step": 770
    },
    {
      "epoch": 0.6330049261083743,
      "grad_norm": 16.232036590576172,
      "learning_rate": 7.570872428635889e-06,
      "loss": 0.8803,
      "num_input_tokens_seen": 616320,
      "step": 771
    },
    {
      "epoch": 0.6338259441707718,
      "grad_norm": 17.875473022460938,
      "learning_rate": 7.508416487165862e-06,
      "loss": 0.9295,
      "num_input_tokens_seen": 617088,
      "step": 772
    },
    {
      "epoch": 0.6346469622331691,
      "grad_norm": 16.541425704956055,
      "learning_rate": 7.4461736751209405e-06,
      "loss": 1.2986,
      "num_input_tokens_seen": 617856,
      "step": 773
    },
    {
      "epoch": 0.6354679802955665,
      "grad_norm": 12.919188499450684,
      "learning_rate": 7.384144750910133e-06,
      "loss": 0.316,
      "num_input_tokens_seen": 618624,
      "step": 774
    },
    {
      "epoch": 0.6362889983579638,
      "grad_norm": 16.953022003173828,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 1.2481,
      "num_input_tokens_seen": 619648,
      "step": 775
    },
    {
      "epoch": 0.6371100164203612,
      "grad_norm": 20.123350143432617,
      "learning_rate": 7.260731586586983e-06,
      "loss": 0.9397,
      "num_input_tokens_seen": 620416,
      "step": 776
    },
    {
      "epoch": 0.6379310344827587,
      "grad_norm": 12.21571159362793,
      "learning_rate": 7.19934885022509e-06,
      "loss": 0.6924,
      "num_input_tokens_seen": 621184,
      "step": 777
    },
    {
      "epoch": 0.638752052545156,
      "grad_norm": 1.7455809116363525,
      "learning_rate": 7.138183009179922e-06,
      "loss": 0.0351,
      "num_input_tokens_seen": 622080,
      "step": 778
    },
    {
      "epoch": 0.6395730706075534,
      "grad_norm": 7.83803129196167,
      "learning_rate": 7.0772348087379315e-06,
      "loss": 0.1806,
      "num_input_tokens_seen": 622848,
      "step": 779
    },
    {
      "epoch": 0.6403940886699507,
      "grad_norm": 24.09235382080078,
      "learning_rate": 7.016504991533726e-06,
      "loss": 1.4734,
      "num_input_tokens_seen": 623616,
      "step": 780
    },
    {
      "epoch": 0.6412151067323482,
      "grad_norm": 7.762509346008301,
      "learning_rate": 6.9559942975409465e-06,
      "loss": 0.3514,
      "num_input_tokens_seen": 624384,
      "step": 781
    },
    {
      "epoch": 0.6420361247947455,
      "grad_norm": 16.870647430419922,
      "learning_rate": 6.895703464063319e-06,
      "loss": 0.3096,
      "num_input_tokens_seen": 625152,
      "step": 782
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 13.691892623901367,
      "learning_rate": 6.835633225725605e-06,
      "loss": 0.6154,
      "num_input_tokens_seen": 625920,
      "step": 783
    },
    {
      "epoch": 0.6436781609195402,
      "grad_norm": 14.52787971496582,
      "learning_rate": 6.775784314464717e-06,
      "loss": 0.634,
      "num_input_tokens_seen": 626688,
      "step": 784
    },
    {
      "epoch": 0.6444991789819376,
      "grad_norm": 18.659509658813477,
      "learning_rate": 6.716157459520739e-06,
      "loss": 0.7968,
      "num_input_tokens_seen": 627456,
      "step": 785
    },
    {
      "epoch": 0.645320197044335,
      "grad_norm": 12.659956932067871,
      "learning_rate": 6.656753387428089e-06,
      "loss": 0.5866,
      "num_input_tokens_seen": 628224,
      "step": 786
    },
    {
      "epoch": 0.6461412151067324,
      "grad_norm": 25.425138473510742,
      "learning_rate": 6.5975728220066425e-06,
      "loss": 1.3809,
      "num_input_tokens_seen": 628992,
      "step": 787
    },
    {
      "epoch": 0.6469622331691297,
      "grad_norm": 22.316009521484375,
      "learning_rate": 6.538616484352902e-06,
      "loss": 1.6026,
      "num_input_tokens_seen": 629760,
      "step": 788
    },
    {
      "epoch": 0.6477832512315271,
      "grad_norm": 6.4226460456848145,
      "learning_rate": 6.47988509283125e-06,
      "loss": 0.2911,
      "num_input_tokens_seen": 630528,
      "step": 789
    },
    {
      "epoch": 0.6486042692939245,
      "grad_norm": 11.51010799407959,
      "learning_rate": 6.421379363065142e-06,
      "loss": 0.8117,
      "num_input_tokens_seen": 631296,
      "step": 790
    },
    {
      "epoch": 0.6494252873563219,
      "grad_norm": 14.798226356506348,
      "learning_rate": 6.363100007928446e-06,
      "loss": 0.723,
      "num_input_tokens_seen": 632064,
      "step": 791
    },
    {
      "epoch": 0.6502463054187192,
      "grad_norm": 21.40264129638672,
      "learning_rate": 6.305047737536707e-06,
      "loss": 0.8705,
      "num_input_tokens_seen": 632832,
      "step": 792
    },
    {
      "epoch": 0.6510673234811166,
      "grad_norm": 17.111297607421875,
      "learning_rate": 6.247223259238511e-06,
      "loss": 0.6939,
      "num_input_tokens_seen": 633600,
      "step": 793
    },
    {
      "epoch": 0.6518883415435139,
      "grad_norm": 16.475217819213867,
      "learning_rate": 6.189627277606894e-06,
      "loss": 0.6859,
      "num_input_tokens_seen": 634496,
      "step": 794
    },
    {
      "epoch": 0.6527093596059114,
      "grad_norm": 9.558223724365234,
      "learning_rate": 6.1322604944307e-06,
      "loss": 0.4557,
      "num_input_tokens_seen": 635392,
      "step": 795
    },
    {
      "epoch": 0.6535303776683087,
      "grad_norm": 12.235757827758789,
      "learning_rate": 6.075123608706093e-06,
      "loss": 0.4248,
      "num_input_tokens_seen": 636160,
      "step": 796
    },
    {
      "epoch": 0.6543513957307061,
      "grad_norm": 9.377400398254395,
      "learning_rate": 6.01821731662798e-06,
      "loss": 0.2712,
      "num_input_tokens_seen": 636928,
      "step": 797
    },
    {
      "epoch": 0.6551724137931034,
      "grad_norm": 10.282675743103027,
      "learning_rate": 5.961542311581586e-06,
      "loss": 0.5883,
      "num_input_tokens_seen": 637696,
      "step": 798
    },
    {
      "epoch": 0.6559934318555009,
      "grad_norm": 21.26702308654785,
      "learning_rate": 5.905099284133952e-06,
      "loss": 0.7437,
      "num_input_tokens_seen": 638464,
      "step": 799
    },
    {
      "epoch": 0.6568144499178982,
      "grad_norm": 14.13918399810791,
      "learning_rate": 5.848888922025553e-06,
      "loss": 0.4697,
      "num_input_tokens_seen": 639232,
      "step": 800
    },
    {
      "epoch": 0.6576354679802956,
      "grad_norm": 3.2703168392181396,
      "learning_rate": 5.792911910161922e-06,
      "loss": 0.0896,
      "num_input_tokens_seen": 640128,
      "step": 801
    },
    {
      "epoch": 0.6584564860426929,
      "grad_norm": 24.97539710998535,
      "learning_rate": 5.737168930605272e-06,
      "loss": 1.348,
      "num_input_tokens_seen": 640896,
      "step": 802
    },
    {
      "epoch": 0.6592775041050903,
      "grad_norm": 12.00285530090332,
      "learning_rate": 5.681660662566224e-06,
      "loss": 0.6727,
      "num_input_tokens_seen": 641664,
      "step": 803
    },
    {
      "epoch": 0.6600985221674877,
      "grad_norm": 13.268601417541504,
      "learning_rate": 5.626387782395512e-06,
      "loss": 0.2653,
      "num_input_tokens_seen": 642432,
      "step": 804
    },
    {
      "epoch": 0.6609195402298851,
      "grad_norm": 7.94285249710083,
      "learning_rate": 5.571350963575728e-06,
      "loss": 0.2529,
      "num_input_tokens_seen": 643200,
      "step": 805
    },
    {
      "epoch": 0.6617405582922824,
      "grad_norm": 10.975334167480469,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 0.2361,
      "num_input_tokens_seen": 644224,
      "step": 806
    },
    {
      "epoch": 0.6625615763546798,
      "grad_norm": 16.474557876586914,
      "learning_rate": 5.461988189529529e-06,
      "loss": 0.5407,
      "num_input_tokens_seen": 644992,
      "step": 807
    },
    {
      "epoch": 0.6633825944170771,
      "grad_norm": 9.47301197052002,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 0.4848,
      "num_input_tokens_seen": 645760,
      "step": 808
    },
    {
      "epoch": 0.6642036124794746,
      "grad_norm": 18.02326202392578,
      "learning_rate": 5.3535776706149505e-06,
      "loss": 0.367,
      "num_input_tokens_seen": 646528,
      "step": 809
    },
    {
      "epoch": 0.6650246305418719,
      "grad_norm": 7.868067264556885,
      "learning_rate": 5.299731159831953e-06,
      "loss": 0.2137,
      "num_input_tokens_seen": 647296,
      "step": 810
    },
    {
      "epoch": 0.6658456486042693,
      "grad_norm": 16.58287239074707,
      "learning_rate": 5.24612469060774e-06,
      "loss": 0.8173,
      "num_input_tokens_seen": 648064,
      "step": 811
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 5.412960052490234,
      "learning_rate": 5.192758916120236e-06,
      "loss": 0.1278,
      "num_input_tokens_seen": 648832,
      "step": 812
    },
    {
      "epoch": 0.6674876847290641,
      "grad_norm": 15.664931297302246,
      "learning_rate": 5.139634486614544e-06,
      "loss": 0.4168,
      "num_input_tokens_seen": 649856,
      "step": 813
    },
    {
      "epoch": 0.6683087027914614,
      "grad_norm": 9.139263153076172,
      "learning_rate": 5.086752049395094e-06,
      "loss": 0.6153,
      "num_input_tokens_seen": 650624,
      "step": 814
    },
    {
      "epoch": 0.6691297208538588,
      "grad_norm": 16.20895767211914,
      "learning_rate": 5.034112248817685e-06,
      "loss": 1.0936,
      "num_input_tokens_seen": 651392,
      "step": 815
    },
    {
      "epoch": 0.6699507389162561,
      "grad_norm": 13.43810749053955,
      "learning_rate": 4.981715726281666e-06,
      "loss": 0.8194,
      "num_input_tokens_seen": 652160,
      "step": 816
    },
    {
      "epoch": 0.6707717569786535,
      "grad_norm": 5.181881904602051,
      "learning_rate": 4.929563120222141e-06,
      "loss": 0.0908,
      "num_input_tokens_seen": 653056,
      "step": 817
    },
    {
      "epoch": 0.6715927750410509,
      "grad_norm": 10.690244674682617,
      "learning_rate": 4.877655066102149e-06,
      "loss": 0.4585,
      "num_input_tokens_seen": 653824,
      "step": 818
    },
    {
      "epoch": 0.6724137931034483,
      "grad_norm": 19.1761417388916,
      "learning_rate": 4.825992196404957e-06,
      "loss": 0.8032,
      "num_input_tokens_seen": 654592,
      "step": 819
    },
    {
      "epoch": 0.6732348111658456,
      "grad_norm": 13.904353141784668,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 0.5635,
      "num_input_tokens_seen": 655488,
      "step": 820
    },
    {
      "epoch": 0.674055829228243,
      "grad_norm": 16.209501266479492,
      "learning_rate": 4.723404525266839e-06,
      "loss": 0.7429,
      "num_input_tokens_seen": 656256,
      "step": 821
    },
    {
      "epoch": 0.6748768472906403,
      "grad_norm": 22.3512020111084,
      "learning_rate": 4.672480973824311e-06,
      "loss": 1.6024,
      "num_input_tokens_seen": 657024,
      "step": 822
    },
    {
      "epoch": 0.6756978653530378,
      "grad_norm": 2.8547873497009277,
      "learning_rate": 4.621805106786142e-06,
      "loss": 0.0511,
      "num_input_tokens_seen": 657920,
      "step": 823
    },
    {
      "epoch": 0.6765188834154351,
      "grad_norm": 8.026897430419922,
      "learning_rate": 4.571377541621788e-06,
      "loss": 0.1792,
      "num_input_tokens_seen": 658688,
      "step": 824
    },
    {
      "epoch": 0.6773399014778325,
      "grad_norm": 14.977333068847656,
      "learning_rate": 4.521198892775203e-06,
      "loss": 1.0699,
      "num_input_tokens_seen": 659584,
      "step": 825
    },
    {
      "epoch": 0.6781609195402298,
      "grad_norm": 11.253602981567383,
      "learning_rate": 4.4712697716574e-06,
      "loss": 0.286,
      "num_input_tokens_seen": 660352,
      "step": 826
    },
    {
      "epoch": 0.6789819376026273,
      "grad_norm": 3.4636387825012207,
      "learning_rate": 4.421590786638951e-06,
      "loss": 0.0807,
      "num_input_tokens_seen": 661120,
      "step": 827
    },
    {
      "epoch": 0.6798029556650246,
      "grad_norm": 6.6934494972229,
      "learning_rate": 4.372162543042624e-06,
      "loss": 0.0796,
      "num_input_tokens_seen": 661888,
      "step": 828
    },
    {
      "epoch": 0.680623973727422,
      "grad_norm": 13.268082618713379,
      "learning_rate": 4.322985643135952e-06,
      "loss": 0.5306,
      "num_input_tokens_seen": 662656,
      "step": 829
    },
    {
      "epoch": 0.6814449917898193,
      "grad_norm": 2.543109655380249,
      "learning_rate": 4.274060686123959e-06,
      "loss": 0.0526,
      "num_input_tokens_seen": 663424,
      "step": 830
    },
    {
      "epoch": 0.6822660098522167,
      "grad_norm": 15.920268058776855,
      "learning_rate": 4.225388268141797e-06,
      "loss": 0.5468,
      "num_input_tokens_seen": 664192,
      "step": 831
    },
    {
      "epoch": 0.6830870279146142,
      "grad_norm": 17.902555465698242,
      "learning_rate": 4.176968982247514e-06,
      "loss": 0.555,
      "num_input_tokens_seen": 665088,
      "step": 832
    },
    {
      "epoch": 0.6839080459770115,
      "grad_norm": 7.358903408050537,
      "learning_rate": 4.128803418414839e-06,
      "loss": 0.1734,
      "num_input_tokens_seen": 665984,
      "step": 833
    },
    {
      "epoch": 0.6847290640394089,
      "grad_norm": 5.591375350952148,
      "learning_rate": 4.08089216352596e-06,
      "loss": 0.0893,
      "num_input_tokens_seen": 666752,
      "step": 834
    },
    {
      "epoch": 0.6855500821018062,
      "grad_norm": 20.50527000427246,
      "learning_rate": 4.0332358013644016e-06,
      "loss": 0.8662,
      "num_input_tokens_seen": 667520,
      "step": 835
    },
    {
      "epoch": 0.6863711001642037,
      "grad_norm": 13.423587799072266,
      "learning_rate": 3.985834912607894e-06,
      "loss": 0.7895,
      "num_input_tokens_seen": 668416,
      "step": 836
    },
    {
      "epoch": 0.687192118226601,
      "grad_norm": 16.899625778198242,
      "learning_rate": 3.938690074821313e-06,
      "loss": 1.1879,
      "num_input_tokens_seen": 669184,
      "step": 837
    },
    {
      "epoch": 0.6880131362889984,
      "grad_norm": 8.989087104797363,
      "learning_rate": 3.891801862449629e-06,
      "loss": 0.529,
      "num_input_tokens_seen": 669952,
      "step": 838
    },
    {
      "epoch": 0.6888341543513957,
      "grad_norm": 11.715303421020508,
      "learning_rate": 3.845170846810902e-06,
      "loss": 0.3216,
      "num_input_tokens_seen": 670848,
      "step": 839
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 16.48906135559082,
      "learning_rate": 3.798797596089351e-06,
      "loss": 0.5177,
      "num_input_tokens_seen": 671616,
      "step": 840
    },
    {
      "epoch": 0.6904761904761905,
      "grad_norm": 3.284104824066162,
      "learning_rate": 3.752682675328406e-06,
      "loss": 0.0691,
      "num_input_tokens_seen": 672512,
      "step": 841
    },
    {
      "epoch": 0.6912972085385879,
      "grad_norm": 10.754862785339355,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 0.3061,
      "num_input_tokens_seen": 673280,
      "step": 842
    },
    {
      "epoch": 0.6921182266009852,
      "grad_norm": 17.406005859375,
      "learning_rate": 3.661230068116811e-06,
      "loss": 0.8527,
      "num_input_tokens_seen": 674048,
      "step": 843
    },
    {
      "epoch": 0.6929392446633826,
      "grad_norm": 17.06417465209961,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 0.6698,
      "num_input_tokens_seen": 674816,
      "step": 844
    },
    {
      "epoch": 0.69376026272578,
      "grad_norm": 8.588855743408203,
      "learning_rate": 3.5708174824471947e-06,
      "loss": 0.165,
      "num_input_tokens_seen": 675584,
      "step": 845
    },
    {
      "epoch": 0.6945812807881774,
      "grad_norm": 11.074764251708984,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 0.3545,
      "num_input_tokens_seen": 676480,
      "step": 846
    },
    {
      "epoch": 0.6954022988505747,
      "grad_norm": 8.430343627929688,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 0.2246,
      "num_input_tokens_seen": 677248,
      "step": 847
    },
    {
      "epoch": 0.6962233169129721,
      "grad_norm": 17.44655990600586,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 0.6992,
      "num_input_tokens_seen": 678016,
      "step": 848
    },
    {
      "epoch": 0.6970443349753694,
      "grad_norm": 12.772637367248535,
      "learning_rate": 3.393129951157384e-06,
      "loss": 0.4106,
      "num_input_tokens_seen": 678912,
      "step": 849
    },
    {
      "epoch": 0.6978653530377669,
      "grad_norm": 9.26449203491211,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.3699,
      "num_input_tokens_seen": 679808,
      "step": 850
    },
    {
      "epoch": 0.6986863711001642,
      "grad_norm": 9.416847229003906,
      "learning_rate": 3.305863665776793e-06,
      "loss": 0.4427,
      "num_input_tokens_seen": 680704,
      "step": 851
    },
    {
      "epoch": 0.6995073891625616,
      "grad_norm": 13.104239463806152,
      "learning_rate": 3.262626762369525e-06,
      "loss": 0.4553,
      "num_input_tokens_seen": 681472,
      "step": 852
    },
    {
      "epoch": 0.7003284072249589,
      "grad_norm": 13.787586212158203,
      "learning_rate": 3.219654721995266e-06,
      "loss": 0.9166,
      "num_input_tokens_seen": 682240,
      "step": 853
    },
    {
      "epoch": 0.7011494252873564,
      "grad_norm": 9.459242820739746,
      "learning_rate": 3.176948068254762e-06,
      "loss": 0.2124,
      "num_input_tokens_seen": 683008,
      "step": 854
    },
    {
      "epoch": 0.7019704433497537,
      "grad_norm": 4.057446479797363,
      "learning_rate": 3.1345073215151066e-06,
      "loss": 0.0963,
      "num_input_tokens_seen": 683776,
      "step": 855
    },
    {
      "epoch": 0.7027914614121511,
      "grad_norm": 3.6170594692230225,
      "learning_rate": 3.092332998903416e-06,
      "loss": 0.0777,
      "num_input_tokens_seen": 684544,
      "step": 856
    },
    {
      "epoch": 0.7036124794745484,
      "grad_norm": 15.803251266479492,
      "learning_rate": 3.0504256143004866e-06,
      "loss": 0.4335,
      "num_input_tokens_seen": 685312,
      "step": 857
    },
    {
      "epoch": 0.7044334975369458,
      "grad_norm": 10.4351224899292,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 0.6298,
      "num_input_tokens_seen": 686080,
      "step": 858
    },
    {
      "epoch": 0.7052545155993432,
      "grad_norm": 12.995474815368652,
      "learning_rate": 2.967413698375196e-06,
      "loss": 0.8668,
      "num_input_tokens_seen": 686848,
      "step": 859
    },
    {
      "epoch": 0.7060755336617406,
      "grad_norm": 15.564934730529785,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 0.5574,
      "num_input_tokens_seen": 687744,
      "step": 860
    },
    {
      "epoch": 0.7068965517241379,
      "grad_norm": 6.77152681350708,
      "learning_rate": 2.8854756196229016e-06,
      "loss": 0.1689,
      "num_input_tokens_seen": 688512,
      "step": 861
    },
    {
      "epoch": 0.7077175697865353,
      "grad_norm": 10.452078819274902,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 0.406,
      "num_input_tokens_seen": 689280,
      "step": 862
    },
    {
      "epoch": 0.7085385878489326,
      "grad_norm": 3.293039083480835,
      "learning_rate": 2.8046153715899692e-06,
      "loss": 0.085,
      "num_input_tokens_seen": 690048,
      "step": 863
    },
    {
      "epoch": 0.7093596059113301,
      "grad_norm": 9.727148056030273,
      "learning_rate": 2.764590667717562e-06,
      "loss": 0.3522,
      "num_input_tokens_seen": 690816,
      "step": 864
    },
    {
      "epoch": 0.7101806239737274,
      "grad_norm": 3.546302556991577,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 0.1208,
      "num_input_tokens_seen": 691712,
      "step": 865
    },
    {
      "epoch": 0.7110016420361248,
      "grad_norm": 13.57166576385498,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 0.6982,
      "num_input_tokens_seen": 692608,
      "step": 866
    },
    {
      "epoch": 0.7118226600985221,
      "grad_norm": 18.62632942199707,
      "learning_rate": 2.646144079015797e-06,
      "loss": 0.9811,
      "num_input_tokens_seen": 693376,
      "step": 867
    },
    {
      "epoch": 0.7126436781609196,
      "grad_norm": 9.994088172912598,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 0.233,
      "num_input_tokens_seen": 694272,
      "step": 868
    },
    {
      "epoch": 0.7134646962233169,
      "grad_norm": 12.37647533416748,
      "learning_rate": 2.5685407581417907e-06,
      "loss": 0.4187,
      "num_input_tokens_seen": 695040,
      "step": 869
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 3.827380895614624,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 0.0837,
      "num_input_tokens_seen": 695808,
      "step": 870
    },
    {
      "epoch": 0.7151067323481116,
      "grad_norm": 18.305774688720703,
      "learning_rate": 2.492030714945162e-06,
      "loss": 0.6958,
      "num_input_tokens_seen": 696576,
      "step": 871
    },
    {
      "epoch": 0.715927750410509,
      "grad_norm": 13.358168601989746,
      "learning_rate": 2.454186839872158e-06,
      "loss": 0.5989,
      "num_input_tokens_seen": 697344,
      "step": 872
    },
    {
      "epoch": 0.7167487684729064,
      "grad_norm": 15.74197006225586,
      "learning_rate": 2.4166176784174795e-06,
      "loss": 0.5118,
      "num_input_tokens_seen": 698112,
      "step": 873
    },
    {
      "epoch": 0.7175697865353038,
      "grad_norm": 15.779123306274414,
      "learning_rate": 2.379323688349516e-06,
      "loss": 0.3664,
      "num_input_tokens_seen": 698880,
      "step": 874
    },
    {
      "epoch": 0.7183908045977011,
      "grad_norm": 12.117182731628418,
      "learning_rate": 2.3423053240837515e-06,
      "loss": 0.3989,
      "num_input_tokens_seen": 699648,
      "step": 875
    },
    {
      "epoch": 0.7192118226600985,
      "grad_norm": 15.420211791992188,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 1.2213,
      "num_input_tokens_seen": 700544,
      "step": 876
    },
    {
      "epoch": 0.7200328407224958,
      "grad_norm": 10.627347946166992,
      "learning_rate": 2.269097273823287e-06,
      "loss": 0.2511,
      "num_input_tokens_seen": 701312,
      "step": 877
    },
    {
      "epoch": 0.7208538587848933,
      "grad_norm": 17.62735366821289,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 0.5311,
      "num_input_tokens_seen": 702080,
      "step": 878
    },
    {
      "epoch": 0.7216748768472906,
      "grad_norm": 17.222923278808594,
      "learning_rate": 2.1969970956931762e-06,
      "loss": 0.9747,
      "num_input_tokens_seen": 702848,
      "step": 879
    },
    {
      "epoch": 0.722495894909688,
      "grad_norm": 18.104286193847656,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 0.5346,
      "num_input_tokens_seen": 703616,
      "step": 880
    },
    {
      "epoch": 0.7233169129720853,
      "grad_norm": 12.974510192871094,
      "learning_rate": 2.1260083037543817e-06,
      "loss": 0.4918,
      "num_input_tokens_seen": 704384,
      "step": 881
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 13.926549911499023,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 0.4925,
      "num_input_tokens_seen": 705280,
      "step": 882
    },
    {
      "epoch": 0.7249589490968801,
      "grad_norm": 15.758983612060547,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 0.8252,
      "num_input_tokens_seen": 706048,
      "step": 883
    },
    {
      "epoch": 0.7257799671592775,
      "grad_norm": 14.27043628692627,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 0.5259,
      "num_input_tokens_seen": 706816,
      "step": 884
    },
    {
      "epoch": 0.7266009852216748,
      "grad_norm": 22.498798370361328,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 1.0361,
      "num_input_tokens_seen": 707712,
      "step": 885
    },
    {
      "epoch": 0.7274220032840722,
      "grad_norm": 9.751901626586914,
      "learning_rate": 1.95342121028749e-06,
      "loss": 0.6076,
      "num_input_tokens_seen": 708480,
      "step": 886
    },
    {
      "epoch": 0.7282430213464697,
      "grad_norm": 11.626886367797852,
      "learning_rate": 1.9197445721754776e-06,
      "loss": 0.4442,
      "num_input_tokens_seen": 709504,
      "step": 887
    },
    {
      "epoch": 0.729064039408867,
      "grad_norm": 13.728806495666504,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 0.9344,
      "num_input_tokens_seen": 710272,
      "step": 888
    },
    {
      "epoch": 0.7298850574712644,
      "grad_norm": 17.9001407623291,
      "learning_rate": 1.8532353797501318e-06,
      "loss": 0.7649,
      "num_input_tokens_seen": 711040,
      "step": 889
    },
    {
      "epoch": 0.7307060755336617,
      "grad_norm": 16.378070831298828,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 0.7515,
      "num_input_tokens_seen": 711808,
      "step": 890
    },
    {
      "epoch": 0.7315270935960592,
      "grad_norm": 12.745523452758789,
      "learning_rate": 1.787854327977162e-06,
      "loss": 0.3517,
      "num_input_tokens_seen": 712704,
      "step": 891
    },
    {
      "epoch": 0.7323481116584565,
      "grad_norm": 15.367332458496094,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 0.7361,
      "num_input_tokens_seen": 713472,
      "step": 892
    },
    {
      "epoch": 0.7331691297208539,
      "grad_norm": 18.69293785095215,
      "learning_rate": 1.7236046034367958e-06,
      "loss": 1.1066,
      "num_input_tokens_seen": 714240,
      "step": 893
    },
    {
      "epoch": 0.7339901477832512,
      "grad_norm": 12.151283264160156,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 0.3473,
      "num_input_tokens_seen": 715008,
      "step": 894
    },
    {
      "epoch": 0.7348111658456487,
      "grad_norm": 16.314556121826172,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 0.3747,
      "num_input_tokens_seen": 715776,
      "step": 895
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 10.749784469604492,
      "learning_rate": 1.629358090099639e-06,
      "loss": 0.2727,
      "num_input_tokens_seen": 716544,
      "step": 896
    },
    {
      "epoch": 0.7364532019704434,
      "grad_norm": 10.010412216186523,
      "learning_rate": 1.5985116065256684e-06,
      "loss": 0.254,
      "num_input_tokens_seen": 717312,
      "step": 897
    },
    {
      "epoch": 0.7372742200328407,
      "grad_norm": 19.499765396118164,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 1.1049,
      "num_input_tokens_seen": 718080,
      "step": 898
    },
    {
      "epoch": 0.7380952380952381,
      "grad_norm": 2.4866209030151367,
      "learning_rate": 1.5376744310111019e-06,
      "loss": 0.0692,
      "num_input_tokens_seen": 718848,
      "step": 899
    },
    {
      "epoch": 0.7389162561576355,
      "grad_norm": 4.514567852020264,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 0.1134,
      "num_input_tokens_seen": 719616,
      "step": 900
    },
    {
      "epoch": 0.7397372742200329,
      "grad_norm": 26.35877799987793,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 1.3012,
      "num_input_tokens_seen": 720384,
      "step": 901
    },
    {
      "epoch": 0.7405582922824302,
      "grad_norm": 6.850817680358887,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 0.2405,
      "num_input_tokens_seen": 721152,
      "step": 902
    },
    {
      "epoch": 0.7413793103448276,
      "grad_norm": 7.883289337158203,
      "learning_rate": 1.4194335513099761e-06,
      "loss": 0.1578,
      "num_input_tokens_seen": 722048,
      "step": 903
    },
    {
      "epoch": 0.7422003284072249,
      "grad_norm": 7.729239463806152,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 0.245,
      "num_input_tokens_seen": 722816,
      "step": 904
    },
    {
      "epoch": 0.7430213464696224,
      "grad_norm": 19.132537841796875,
      "learning_rate": 1.362035610017079e-06,
      "loss": 0.6088,
      "num_input_tokens_seen": 723584,
      "step": 905
    },
    {
      "epoch": 0.7438423645320197,
      "grad_norm": 11.213126182556152,
      "learning_rate": 1.333768497107593e-06,
      "loss": 0.3795,
      "num_input_tokens_seen": 724352,
      "step": 906
    },
    {
      "epoch": 0.7446633825944171,
      "grad_norm": 22.64520835876465,
      "learning_rate": 1.305789749760361e-06,
      "loss": 0.7015,
      "num_input_tokens_seen": 725120,
      "step": 907
    },
    {
      "epoch": 0.7454844006568144,
      "grad_norm": 10.209447860717773,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 0.3456,
      "num_input_tokens_seen": 725888,
      "step": 908
    },
    {
      "epoch": 0.7463054187192119,
      "grad_norm": 21.001602172851562,
      "learning_rate": 1.250698711883691e-06,
      "loss": 0.8984,
      "num_input_tokens_seen": 726656,
      "step": 909
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 7.9084248542785645,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 0.3359,
      "num_input_tokens_seen": 727296,
      "step": 910
    },
    {
      "epoch": 0.7479474548440066,
      "grad_norm": 8.668355941772461,
      "learning_rate": 1.1967651814465354e-06,
      "loss": 0.2857,
      "num_input_tokens_seen": 728064,
      "step": 911
    },
    {
      "epoch": 0.7487684729064039,
      "grad_norm": 18.834888458251953,
      "learning_rate": 1.170233305176327e-06,
      "loss": 0.3521,
      "num_input_tokens_seen": 728960,
      "step": 912
    },
    {
      "epoch": 0.7495894909688013,
      "grad_norm": 11.764208793640137,
      "learning_rate": 1.1439917870930793e-06,
      "loss": 0.436,
      "num_input_tokens_seen": 729728,
      "step": 913
    },
    {
      "epoch": 0.7504105090311987,
      "grad_norm": 11.416499137878418,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 0.2132,
      "num_input_tokens_seen": 730624,
      "step": 914
    },
    {
      "epoch": 0.7512315270935961,
      "grad_norm": 4.770811557769775,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 0.1053,
      "num_input_tokens_seen": 731392,
      "step": 915
    },
    {
      "epoch": 0.7520525451559934,
      "grad_norm": 15.584863662719727,
      "learning_rate": 1.067012561698319e-06,
      "loss": 0.5186,
      "num_input_tokens_seen": 732416,
      "step": 916
    },
    {
      "epoch": 0.7528735632183908,
      "grad_norm": 14.483280181884766,
      "learning_rate": 1.0419356383716688e-06,
      "loss": 0.3922,
      "num_input_tokens_seen": 733184,
      "step": 917
    },
    {
      "epoch": 0.7536945812807881,
      "grad_norm": 6.013375759124756,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 0.0845,
      "num_input_tokens_seen": 733952,
      "step": 918
    },
    {
      "epoch": 0.7545155993431856,
      "grad_norm": 12.252685546875,
      "learning_rate": 9.926578580764234e-07,
      "loss": 0.4987,
      "num_input_tokens_seen": 734720,
      "step": 919
    },
    {
      "epoch": 0.7553366174055829,
      "grad_norm": 15.239005088806152,
      "learning_rate": 9.684576015420278e-07,
      "loss": 0.7742,
      "num_input_tokens_seen": 735616,
      "step": 920
    },
    {
      "epoch": 0.7561576354679803,
      "grad_norm": 3.938779830932617,
      "learning_rate": 9.445501617678654e-07,
      "loss": 0.071,
      "num_input_tokens_seen": 736384,
      "step": 921
    },
    {
      "epoch": 0.7569786535303776,
      "grad_norm": 8.252664566040039,
      "learning_rate": 9.209358300585474e-07,
      "loss": 0.2634,
      "num_input_tokens_seen": 737152,
      "step": 922
    },
    {
      "epoch": 0.7577996715927751,
      "grad_norm": 12.179768562316895,
      "learning_rate": 8.976148941472501e-07,
      "loss": 0.6102,
      "num_input_tokens_seen": 737920,
      "step": 923
    },
    {
      "epoch": 0.7586206896551724,
      "grad_norm": 14.409967422485352,
      "learning_rate": 8.745876381922147e-07,
      "loss": 0.3982,
      "num_input_tokens_seen": 738816,
      "step": 924
    },
    {
      "epoch": 0.7594417077175698,
      "grad_norm": 10.257251739501953,
      "learning_rate": 8.51854342773295e-07,
      "loss": 0.4182,
      "num_input_tokens_seen": 739712,
      "step": 925
    },
    {
      "epoch": 0.7602627257799671,
      "grad_norm": 12.995373725891113,
      "learning_rate": 8.294152848885157e-07,
      "loss": 0.6888,
      "num_input_tokens_seen": 740480,
      "step": 926
    },
    {
      "epoch": 0.7610837438423645,
      "grad_norm": 27.525800704956055,
      "learning_rate": 8.072707379507216e-07,
      "loss": 1.115,
      "num_input_tokens_seen": 741248,
      "step": 927
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 4.031478404998779,
      "learning_rate": 7.854209717842231e-07,
      "loss": 0.0707,
      "num_input_tokens_seen": 742016,
      "step": 928
    },
    {
      "epoch": 0.7627257799671593,
      "grad_norm": 11.205047607421875,
      "learning_rate": 7.638662526215284e-07,
      "loss": 0.5366,
      "num_input_tokens_seen": 742784,
      "step": 929
    },
    {
      "epoch": 0.7635467980295566,
      "grad_norm": 25.84318733215332,
      "learning_rate": 7.426068431000882e-07,
      "loss": 1.4449,
      "num_input_tokens_seen": 743552,
      "step": 930
    },
    {
      "epoch": 0.764367816091954,
      "grad_norm": 16.994779586791992,
      "learning_rate": 7.216430022591008e-07,
      "loss": 0.6105,
      "num_input_tokens_seen": 744320,
      "step": 931
    },
    {
      "epoch": 0.7651888341543513,
      "grad_norm": 18.992717742919922,
      "learning_rate": 7.009749855363456e-07,
      "loss": 0.6986,
      "num_input_tokens_seen": 745216,
      "step": 932
    },
    {
      "epoch": 0.7660098522167488,
      "grad_norm": 14.951309204101562,
      "learning_rate": 6.806030447650879e-07,
      "loss": 0.4018,
      "num_input_tokens_seen": 745984,
      "step": 933
    },
    {
      "epoch": 0.7668308702791461,
      "grad_norm": 14.718645095825195,
      "learning_rate": 6.605274281709928e-07,
      "loss": 0.5068,
      "num_input_tokens_seen": 746752,
      "step": 934
    },
    {
      "epoch": 0.7676518883415435,
      "grad_norm": 6.682572364807129,
      "learning_rate": 6.407483803691216e-07,
      "loss": 0.1175,
      "num_input_tokens_seen": 747520,
      "step": 935
    },
    {
      "epoch": 0.7684729064039408,
      "grad_norm": 17.54044532775879,
      "learning_rate": 6.212661423609184e-07,
      "loss": 0.81,
      "num_input_tokens_seen": 748288,
      "step": 936
    },
    {
      "epoch": 0.7692939244663383,
      "grad_norm": 12.570528984069824,
      "learning_rate": 6.020809515313142e-07,
      "loss": 0.5856,
      "num_input_tokens_seen": 749056,
      "step": 937
    },
    {
      "epoch": 0.7701149425287356,
      "grad_norm": 16.8795223236084,
      "learning_rate": 5.83193041645802e-07,
      "loss": 0.6407,
      "num_input_tokens_seen": 749824,
      "step": 938
    },
    {
      "epoch": 0.770935960591133,
      "grad_norm": 10.361556053161621,
      "learning_rate": 5.646026428476031e-07,
      "loss": 0.423,
      "num_input_tokens_seen": 750720,
      "step": 939
    },
    {
      "epoch": 0.7717569786535303,
      "grad_norm": 12.41311264038086,
      "learning_rate": 5.463099816548579e-07,
      "loss": 0.7118,
      "num_input_tokens_seen": 751616,
      "step": 940
    },
    {
      "epoch": 0.7725779967159278,
      "grad_norm": 12.577340126037598,
      "learning_rate": 5.283152809578751e-07,
      "loss": 0.3747,
      "num_input_tokens_seen": 752384,
      "step": 941
    },
    {
      "epoch": 0.7733990147783252,
      "grad_norm": 13.63998031616211,
      "learning_rate": 5.106187600163987e-07,
      "loss": 0.4513,
      "num_input_tokens_seen": 753152,
      "step": 942
    },
    {
      "epoch": 0.7742200328407225,
      "grad_norm": 10.370036125183105,
      "learning_rate": 4.932206344569562e-07,
      "loss": 0.2317,
      "num_input_tokens_seen": 753920,
      "step": 943
    },
    {
      "epoch": 0.7750410509031199,
      "grad_norm": 6.392043590545654,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 0.2371,
      "num_input_tokens_seen": 754688,
      "step": 944
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 6.942141056060791,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 0.369,
      "num_input_tokens_seen": 755456,
      "step": 945
    },
    {
      "epoch": 0.7766830870279147,
      "grad_norm": 14.401252746582031,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 0.7979,
      "num_input_tokens_seen": 756224,
      "step": 946
    },
    {
      "epoch": 0.777504105090312,
      "grad_norm": 17.113754272460938,
      "learning_rate": 4.26616271261146e-07,
      "loss": 0.6964,
      "num_input_tokens_seen": 757120,
      "step": 947
    },
    {
      "epoch": 0.7783251231527094,
      "grad_norm": 10.579309463500977,
      "learning_rate": 4.107132296653549e-07,
      "loss": 0.4716,
      "num_input_tokens_seen": 758016,
      "step": 948
    },
    {
      "epoch": 0.7791461412151067,
      "grad_norm": 10.355973243713379,
      "learning_rate": 3.95109800768953e-07,
      "loss": 0.3488,
      "num_input_tokens_seen": 758784,
      "step": 949
    },
    {
      "epoch": 0.7799671592775042,
      "grad_norm": 6.171248435974121,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 0.2275,
      "num_input_tokens_seen": 759552,
      "step": 950
    },
    {
      "epoch": 0.7807881773399015,
      "grad_norm": 16.764801025390625,
      "learning_rate": 3.6480253791274786e-07,
      "loss": 0.5894,
      "num_input_tokens_seen": 760320,
      "step": 951
    },
    {
      "epoch": 0.7816091954022989,
      "grad_norm": 9.4854097366333,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 0.2831,
      "num_input_tokens_seen": 761088,
      "step": 952
    },
    {
      "epoch": 0.7824302134646962,
      "grad_norm": 6.115161895751953,
      "learning_rate": 3.3569595982576583e-07,
      "loss": 0.3236,
      "num_input_tokens_seen": 761984,
      "step": 953
    },
    {
      "epoch": 0.7832512315270936,
      "grad_norm": 7.616729259490967,
      "learning_rate": 3.215933731753024e-07,
      "loss": 0.1949,
      "num_input_tokens_seen": 762752,
      "step": 954
    },
    {
      "epoch": 0.784072249589491,
      "grad_norm": 8.84116268157959,
      "learning_rate": 3.077914851215585e-07,
      "loss": 0.2365,
      "num_input_tokens_seen": 763776,
      "step": 955
    },
    {
      "epoch": 0.7848932676518884,
      "grad_norm": 11.2517728805542,
      "learning_rate": 2.942904638361804e-07,
      "loss": 0.6113,
      "num_input_tokens_seen": 764544,
      "step": 956
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 15.234173774719238,
      "learning_rate": 2.810904738248549e-07,
      "loss": 0.3203,
      "num_input_tokens_seen": 765440,
      "step": 957
    },
    {
      "epoch": 0.7865353037766831,
      "grad_norm": 28.338483810424805,
      "learning_rate": 2.681916759252917e-07,
      "loss": 1.4328,
      "num_input_tokens_seen": 766208,
      "step": 958
    },
    {
      "epoch": 0.7873563218390804,
      "grad_norm": 9.06120777130127,
      "learning_rate": 2.555942273052753e-07,
      "loss": 0.2382,
      "num_input_tokens_seen": 766976,
      "step": 959
    },
    {
      "epoch": 0.7881773399014779,
      "grad_norm": 7.192202091217041,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 0.1594,
      "num_input_tokens_seen": 767744,
      "step": 960
    },
    {
      "epoch": 0.7889983579638752,
      "grad_norm": 16.83955955505371,
      "learning_rate": 2.3130398821391007e-07,
      "loss": 0.5096,
      "num_input_tokens_seen": 768512,
      "step": 961
    },
    {
      "epoch": 0.7898193760262726,
      "grad_norm": 12.190963745117188,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 0.3139,
      "num_input_tokens_seen": 769280,
      "step": 962
    },
    {
      "epoch": 0.7906403940886699,
      "grad_norm": 18.49597930908203,
      "learning_rate": 2.0822094042274032e-07,
      "loss": 0.7555,
      "num_input_tokens_seen": 770048,
      "step": 963
    },
    {
      "epoch": 0.7914614121510674,
      "grad_norm": 9.505480766296387,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 0.2211,
      "num_input_tokens_seen": 770944,
      "step": 964
    },
    {
      "epoch": 0.7922824302134647,
      "grad_norm": 19.445205688476562,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 1.0876,
      "num_input_tokens_seen": 771712,
      "step": 965
    },
    {
      "epoch": 0.7931034482758621,
      "grad_norm": 3.6718132495880127,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 0.121,
      "num_input_tokens_seen": 772608,
      "step": 966
    },
    {
      "epoch": 0.7939244663382594,
      "grad_norm": 5.376211166381836,
      "learning_rate": 1.6568085999008888e-07,
      "loss": 0.1218,
      "num_input_tokens_seen": 773376,
      "step": 967
    },
    {
      "epoch": 0.7947454844006568,
      "grad_norm": 19.228618621826172,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 0.8125,
      "num_input_tokens_seen": 774144,
      "step": 968
    },
    {
      "epoch": 0.7955665024630542,
      "grad_norm": 23.068805694580078,
      "learning_rate": 1.4622590069211516e-07,
      "loss": 1.4211,
      "num_input_tokens_seen": 774912,
      "step": 969
    },
    {
      "epoch": 0.7963875205254516,
      "grad_norm": 8.263235092163086,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 0.2402,
      "num_input_tokens_seen": 775680,
      "step": 970
    },
    {
      "epoch": 0.7972085385878489,
      "grad_norm": 7.257675647735596,
      "learning_rate": 1.2798227928029482e-07,
      "loss": 0.137,
      "num_input_tokens_seen": 776448,
      "step": 971
    },
    {
      "epoch": 0.7980295566502463,
      "grad_norm": 13.247273445129395,
      "learning_rate": 1.193150004542204e-07,
      "loss": 0.6631,
      "num_input_tokens_seen": 777216,
      "step": 972
    },
    {
      "epoch": 0.7988505747126436,
      "grad_norm": 10.84108829498291,
      "learning_rate": 1.109508849230001e-07,
      "loss": 0.2657,
      "num_input_tokens_seen": 778112,
      "step": 973
    },
    {
      "epoch": 0.7996715927750411,
      "grad_norm": 20.033273696899414,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 0.8556,
      "num_input_tokens_seen": 779008,
      "step": 974
    },
    {
      "epoch": 0.8004926108374384,
      "grad_norm": 8.782745361328125,
      "learning_rate": 9.513254770636137e-08,
      "loss": 0.1316,
      "num_input_tokens_seen": 779776,
      "step": 975
    },
    {
      "epoch": 0.8013136288998358,
      "grad_norm": 20.271276473999023,
      "learning_rate": 8.767851876239074e-08,
      "loss": 0.5437,
      "num_input_tokens_seen": 780544,
      "step": 976
    },
    {
      "epoch": 0.8021346469622331,
      "grad_norm": 15.221613883972168,
      "learning_rate": 8.052803859382174e-08,
      "loss": 0.4974,
      "num_input_tokens_seen": 781312,
      "step": 977
    },
    {
      "epoch": 0.8029556650246306,
      "grad_norm": 5.983894348144531,
      "learning_rate": 7.368119432699383e-08,
      "loss": 0.2582,
      "num_input_tokens_seen": 782080,
      "step": 978
    },
    {
      "epoch": 0.8037766830870279,
      "grad_norm": 12.840246200561523,
      "learning_rate": 6.71380693885476e-08,
      "loss": 0.592,
      "num_input_tokens_seen": 782848,
      "step": 979
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 33.734012603759766,
      "learning_rate": 6.089874350439506e-08,
      "loss": 0.3655,
      "num_input_tokens_seen": 783616,
      "step": 980
    },
    {
      "epoch": 0.8054187192118226,
      "grad_norm": 6.273918628692627,
      "learning_rate": 5.496329269875089e-08,
      "loss": 0.1474,
      "num_input_tokens_seen": 784384,
      "step": 981
    },
    {
      "epoch": 0.80623973727422,
      "grad_norm": 13.89202880859375,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 0.8561,
      "num_input_tokens_seen": 785152,
      "step": 982
    },
    {
      "epoch": 0.8070607553366174,
      "grad_norm": 4.27325439453125,
      "learning_rate": 4.400430190586724e-08,
      "loss": 0.1088,
      "num_input_tokens_seen": 785920,
      "step": 983
    },
    {
      "epoch": 0.8078817733990148,
      "grad_norm": 14.76479434967041,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 0.5915,
      "num_input_tokens_seen": 786688,
      "step": 984
    },
    {
      "epoch": 0.8087027914614121,
      "grad_norm": 12.790558815002441,
      "learning_rate": 3.426163113565417e-08,
      "loss": 0.398,
      "num_input_tokens_seen": 787584,
      "step": 985
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 11.478763580322266,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 0.6115,
      "num_input_tokens_seen": 788480,
      "step": 986
    },
    {
      "epoch": 0.8103448275862069,
      "grad_norm": 16.10714340209961,
      "learning_rate": 2.5735755232134118e-08,
      "loss": 0.9837,
      "num_input_tokens_seen": 789376,
      "step": 987
    },
    {
      "epoch": 0.8111658456486043,
      "grad_norm": 34.881996154785156,
      "learning_rate": 2.192924752854042e-08,
      "loss": 1.14,
      "num_input_tokens_seen": 790144,
      "step": 988
    },
    {
      "epoch": 0.8119868637110016,
      "grad_norm": 4.926480293273926,
      "learning_rate": 1.842708973447127e-08,
      "loss": 0.0597,
      "num_input_tokens_seen": 790912,
      "step": 989
    },
    {
      "epoch": 0.812807881773399,
      "grad_norm": 12.468342781066895,
      "learning_rate": 1.522932452260595e-08,
      "loss": 0.5059,
      "num_input_tokens_seen": 791680,
      "step": 990
    },
    {
      "epoch": 0.8136288998357963,
      "grad_norm": 12.28907585144043,
      "learning_rate": 1.233599085671e-08,
      "loss": 0.5801,
      "num_input_tokens_seen": 792448,
      "step": 991
    },
    {
      "epoch": 0.8144499178981938,
      "grad_norm": 3.584149122238159,
      "learning_rate": 9.747123991141194e-09,
      "loss": 0.0707,
      "num_input_tokens_seen": 793216,
      "step": 992
    },
    {
      "epoch": 0.8152709359605911,
      "grad_norm": 9.256465911865234,
      "learning_rate": 7.462755470422078e-09,
      "loss": 0.1872,
      "num_input_tokens_seen": 793984,
      "step": 993
    },
    {
      "epoch": 0.8160919540229885,
      "grad_norm": 14.111246109008789,
      "learning_rate": 5.48291312886251e-09,
      "loss": 0.4747,
      "num_input_tokens_seen": 794880,
      "step": 994
    },
    {
      "epoch": 0.8169129720853858,
      "grad_norm": 7.509424209594727,
      "learning_rate": 3.807621090218261e-09,
      "loss": 0.3088,
      "num_input_tokens_seen": 795648,
      "step": 995
    },
    {
      "epoch": 0.8177339901477833,
      "grad_norm": 14.425299644470215,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 0.8355,
      "num_input_tokens_seen": 796416,
      "step": 996
    },
    {
      "epoch": 0.8185550082101807,
      "grad_norm": 19.477540969848633,
      "learning_rate": 1.3707658621964215e-09,
      "loss": 0.9048,
      "num_input_tokens_seen": 797184,
      "step": 997
    },
    {
      "epoch": 0.819376026272578,
      "grad_norm": 12.973148345947266,
      "learning_rate": 6.092323651313292e-10,
      "loss": 0.5211,
      "num_input_tokens_seen": 797952,
      "step": 998
    },
    {
      "epoch": 0.8201970443349754,
      "grad_norm": 8.058974266052246,
      "learning_rate": 1.5230855524017708e-10,
      "loss": 0.3965,
      "num_input_tokens_seen": 798720,
      "step": 999
    },
    {
      "epoch": 0.8210180623973727,
      "grad_norm": 2.5147082805633545,
      "learning_rate": 0.0,
      "loss": 0.0541,
      "num_input_tokens_seen": 799744,
      "step": 1000
    },
    {
      "epoch": 0.8210180623973727,
      "eval_loss": 0.4567340314388275,
      "eval_runtime": 12.0131,
      "eval_samples_per_second": 101.639,
      "eval_steps_per_second": 12.736,
      "num_input_tokens_seen": 799744,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 799744,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.6028463412412416e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
