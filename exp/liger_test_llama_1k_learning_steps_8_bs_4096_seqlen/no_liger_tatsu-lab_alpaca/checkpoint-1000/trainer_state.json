{
  "best_metric": 1.2649707794189453,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_8_bs_4096_seqlen/no_liger_tatsu-lab_alpaca/checkpoint-1000",
  "epoch": 0.17091095539224063,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00017091095539224064,
      "grad_norm": 0.7901930809020996,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.9093,
      "num_input_tokens_seen": 1152,
      "step": 1
    },
    {
      "epoch": 0.0003418219107844813,
      "grad_norm": 0.8928191065788269,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.6325,
      "num_input_tokens_seen": 2688,
      "step": 2
    },
    {
      "epoch": 0.0005127328661767219,
      "grad_norm": 1.0680166482925415,
      "learning_rate": 1.5e-06,
      "loss": 1.6436,
      "num_input_tokens_seen": 3840,
      "step": 3
    },
    {
      "epoch": 0.0006836438215689625,
      "grad_norm": 1.0899007320404053,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.8898,
      "num_input_tokens_seen": 4992,
      "step": 4
    },
    {
      "epoch": 0.0008545547769612033,
      "grad_norm": 1.056504487991333,
      "learning_rate": 2.5e-06,
      "loss": 1.7346,
      "num_input_tokens_seen": 6784,
      "step": 5
    },
    {
      "epoch": 0.0010254657323534439,
      "grad_norm": 0.7517726421356201,
      "learning_rate": 3e-06,
      "loss": 1.7841,
      "num_input_tokens_seen": 9216,
      "step": 6
    },
    {
      "epoch": 0.0011963766877456845,
      "grad_norm": 0.827029824256897,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.6353,
      "num_input_tokens_seen": 10496,
      "step": 7
    },
    {
      "epoch": 0.001367287643137925,
      "grad_norm": 0.7992870211601257,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6681,
      "num_input_tokens_seen": 12288,
      "step": 8
    },
    {
      "epoch": 0.0015381985985301657,
      "grad_norm": 0.823616623878479,
      "learning_rate": 4.5e-06,
      "loss": 1.6947,
      "num_input_tokens_seen": 14336,
      "step": 9
    },
    {
      "epoch": 0.0017091095539224065,
      "grad_norm": 1.0618501901626587,
      "learning_rate": 5e-06,
      "loss": 1.7166,
      "num_input_tokens_seen": 15488,
      "step": 10
    },
    {
      "epoch": 0.0018800205093146471,
      "grad_norm": 0.815070629119873,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.5574,
      "num_input_tokens_seen": 17024,
      "step": 11
    },
    {
      "epoch": 0.0020509314647068877,
      "grad_norm": 0.8912754654884338,
      "learning_rate": 6e-06,
      "loss": 1.9184,
      "num_input_tokens_seen": 19072,
      "step": 12
    },
    {
      "epoch": 0.002221842420099128,
      "grad_norm": 0.93158358335495,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.7661,
      "num_input_tokens_seen": 20224,
      "step": 13
    },
    {
      "epoch": 0.002392753375491369,
      "grad_norm": 1.5634127855300903,
      "learning_rate": 7.000000000000001e-06,
      "loss": 2.1066,
      "num_input_tokens_seen": 21760,
      "step": 14
    },
    {
      "epoch": 0.00256366433088361,
      "grad_norm": 0.9328048229217529,
      "learning_rate": 7.5e-06,
      "loss": 1.7311,
      "num_input_tokens_seen": 23296,
      "step": 15
    },
    {
      "epoch": 0.00273457528627585,
      "grad_norm": 0.7502769231796265,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.492,
      "num_input_tokens_seen": 24832,
      "step": 16
    },
    {
      "epoch": 0.002905486241668091,
      "grad_norm": 0.7995829582214355,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.7458,
      "num_input_tokens_seen": 26752,
      "step": 17
    },
    {
      "epoch": 0.0030763971970603314,
      "grad_norm": 1.1303435564041138,
      "learning_rate": 9e-06,
      "loss": 1.6801,
      "num_input_tokens_seen": 28544,
      "step": 18
    },
    {
      "epoch": 0.0032473081524525722,
      "grad_norm": 0.9966654181480408,
      "learning_rate": 9.5e-06,
      "loss": 1.7048,
      "num_input_tokens_seen": 30336,
      "step": 19
    },
    {
      "epoch": 0.003418219107844813,
      "grad_norm": 0.8855854868888855,
      "learning_rate": 1e-05,
      "loss": 1.6662,
      "num_input_tokens_seen": 32896,
      "step": 20
    },
    {
      "epoch": 0.0035891300632370535,
      "grad_norm": 1.1519876718521118,
      "learning_rate": 1.05e-05,
      "loss": 1.8115,
      "num_input_tokens_seen": 34048,
      "step": 21
    },
    {
      "epoch": 0.0037600410186292943,
      "grad_norm": 1.084031581878662,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.5563,
      "num_input_tokens_seen": 35328,
      "step": 22
    },
    {
      "epoch": 0.003930951974021535,
      "grad_norm": 0.9126018285751343,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.3337,
      "num_input_tokens_seen": 37120,
      "step": 23
    },
    {
      "epoch": 0.0041018629294137755,
      "grad_norm": 0.8546198010444641,
      "learning_rate": 1.2e-05,
      "loss": 1.6351,
      "num_input_tokens_seen": 39808,
      "step": 24
    },
    {
      "epoch": 0.004272773884806016,
      "grad_norm": 1.4640554189682007,
      "learning_rate": 1.25e-05,
      "loss": 1.627,
      "num_input_tokens_seen": 40832,
      "step": 25
    },
    {
      "epoch": 0.004443684840198256,
      "grad_norm": 1.0829036235809326,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.8663,
      "num_input_tokens_seen": 42624,
      "step": 26
    },
    {
      "epoch": 0.004614595795590497,
      "grad_norm": 1.0766422748565674,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.6456,
      "num_input_tokens_seen": 43776,
      "step": 27
    },
    {
      "epoch": 0.004785506750982738,
      "grad_norm": 1.0327988862991333,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.7715,
      "num_input_tokens_seen": 45952,
      "step": 28
    },
    {
      "epoch": 0.004956417706374979,
      "grad_norm": 0.8724311590194702,
      "learning_rate": 1.45e-05,
      "loss": 1.5484,
      "num_input_tokens_seen": 48256,
      "step": 29
    },
    {
      "epoch": 0.00512732866176722,
      "grad_norm": 0.9218023419380188,
      "learning_rate": 1.5e-05,
      "loss": 1.224,
      "num_input_tokens_seen": 49792,
      "step": 30
    },
    {
      "epoch": 0.0052982396171594595,
      "grad_norm": 1.3065447807312012,
      "learning_rate": 1.55e-05,
      "loss": 1.5351,
      "num_input_tokens_seen": 51072,
      "step": 31
    },
    {
      "epoch": 0.0054691505725517,
      "grad_norm": 1.3883192539215088,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9757,
      "num_input_tokens_seen": 52480,
      "step": 32
    },
    {
      "epoch": 0.005640061527943941,
      "grad_norm": 1.2370030879974365,
      "learning_rate": 1.65e-05,
      "loss": 1.1009,
      "num_input_tokens_seen": 53504,
      "step": 33
    },
    {
      "epoch": 0.005810972483336182,
      "grad_norm": 1.1829453706741333,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.6192,
      "num_input_tokens_seen": 55296,
      "step": 34
    },
    {
      "epoch": 0.005981883438728423,
      "grad_norm": 1.2340123653411865,
      "learning_rate": 1.75e-05,
      "loss": 1.8038,
      "num_input_tokens_seen": 56960,
      "step": 35
    },
    {
      "epoch": 0.006152794394120663,
      "grad_norm": 1.7302695512771606,
      "learning_rate": 1.8e-05,
      "loss": 1.5195,
      "num_input_tokens_seen": 58368,
      "step": 36
    },
    {
      "epoch": 0.006323705349512904,
      "grad_norm": 1.4846469163894653,
      "learning_rate": 1.85e-05,
      "loss": 1.6905,
      "num_input_tokens_seen": 59904,
      "step": 37
    },
    {
      "epoch": 0.0064946163049051445,
      "grad_norm": 1.4030247926712036,
      "learning_rate": 1.9e-05,
      "loss": 1.4058,
      "num_input_tokens_seen": 61440,
      "step": 38
    },
    {
      "epoch": 0.006665527260297385,
      "grad_norm": 1.6873831748962402,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.7971,
      "num_input_tokens_seen": 62848,
      "step": 39
    },
    {
      "epoch": 0.006836438215689626,
      "grad_norm": 1.2261556386947632,
      "learning_rate": 2e-05,
      "loss": 0.9659,
      "num_input_tokens_seen": 64512,
      "step": 40
    },
    {
      "epoch": 0.007007349171081866,
      "grad_norm": 1.8082369565963745,
      "learning_rate": 2.05e-05,
      "loss": 1.6134,
      "num_input_tokens_seen": 65664,
      "step": 41
    },
    {
      "epoch": 0.007178260126474107,
      "grad_norm": 1.2012189626693726,
      "learning_rate": 2.1e-05,
      "loss": 1.5689,
      "num_input_tokens_seen": 68480,
      "step": 42
    },
    {
      "epoch": 0.007349171081866348,
      "grad_norm": 1.8300682306289673,
      "learning_rate": 2.15e-05,
      "loss": 1.6702,
      "num_input_tokens_seen": 69760,
      "step": 43
    },
    {
      "epoch": 0.0075200820372585886,
      "grad_norm": 1.7591111660003662,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.5317,
      "num_input_tokens_seen": 71168,
      "step": 44
    },
    {
      "epoch": 0.0076909929926508285,
      "grad_norm": 1.6453062295913696,
      "learning_rate": 2.25e-05,
      "loss": 1.4866,
      "num_input_tokens_seen": 72960,
      "step": 45
    },
    {
      "epoch": 0.00786190394804307,
      "grad_norm": 2.03118896484375,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.6977,
      "num_input_tokens_seen": 74240,
      "step": 46
    },
    {
      "epoch": 0.00803281490343531,
      "grad_norm": 1.69283127784729,
      "learning_rate": 2.35e-05,
      "loss": 1.7054,
      "num_input_tokens_seen": 75520,
      "step": 47
    },
    {
      "epoch": 0.008203725858827551,
      "grad_norm": 2.032726764678955,
      "learning_rate": 2.4e-05,
      "loss": 1.6884,
      "num_input_tokens_seen": 77056,
      "step": 48
    },
    {
      "epoch": 0.008374636814219792,
      "grad_norm": 1.1516492366790771,
      "learning_rate": 2.45e-05,
      "loss": 1.1429,
      "num_input_tokens_seen": 79104,
      "step": 49
    },
    {
      "epoch": 0.008545547769612033,
      "grad_norm": 1.7636171579360962,
      "learning_rate": 2.5e-05,
      "loss": 1.5036,
      "num_input_tokens_seen": 81152,
      "step": 50
    },
    {
      "epoch": 0.008716458725004273,
      "grad_norm": 1.6526856422424316,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.2924,
      "num_input_tokens_seen": 82176,
      "step": 51
    },
    {
      "epoch": 0.008887369680396513,
      "grad_norm": 1.6218656301498413,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1538,
      "num_input_tokens_seen": 83200,
      "step": 52
    },
    {
      "epoch": 0.009058280635788753,
      "grad_norm": 1.2379751205444336,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.4313,
      "num_input_tokens_seen": 85888,
      "step": 53
    },
    {
      "epoch": 0.009229191591180994,
      "grad_norm": 1.3234796524047852,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.594,
      "num_input_tokens_seen": 87424,
      "step": 54
    },
    {
      "epoch": 0.009400102546573235,
      "grad_norm": 1.3863588571548462,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.4737,
      "num_input_tokens_seen": 88576,
      "step": 55
    },
    {
      "epoch": 0.009571013501965476,
      "grad_norm": 1.5558987855911255,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2653,
      "num_input_tokens_seen": 89856,
      "step": 56
    },
    {
      "epoch": 0.009741924457357717,
      "grad_norm": 0.8167198896408081,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.3222,
      "num_input_tokens_seen": 91136,
      "step": 57
    },
    {
      "epoch": 0.009912835412749958,
      "grad_norm": 0.7946738600730896,
      "learning_rate": 2.9e-05,
      "loss": 1.5321,
      "num_input_tokens_seen": 93568,
      "step": 58
    },
    {
      "epoch": 0.010083746368142198,
      "grad_norm": 0.6687101125717163,
      "learning_rate": 2.95e-05,
      "loss": 1.3939,
      "num_input_tokens_seen": 94848,
      "step": 59
    },
    {
      "epoch": 0.01025465732353444,
      "grad_norm": 0.917375922203064,
      "learning_rate": 3e-05,
      "loss": 1.453,
      "num_input_tokens_seen": 96000,
      "step": 60
    },
    {
      "epoch": 0.01042556827892668,
      "grad_norm": 1.1747183799743652,
      "learning_rate": 3.05e-05,
      "loss": 1.5261,
      "num_input_tokens_seen": 97024,
      "step": 61
    },
    {
      "epoch": 0.010596479234318919,
      "grad_norm": 1.0229103565216064,
      "learning_rate": 3.1e-05,
      "loss": 0.9536,
      "num_input_tokens_seen": 98176,
      "step": 62
    },
    {
      "epoch": 0.01076739018971116,
      "grad_norm": 1.06194007396698,
      "learning_rate": 3.15e-05,
      "loss": 1.7945,
      "num_input_tokens_seen": 99456,
      "step": 63
    },
    {
      "epoch": 0.0109383011451034,
      "grad_norm": 0.5723736882209778,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.9876,
      "num_input_tokens_seen": 101888,
      "step": 64
    },
    {
      "epoch": 0.011109212100495642,
      "grad_norm": 0.9505805373191833,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.2816,
      "num_input_tokens_seen": 103168,
      "step": 65
    },
    {
      "epoch": 0.011280123055887882,
      "grad_norm": 0.8175946474075317,
      "learning_rate": 3.3e-05,
      "loss": 1.1442,
      "num_input_tokens_seen": 105472,
      "step": 66
    },
    {
      "epoch": 0.011451034011280123,
      "grad_norm": 0.7320437431335449,
      "learning_rate": 3.35e-05,
      "loss": 1.488,
      "num_input_tokens_seen": 107136,
      "step": 67
    },
    {
      "epoch": 0.011621944966672364,
      "grad_norm": 1.2031220197677612,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.479,
      "num_input_tokens_seen": 108288,
      "step": 68
    },
    {
      "epoch": 0.011792855922064605,
      "grad_norm": 0.7064288258552551,
      "learning_rate": 3.45e-05,
      "loss": 1.6203,
      "num_input_tokens_seen": 110464,
      "step": 69
    },
    {
      "epoch": 0.011963766877456846,
      "grad_norm": 0.5749320387840271,
      "learning_rate": 3.5e-05,
      "loss": 0.9748,
      "num_input_tokens_seen": 113408,
      "step": 70
    },
    {
      "epoch": 0.012134677832849085,
      "grad_norm": 1.1838645935058594,
      "learning_rate": 3.55e-05,
      "loss": 1.5808,
      "num_input_tokens_seen": 114304,
      "step": 71
    },
    {
      "epoch": 0.012305588788241326,
      "grad_norm": 0.644761323928833,
      "learning_rate": 3.6e-05,
      "loss": 1.5768,
      "num_input_tokens_seen": 116352,
      "step": 72
    },
    {
      "epoch": 0.012476499743633566,
      "grad_norm": 0.7350552082061768,
      "learning_rate": 3.65e-05,
      "loss": 1.0829,
      "num_input_tokens_seen": 117760,
      "step": 73
    },
    {
      "epoch": 0.012647410699025807,
      "grad_norm": 0.6727873682975769,
      "learning_rate": 3.7e-05,
      "loss": 1.5023,
      "num_input_tokens_seen": 119936,
      "step": 74
    },
    {
      "epoch": 0.012818321654418048,
      "grad_norm": 0.8383559584617615,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.2761,
      "num_input_tokens_seen": 121728,
      "step": 75
    },
    {
      "epoch": 0.012989232609810289,
      "grad_norm": 0.7713782787322998,
      "learning_rate": 3.8e-05,
      "loss": 1.4994,
      "num_input_tokens_seen": 124160,
      "step": 76
    },
    {
      "epoch": 0.01316014356520253,
      "grad_norm": 0.8367883563041687,
      "learning_rate": 3.85e-05,
      "loss": 1.7855,
      "num_input_tokens_seen": 125568,
      "step": 77
    },
    {
      "epoch": 0.01333105452059477,
      "grad_norm": 0.6851012110710144,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.2485,
      "num_input_tokens_seen": 126976,
      "step": 78
    },
    {
      "epoch": 0.013501965475987011,
      "grad_norm": 0.5357105135917664,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.1775,
      "num_input_tokens_seen": 128896,
      "step": 79
    },
    {
      "epoch": 0.013672876431379252,
      "grad_norm": 1.0880916118621826,
      "learning_rate": 4e-05,
      "loss": 1.4504,
      "num_input_tokens_seen": 130304,
      "step": 80
    },
    {
      "epoch": 0.013843787386771491,
      "grad_norm": 1.1779428720474243,
      "learning_rate": 4.05e-05,
      "loss": 1.2666,
      "num_input_tokens_seen": 131328,
      "step": 81
    },
    {
      "epoch": 0.014014698342163732,
      "grad_norm": 0.7600643038749695,
      "learning_rate": 4.1e-05,
      "loss": 1.2449,
      "num_input_tokens_seen": 132736,
      "step": 82
    },
    {
      "epoch": 0.014185609297555973,
      "grad_norm": 0.5999438762664795,
      "learning_rate": 4.15e-05,
      "loss": 0.914,
      "num_input_tokens_seen": 135424,
      "step": 83
    },
    {
      "epoch": 0.014356520252948214,
      "grad_norm": 0.786974310874939,
      "learning_rate": 4.2e-05,
      "loss": 1.2317,
      "num_input_tokens_seen": 136832,
      "step": 84
    },
    {
      "epoch": 0.014527431208340455,
      "grad_norm": 0.6997110247612,
      "learning_rate": 4.25e-05,
      "loss": 1.186,
      "num_input_tokens_seen": 138240,
      "step": 85
    },
    {
      "epoch": 0.014698342163732695,
      "grad_norm": 0.8596630096435547,
      "learning_rate": 4.3e-05,
      "loss": 1.9606,
      "num_input_tokens_seen": 139520,
      "step": 86
    },
    {
      "epoch": 0.014869253119124936,
      "grad_norm": 0.5809409022331238,
      "learning_rate": 4.35e-05,
      "loss": 1.5102,
      "num_input_tokens_seen": 140800,
      "step": 87
    },
    {
      "epoch": 0.015040164074517177,
      "grad_norm": 1.0727113485336304,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.6939,
      "num_input_tokens_seen": 141952,
      "step": 88
    },
    {
      "epoch": 0.015211075029909418,
      "grad_norm": 0.6151033043861389,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.4108,
      "num_input_tokens_seen": 143616,
      "step": 89
    },
    {
      "epoch": 0.015381985985301657,
      "grad_norm": 0.8659923076629639,
      "learning_rate": 4.5e-05,
      "loss": 1.5611,
      "num_input_tokens_seen": 145024,
      "step": 90
    },
    {
      "epoch": 0.015552896940693898,
      "grad_norm": 0.6640591025352478,
      "learning_rate": 4.55e-05,
      "loss": 1.3593,
      "num_input_tokens_seen": 146560,
      "step": 91
    },
    {
      "epoch": 0.01572380789608614,
      "grad_norm": 0.7715206742286682,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.4082,
      "num_input_tokens_seen": 147712,
      "step": 92
    },
    {
      "epoch": 0.01589471885147838,
      "grad_norm": 0.4368993043899536,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.178,
      "num_input_tokens_seen": 149888,
      "step": 93
    },
    {
      "epoch": 0.01606562980687062,
      "grad_norm": 0.8351138830184937,
      "learning_rate": 4.7e-05,
      "loss": 1.3346,
      "num_input_tokens_seen": 151424,
      "step": 94
    },
    {
      "epoch": 0.01623654076226286,
      "grad_norm": 0.7608038187026978,
      "learning_rate": 4.75e-05,
      "loss": 1.4247,
      "num_input_tokens_seen": 152832,
      "step": 95
    },
    {
      "epoch": 0.016407451717655102,
      "grad_norm": 0.7928726673126221,
      "learning_rate": 4.8e-05,
      "loss": 1.2185,
      "num_input_tokens_seen": 153856,
      "step": 96
    },
    {
      "epoch": 0.016578362673047343,
      "grad_norm": 0.5468449592590332,
      "learning_rate": 4.85e-05,
      "loss": 1.4801,
      "num_input_tokens_seen": 156800,
      "step": 97
    },
    {
      "epoch": 0.016749273628439584,
      "grad_norm": 1.0511754751205444,
      "learning_rate": 4.9e-05,
      "loss": 1.1098,
      "num_input_tokens_seen": 158336,
      "step": 98
    },
    {
      "epoch": 0.016920184583831824,
      "grad_norm": 0.5121496915817261,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 1.2844,
      "num_input_tokens_seen": 160512,
      "step": 99
    },
    {
      "epoch": 0.017091095539224065,
      "grad_norm": 1.2290772199630737,
      "learning_rate": 5e-05,
      "loss": 0.8538,
      "num_input_tokens_seen": 161536,
      "step": 100
    },
    {
      "epoch": 0.017262006494616306,
      "grad_norm": 0.5101739168167114,
      "learning_rate": 4.999984769144476e-05,
      "loss": 1.2967,
      "num_input_tokens_seen": 163456,
      "step": 101
    },
    {
      "epoch": 0.017432917450008547,
      "grad_norm": 1.1004080772399902,
      "learning_rate": 4.999939076763487e-05,
      "loss": 1.4567,
      "num_input_tokens_seen": 164864,
      "step": 102
    },
    {
      "epoch": 0.017603828405400788,
      "grad_norm": 1.1591134071350098,
      "learning_rate": 4.999862923413781e-05,
      "loss": 1.0878,
      "num_input_tokens_seen": 165888,
      "step": 103
    },
    {
      "epoch": 0.017774739360793025,
      "grad_norm": 0.684755802154541,
      "learning_rate": 4.999756310023261e-05,
      "loss": 1.2912,
      "num_input_tokens_seen": 167168,
      "step": 104
    },
    {
      "epoch": 0.017945650316185266,
      "grad_norm": 0.7475521564483643,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 1.6592,
      "num_input_tokens_seen": 168960,
      "step": 105
    },
    {
      "epoch": 0.018116561271577507,
      "grad_norm": 0.9258105754852295,
      "learning_rate": 4.999451708687114e-05,
      "loss": 1.5296,
      "num_input_tokens_seen": 170752,
      "step": 106
    },
    {
      "epoch": 0.018287472226969748,
      "grad_norm": 0.7449769973754883,
      "learning_rate": 4.999253724452958e-05,
      "loss": 1.4904,
      "num_input_tokens_seen": 172800,
      "step": 107
    },
    {
      "epoch": 0.01845838318236199,
      "grad_norm": 0.8927878141403198,
      "learning_rate": 4.999025287600886e-05,
      "loss": 1.0696,
      "num_input_tokens_seen": 174208,
      "step": 108
    },
    {
      "epoch": 0.01862929413775423,
      "grad_norm": 0.7548288106918335,
      "learning_rate": 4.998766400914329e-05,
      "loss": 1.1822,
      "num_input_tokens_seen": 175488,
      "step": 109
    },
    {
      "epoch": 0.01880020509314647,
      "grad_norm": 1.0622750520706177,
      "learning_rate": 4.99847706754774e-05,
      "loss": 1.7487,
      "num_input_tokens_seen": 176512,
      "step": 110
    },
    {
      "epoch": 0.01897111604853871,
      "grad_norm": 1.0703930854797363,
      "learning_rate": 4.998157291026553e-05,
      "loss": 1.0201,
      "num_input_tokens_seen": 177408,
      "step": 111
    },
    {
      "epoch": 0.019142027003930952,
      "grad_norm": 0.6748459935188293,
      "learning_rate": 4.997807075247146e-05,
      "loss": 1.4408,
      "num_input_tokens_seen": 178944,
      "step": 112
    },
    {
      "epoch": 0.019312937959323193,
      "grad_norm": 0.6887047290802002,
      "learning_rate": 4.997426424476787e-05,
      "loss": 1.3952,
      "num_input_tokens_seen": 180224,
      "step": 113
    },
    {
      "epoch": 0.019483848914715433,
      "grad_norm": 0.615265965461731,
      "learning_rate": 4.997015343353585e-05,
      "loss": 1.5401,
      "num_input_tokens_seen": 181888,
      "step": 114
    },
    {
      "epoch": 0.019654759870107674,
      "grad_norm": 0.9489994049072266,
      "learning_rate": 4.996573836886435e-05,
      "loss": 1.3389,
      "num_input_tokens_seen": 183296,
      "step": 115
    },
    {
      "epoch": 0.019825670825499915,
      "grad_norm": 0.5779714584350586,
      "learning_rate": 4.996101910454953e-05,
      "loss": 1.1354,
      "num_input_tokens_seen": 184960,
      "step": 116
    },
    {
      "epoch": 0.019996581780892156,
      "grad_norm": 0.7449826002120972,
      "learning_rate": 4.995599569809414e-05,
      "loss": 1.452,
      "num_input_tokens_seen": 186496,
      "step": 117
    },
    {
      "epoch": 0.020167492736284397,
      "grad_norm": 0.592430830001831,
      "learning_rate": 4.995066821070679e-05,
      "loss": 1.1822,
      "num_input_tokens_seen": 187904,
      "step": 118
    },
    {
      "epoch": 0.020338403691676638,
      "grad_norm": 0.644411027431488,
      "learning_rate": 4.994503670730125e-05,
      "loss": 1.6817,
      "num_input_tokens_seen": 190080,
      "step": 119
    },
    {
      "epoch": 0.02050931464706888,
      "grad_norm": 1.1836949586868286,
      "learning_rate": 4.993910125649561e-05,
      "loss": 1.32,
      "num_input_tokens_seen": 191104,
      "step": 120
    },
    {
      "epoch": 0.02068022560246112,
      "grad_norm": 0.619757354259491,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 1.3187,
      "num_input_tokens_seen": 192512,
      "step": 121
    },
    {
      "epoch": 0.02085113655785336,
      "grad_norm": 0.8432345390319824,
      "learning_rate": 4.992631880567301e-05,
      "loss": 1.2319,
      "num_input_tokens_seen": 193664,
      "step": 122
    },
    {
      "epoch": 0.021022047513245597,
      "grad_norm": 1.1152762174606323,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.7571,
      "num_input_tokens_seen": 194688,
      "step": 123
    },
    {
      "epoch": 0.021192958468637838,
      "grad_norm": 0.7259035706520081,
      "learning_rate": 4.991232148123761e-05,
      "loss": 1.5293,
      "num_input_tokens_seen": 195968,
      "step": 124
    },
    {
      "epoch": 0.02136386942403008,
      "grad_norm": 0.9862359166145325,
      "learning_rate": 4.990486745229364e-05,
      "loss": 1.0472,
      "num_input_tokens_seen": 197248,
      "step": 125
    },
    {
      "epoch": 0.02153478037942232,
      "grad_norm": 0.9642518162727356,
      "learning_rate": 4.989710996539926e-05,
      "loss": 1.4137,
      "num_input_tokens_seen": 198912,
      "step": 126
    },
    {
      "epoch": 0.02170569133481456,
      "grad_norm": 0.6264151334762573,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 1.5312,
      "num_input_tokens_seen": 202240,
      "step": 127
    },
    {
      "epoch": 0.0218766022902068,
      "grad_norm": 1.0080753564834595,
      "learning_rate": 4.988068499954578e-05,
      "loss": 1.1207,
      "num_input_tokens_seen": 203904,
      "step": 128
    },
    {
      "epoch": 0.022047513245599042,
      "grad_norm": 0.8976799845695496,
      "learning_rate": 4.987201772071971e-05,
      "loss": 1.5814,
      "num_input_tokens_seen": 205440,
      "step": 129
    },
    {
      "epoch": 0.022218424200991283,
      "grad_norm": 0.7827826142311096,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 1.3746,
      "num_input_tokens_seen": 206848,
      "step": 130
    },
    {
      "epoch": 0.022389335156383524,
      "grad_norm": 0.5148786902427673,
      "learning_rate": 4.985377409930789e-05,
      "loss": 1.4287,
      "num_input_tokens_seen": 208640,
      "step": 131
    },
    {
      "epoch": 0.022560246111775765,
      "grad_norm": 1.241485357284546,
      "learning_rate": 4.984419797901491e-05,
      "loss": 1.3048,
      "num_input_tokens_seen": 210048,
      "step": 132
    },
    {
      "epoch": 0.022731157067168006,
      "grad_norm": 0.7774345278739929,
      "learning_rate": 4.983431914000991e-05,
      "loss": 1.219,
      "num_input_tokens_seen": 211712,
      "step": 133
    },
    {
      "epoch": 0.022902068022560246,
      "grad_norm": 0.7557172775268555,
      "learning_rate": 4.982413770266342e-05,
      "loss": 1.7593,
      "num_input_tokens_seen": 213248,
      "step": 134
    },
    {
      "epoch": 0.023072978977952487,
      "grad_norm": 1.497115969657898,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 1.4127,
      "num_input_tokens_seen": 214272,
      "step": 135
    },
    {
      "epoch": 0.023243889933344728,
      "grad_norm": 1.0337986946105957,
      "learning_rate": 4.980286753286195e-05,
      "loss": 1.043,
      "num_input_tokens_seen": 215424,
      "step": 136
    },
    {
      "epoch": 0.02341480088873697,
      "grad_norm": 0.8167257308959961,
      "learning_rate": 4.979177905957726e-05,
      "loss": 1.3745,
      "num_input_tokens_seen": 216960,
      "step": 137
    },
    {
      "epoch": 0.02358571184412921,
      "grad_norm": 1.363171100616455,
      "learning_rate": 4.978038850628854e-05,
      "loss": 1.1099,
      "num_input_tokens_seen": 217984,
      "step": 138
    },
    {
      "epoch": 0.02375662279952145,
      "grad_norm": 0.7164723873138428,
      "learning_rate": 4.976869601178609e-05,
      "loss": 1.3872,
      "num_input_tokens_seen": 220544,
      "step": 139
    },
    {
      "epoch": 0.02392753375491369,
      "grad_norm": 0.9667837619781494,
      "learning_rate": 4.975670171853926e-05,
      "loss": 1.2518,
      "num_input_tokens_seen": 221696,
      "step": 140
    },
    {
      "epoch": 0.024098444710305932,
      "grad_norm": 0.7958712577819824,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 1.2651,
      "num_input_tokens_seen": 222976,
      "step": 141
    },
    {
      "epoch": 0.02426935566569817,
      "grad_norm": 0.8773461580276489,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 1.8709,
      "num_input_tokens_seen": 224512,
      "step": 142
    },
    {
      "epoch": 0.02444026662109041,
      "grad_norm": 1.1198506355285645,
      "learning_rate": 4.971890952617515e-05,
      "loss": 1.382,
      "num_input_tokens_seen": 225792,
      "step": 143
    },
    {
      "epoch": 0.02461117757648265,
      "grad_norm": 0.9605275988578796,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 1.5515,
      "num_input_tokens_seen": 226816,
      "step": 144
    },
    {
      "epoch": 0.024782088531874892,
      "grad_norm": 0.918910026550293,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 1.6442,
      "num_input_tokens_seen": 229248,
      "step": 145
    },
    {
      "epoch": 0.024952999487267133,
      "grad_norm": 1.0211495161056519,
      "learning_rate": 4.96784066268247e-05,
      "loss": 1.3855,
      "num_input_tokens_seen": 230656,
      "step": 146
    },
    {
      "epoch": 0.025123910442659374,
      "grad_norm": 0.9038419127464294,
      "learning_rate": 4.966430404017424e-05,
      "loss": 1.4091,
      "num_input_tokens_seen": 232064,
      "step": 147
    },
    {
      "epoch": 0.025294821398051615,
      "grad_norm": 0.7765509486198425,
      "learning_rate": 4.964990092676263e-05,
      "loss": 1.4867,
      "num_input_tokens_seen": 233728,
      "step": 148
    },
    {
      "epoch": 0.025465732353443855,
      "grad_norm": 0.8611145615577698,
      "learning_rate": 4.963519746208726e-05,
      "loss": 1.3581,
      "num_input_tokens_seen": 235008,
      "step": 149
    },
    {
      "epoch": 0.025636643308836096,
      "grad_norm": 1.4356282949447632,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.8524,
      "num_input_tokens_seen": 235776,
      "step": 150
    },
    {
      "epoch": 0.025807554264228337,
      "grad_norm": 0.8072199821472168,
      "learning_rate": 4.960489019923105e-05,
      "loss": 1.5263,
      "num_input_tokens_seen": 237440,
      "step": 151
    },
    {
      "epoch": 0.025978465219620578,
      "grad_norm": 1.069689154624939,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 1.2619,
      "num_input_tokens_seen": 238720,
      "step": 152
    },
    {
      "epoch": 0.02614937617501282,
      "grad_norm": 0.9440941214561462,
      "learning_rate": 4.957338372873886e-05,
      "loss": 1.4025,
      "num_input_tokens_seen": 240128,
      "step": 153
    },
    {
      "epoch": 0.02632028713040506,
      "grad_norm": 0.6023850440979004,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.8964,
      "num_input_tokens_seen": 242560,
      "step": 154
    },
    {
      "epoch": 0.0264911980857973,
      "grad_norm": 0.7971007227897644,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 1.4682,
      "num_input_tokens_seen": 244608,
      "step": 155
    },
    {
      "epoch": 0.02666210904118954,
      "grad_norm": 0.9614905118942261,
      "learning_rate": 4.952387888372979e-05,
      "loss": 1.1424,
      "num_input_tokens_seen": 245760,
      "step": 156
    },
    {
      "epoch": 0.026833019996581782,
      "grad_norm": 1.1557332277297974,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 1.4969,
      "num_input_tokens_seen": 247296,
      "step": 157
    },
    {
      "epoch": 0.027003930951974023,
      "grad_norm": 1.0219954252243042,
      "learning_rate": 4.94893812399836e-05,
      "loss": 1.5463,
      "num_input_tokens_seen": 248448,
      "step": 158
    },
    {
      "epoch": 0.027174841907366264,
      "grad_norm": 0.7660773992538452,
      "learning_rate": 4.947168471904213e-05,
      "loss": 1.0477,
      "num_input_tokens_seen": 249856,
      "step": 159
    },
    {
      "epoch": 0.027345752862758504,
      "grad_norm": 1.0947810411453247,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 1.5477,
      "num_input_tokens_seen": 251392,
      "step": 160
    },
    {
      "epoch": 0.027516663818150742,
      "grad_norm": 1.1583284139633179,
      "learning_rate": 4.94353973571524e-05,
      "loss": 1.0984,
      "num_input_tokens_seen": 252416,
      "step": 161
    },
    {
      "epoch": 0.027687574773542983,
      "grad_norm": 1.5384682416915894,
      "learning_rate": 4.94168069583542e-05,
      "loss": 1.4722,
      "num_input_tokens_seen": 253696,
      "step": 162
    },
    {
      "epoch": 0.027858485728935223,
      "grad_norm": 1.0456067323684692,
      "learning_rate": 4.939791904846869e-05,
      "loss": 1.2568,
      "num_input_tokens_seen": 255104,
      "step": 163
    },
    {
      "epoch": 0.028029396684327464,
      "grad_norm": 0.946577250957489,
      "learning_rate": 4.937873385763908e-05,
      "loss": 1.5566,
      "num_input_tokens_seen": 257408,
      "step": 164
    },
    {
      "epoch": 0.028200307639719705,
      "grad_norm": 0.8265803456306458,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 1.4875,
      "num_input_tokens_seen": 259584,
      "step": 165
    },
    {
      "epoch": 0.028371218595111946,
      "grad_norm": 1.0739526748657227,
      "learning_rate": 4.933947257182901e-05,
      "loss": 1.6229,
      "num_input_tokens_seen": 260992,
      "step": 166
    },
    {
      "epoch": 0.028542129550504187,
      "grad_norm": 0.8965349197387695,
      "learning_rate": 4.931939695523492e-05,
      "loss": 1.3885,
      "num_input_tokens_seen": 262400,
      "step": 167
    },
    {
      "epoch": 0.028713040505896428,
      "grad_norm": 0.8494608402252197,
      "learning_rate": 4.929902501446366e-05,
      "loss": 1.3819,
      "num_input_tokens_seen": 264192,
      "step": 168
    },
    {
      "epoch": 0.02888395146128867,
      "grad_norm": 0.9903549551963806,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 1.4564,
      "num_input_tokens_seen": 265728,
      "step": 169
    },
    {
      "epoch": 0.02905486241668091,
      "grad_norm": 1.455828070640564,
      "learning_rate": 4.925739315689991e-05,
      "loss": 1.303,
      "num_input_tokens_seen": 267008,
      "step": 170
    },
    {
      "epoch": 0.02922577337207315,
      "grad_norm": 1.2292678356170654,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 1.3495,
      "num_input_tokens_seen": 268416,
      "step": 171
    },
    {
      "epoch": 0.02939668432746539,
      "grad_norm": 0.885446310043335,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 1.2739,
      "num_input_tokens_seen": 270208,
      "step": 172
    },
    {
      "epoch": 0.029567595282857632,
      "grad_norm": 1.1462482213974,
      "learning_rate": 4.919272926204929e-05,
      "loss": 1.1086,
      "num_input_tokens_seen": 271232,
      "step": 173
    },
    {
      "epoch": 0.029738506238249873,
      "grad_norm": 1.0181411504745483,
      "learning_rate": 4.917058471511149e-05,
      "loss": 1.2721,
      "num_input_tokens_seen": 272512,
      "step": 174
    },
    {
      "epoch": 0.029909417193642113,
      "grad_norm": 1.1953258514404297,
      "learning_rate": 4.914814565722671e-05,
      "loss": 1.3515,
      "num_input_tokens_seen": 273920,
      "step": 175
    },
    {
      "epoch": 0.030080328149034354,
      "grad_norm": 1.3964471817016602,
      "learning_rate": 4.912541236180779e-05,
      "loss": 1.3063,
      "num_input_tokens_seen": 275072,
      "step": 176
    },
    {
      "epoch": 0.030251239104426595,
      "grad_norm": 1.2992331981658936,
      "learning_rate": 4.910238510585276e-05,
      "loss": 1.2526,
      "num_input_tokens_seen": 275968,
      "step": 177
    },
    {
      "epoch": 0.030422150059818836,
      "grad_norm": 1.1683461666107178,
      "learning_rate": 4.907906416994146e-05,
      "loss": 1.3159,
      "num_input_tokens_seen": 277632,
      "step": 178
    },
    {
      "epoch": 0.030593061015211077,
      "grad_norm": 0.9948767423629761,
      "learning_rate": 4.905544983823214e-05,
      "loss": 1.5395,
      "num_input_tokens_seen": 279808,
      "step": 179
    },
    {
      "epoch": 0.030763971970603314,
      "grad_norm": 1.0310691595077515,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 1.2308,
      "num_input_tokens_seen": 281728,
      "step": 180
    },
    {
      "epoch": 0.030934882925995555,
      "grad_norm": 1.1389057636260986,
      "learning_rate": 4.900734214192358e-05,
      "loss": 1.0003,
      "num_input_tokens_seen": 283520,
      "step": 181
    },
    {
      "epoch": 0.031105793881387796,
      "grad_norm": 0.809341549873352,
      "learning_rate": 4.898284936350144e-05,
      "loss": 1.2575,
      "num_input_tokens_seen": 285184,
      "step": 182
    },
    {
      "epoch": 0.03127670483678004,
      "grad_norm": 0.8440907597541809,
      "learning_rate": 4.895806436162833e-05,
      "loss": 1.531,
      "num_input_tokens_seen": 287360,
      "step": 183
    },
    {
      "epoch": 0.03144761579217228,
      "grad_norm": 1.4059702157974243,
      "learning_rate": 4.893298743830168e-05,
      "loss": 1.1626,
      "num_input_tokens_seen": 288512,
      "step": 184
    },
    {
      "epoch": 0.03161852674756452,
      "grad_norm": 1.2913812398910522,
      "learning_rate": 4.890761889907589e-05,
      "loss": 1.3262,
      "num_input_tokens_seen": 289792,
      "step": 185
    },
    {
      "epoch": 0.03178943770295676,
      "grad_norm": 1.0097531080245972,
      "learning_rate": 4.888195905305859e-05,
      "loss": 1.2282,
      "num_input_tokens_seen": 291072,
      "step": 186
    },
    {
      "epoch": 0.031960348658349,
      "grad_norm": 0.9522632360458374,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 1.3982,
      "num_input_tokens_seen": 292736,
      "step": 187
    },
    {
      "epoch": 0.03213125961374124,
      "grad_norm": 1.0762457847595215,
      "learning_rate": 4.882976669482367e-05,
      "loss": 1.4673,
      "num_input_tokens_seen": 293760,
      "step": 188
    },
    {
      "epoch": 0.03230217056913348,
      "grad_norm": 0.6646410226821899,
      "learning_rate": 4.880323481855347e-05,
      "loss": 1.5692,
      "num_input_tokens_seen": 295808,
      "step": 189
    },
    {
      "epoch": 0.03247308152452572,
      "grad_norm": 1.0720038414001465,
      "learning_rate": 4.877641290737884e-05,
      "loss": 1.5434,
      "num_input_tokens_seen": 297088,
      "step": 190
    },
    {
      "epoch": 0.03264399247991796,
      "grad_norm": 1.089917778968811,
      "learning_rate": 4.874930128811631e-05,
      "loss": 1.5314,
      "num_input_tokens_seen": 298368,
      "step": 191
    },
    {
      "epoch": 0.032814903435310204,
      "grad_norm": 0.8361433148384094,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 1.434,
      "num_input_tokens_seen": 300672,
      "step": 192
    },
    {
      "epoch": 0.032985814390702445,
      "grad_norm": 1.4376323223114014,
      "learning_rate": 4.869421025023965e-05,
      "loss": 0.8042,
      "num_input_tokens_seen": 302208,
      "step": 193
    },
    {
      "epoch": 0.033156725346094686,
      "grad_norm": 1.5424588918685913,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 1.1029,
      "num_input_tokens_seen": 302976,
      "step": 194
    },
    {
      "epoch": 0.033327636301486926,
      "grad_norm": 0.8246604800224304,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 1.003,
      "num_input_tokens_seen": 304128,
      "step": 195
    },
    {
      "epoch": 0.03349854725687917,
      "grad_norm": 1.0136905908584595,
      "learning_rate": 4.860940925593703e-05,
      "loss": 1.3321,
      "num_input_tokens_seen": 305536,
      "step": 196
    },
    {
      "epoch": 0.03366945821227141,
      "grad_norm": 0.9958058595657349,
      "learning_rate": 4.858056644869002e-05,
      "loss": 1.3162,
      "num_input_tokens_seen": 306688,
      "step": 197
    },
    {
      "epoch": 0.03384036916766365,
      "grad_norm": 1.1347723007202148,
      "learning_rate": 4.855143631968242e-05,
      "loss": 2.0938,
      "num_input_tokens_seen": 308096,
      "step": 198
    },
    {
      "epoch": 0.03401128012305589,
      "grad_norm": 1.0530468225479126,
      "learning_rate": 4.852201922385564e-05,
      "loss": 1.5711,
      "num_input_tokens_seen": 309760,
      "step": 199
    },
    {
      "epoch": 0.03418219107844813,
      "grad_norm": 1.4536043405532837,
      "learning_rate": 4.849231551964771e-05,
      "loss": 1.0984,
      "num_input_tokens_seen": 311168,
      "step": 200
    },
    {
      "epoch": 0.03435310203384037,
      "grad_norm": 0.9897109270095825,
      "learning_rate": 4.84623255689889e-05,
      "loss": 1.4184,
      "num_input_tokens_seen": 312576,
      "step": 201
    },
    {
      "epoch": 0.03452401298923261,
      "grad_norm": 0.8617970943450928,
      "learning_rate": 4.843204973729729e-05,
      "loss": 1.4118,
      "num_input_tokens_seen": 314496,
      "step": 202
    },
    {
      "epoch": 0.03469492394462485,
      "grad_norm": 0.9445804953575134,
      "learning_rate": 4.840148839347434e-05,
      "loss": 1.3193,
      "num_input_tokens_seen": 316416,
      "step": 203
    },
    {
      "epoch": 0.034865834900017094,
      "grad_norm": 1.0884565114974976,
      "learning_rate": 4.837064190990036e-05,
      "loss": 1.2833,
      "num_input_tokens_seen": 317696,
      "step": 204
    },
    {
      "epoch": 0.035036745855409335,
      "grad_norm": 0.9798941016197205,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 1.6102,
      "num_input_tokens_seen": 318976,
      "step": 205
    },
    {
      "epoch": 0.035207656810801576,
      "grad_norm": 0.9384295344352722,
      "learning_rate": 4.830809503038781e-05,
      "loss": 1.4229,
      "num_input_tokens_seen": 320896,
      "step": 206
    },
    {
      "epoch": 0.035378567766193816,
      "grad_norm": 1.3050521612167358,
      "learning_rate": 4.827639539656321e-05,
      "loss": 1.4039,
      "num_input_tokens_seen": 321920,
      "step": 207
    },
    {
      "epoch": 0.03554947872158605,
      "grad_norm": 1.1776509284973145,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 1.4916,
      "num_input_tokens_seen": 323200,
      "step": 208
    },
    {
      "epoch": 0.03572038967697829,
      "grad_norm": 0.9341827034950256,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 1.421,
      "num_input_tokens_seen": 324608,
      "step": 209
    },
    {
      "epoch": 0.03589130063237053,
      "grad_norm": 1.0541183948516846,
      "learning_rate": 4.817959636416969e-05,
      "loss": 1.1077,
      "num_input_tokens_seen": 326272,
      "step": 210
    },
    {
      "epoch": 0.03606221158776277,
      "grad_norm": 1.0567307472229004,
      "learning_rate": 4.814676462024988e-05,
      "loss": 1.1873,
      "num_input_tokens_seen": 327552,
      "step": 211
    },
    {
      "epoch": 0.036233122543155014,
      "grad_norm": 0.9638366103172302,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 1.471,
      "num_input_tokens_seen": 328832,
      "step": 212
    },
    {
      "epoch": 0.036404033498547254,
      "grad_norm": 1.1310293674468994,
      "learning_rate": 4.808025542782453e-05,
      "loss": 1.5184,
      "num_input_tokens_seen": 330240,
      "step": 213
    },
    {
      "epoch": 0.036574944453939495,
      "grad_norm": 1.5521392822265625,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 1.3724,
      "num_input_tokens_seen": 331520,
      "step": 214
    },
    {
      "epoch": 0.036745855409331736,
      "grad_norm": 0.9737457036972046,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 1.2453,
      "num_input_tokens_seen": 333184,
      "step": 215
    },
    {
      "epoch": 0.03691676636472398,
      "grad_norm": 1.7642221450805664,
      "learning_rate": 4.797838348138086e-05,
      "loss": 1.479,
      "num_input_tokens_seen": 334208,
      "step": 216
    },
    {
      "epoch": 0.03708767732011622,
      "grad_norm": 1.390684962272644,
      "learning_rate": 4.794386564209953e-05,
      "loss": 1.356,
      "num_input_tokens_seen": 335488,
      "step": 217
    },
    {
      "epoch": 0.03725858827550846,
      "grad_norm": 1.1304757595062256,
      "learning_rate": 4.790906823905599e-05,
      "loss": 1.8002,
      "num_input_tokens_seen": 336640,
      "step": 218
    },
    {
      "epoch": 0.0374294992309007,
      "grad_norm": 1.9855809211730957,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 1.4861,
      "num_input_tokens_seen": 337408,
      "step": 219
    },
    {
      "epoch": 0.03760041018629294,
      "grad_norm": 1.1003497838974,
      "learning_rate": 4.783863644106502e-05,
      "loss": 1.5227,
      "num_input_tokens_seen": 339200,
      "step": 220
    },
    {
      "epoch": 0.03777132114168518,
      "grad_norm": 1.0927867889404297,
      "learning_rate": 4.780300290430682e-05,
      "loss": 1.5344,
      "num_input_tokens_seen": 341248,
      "step": 221
    },
    {
      "epoch": 0.03794223209707742,
      "grad_norm": 1.0769299268722534,
      "learning_rate": 4.776709152015443e-05,
      "loss": 1.5009,
      "num_input_tokens_seen": 343040,
      "step": 222
    },
    {
      "epoch": 0.03811314305246966,
      "grad_norm": 1.1011992692947388,
      "learning_rate": 4.773090272617672e-05,
      "loss": 1.2626,
      "num_input_tokens_seen": 344320,
      "step": 223
    },
    {
      "epoch": 0.038284054007861903,
      "grad_norm": 0.7993846535682678,
      "learning_rate": 4.769443696332272e-05,
      "loss": 1.2836,
      "num_input_tokens_seen": 345984,
      "step": 224
    },
    {
      "epoch": 0.038454964963254144,
      "grad_norm": 2.065643548965454,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.836,
      "num_input_tokens_seen": 346880,
      "step": 225
    },
    {
      "epoch": 0.038625875918646385,
      "grad_norm": 0.8909471035003662,
      "learning_rate": 4.762067631165049e-05,
      "loss": 1.2099,
      "num_input_tokens_seen": 348160,
      "step": 226
    },
    {
      "epoch": 0.038796786874038626,
      "grad_norm": 1.1077932119369507,
      "learning_rate": 4.758338232158252e-05,
      "loss": 1.6885,
      "num_input_tokens_seen": 349568,
      "step": 227
    },
    {
      "epoch": 0.03896769782943087,
      "grad_norm": 1.2809644937515259,
      "learning_rate": 4.754581316012785e-05,
      "loss": 1.1606,
      "num_input_tokens_seen": 350848,
      "step": 228
    },
    {
      "epoch": 0.03913860878482311,
      "grad_norm": 0.9842413067817688,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 1.5352,
      "num_input_tokens_seen": 352256,
      "step": 229
    },
    {
      "epoch": 0.03930951974021535,
      "grad_norm": 0.6726354360580444,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 1.2792,
      "num_input_tokens_seen": 355072,
      "step": 230
    },
    {
      "epoch": 0.03948043069560759,
      "grad_norm": 1.432436466217041,
      "learning_rate": 4.743145924185821e-05,
      "loss": 1.255,
      "num_input_tokens_seen": 356352,
      "step": 231
    },
    {
      "epoch": 0.03965134165099983,
      "grad_norm": 0.9129321575164795,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 1.3923,
      "num_input_tokens_seen": 358016,
      "step": 232
    },
    {
      "epoch": 0.03982225260639207,
      "grad_norm": 0.9230162501335144,
      "learning_rate": 4.73538559209842e-05,
      "loss": 1.3192,
      "num_input_tokens_seen": 360064,
      "step": 233
    },
    {
      "epoch": 0.03999316356178431,
      "grad_norm": 1.4387879371643066,
      "learning_rate": 4.731464546130314e-05,
      "loss": 0.7217,
      "num_input_tokens_seen": 361344,
      "step": 234
    },
    {
      "epoch": 0.04016407451717655,
      "grad_norm": 1.2109190225601196,
      "learning_rate": 4.72751631047092e-05,
      "loss": 1.9336,
      "num_input_tokens_seen": 363392,
      "step": 235
    },
    {
      "epoch": 0.04033498547256879,
      "grad_norm": 0.9540138244628906,
      "learning_rate": 4.723540933228244e-05,
      "loss": 1.2537,
      "num_input_tokens_seen": 364672,
      "step": 236
    },
    {
      "epoch": 0.040505896427961034,
      "grad_norm": 1.1448915004730225,
      "learning_rate": 4.719538462841003e-05,
      "loss": 1.0758,
      "num_input_tokens_seen": 366592,
      "step": 237
    },
    {
      "epoch": 0.040676807383353275,
      "grad_norm": 1.851271390914917,
      "learning_rate": 4.715508948078037e-05,
      "loss": 1.1332,
      "num_input_tokens_seen": 367744,
      "step": 238
    },
    {
      "epoch": 0.040847718338745516,
      "grad_norm": 0.822655975818634,
      "learning_rate": 4.71145243803771e-05,
      "loss": 1.3209,
      "num_input_tokens_seen": 370048,
      "step": 239
    },
    {
      "epoch": 0.04101862929413776,
      "grad_norm": 1.0784082412719727,
      "learning_rate": 4.707368982147318e-05,
      "loss": 1.1532,
      "num_input_tokens_seen": 371200,
      "step": 240
    },
    {
      "epoch": 0.04118954024953,
      "grad_norm": 0.994678258895874,
      "learning_rate": 4.70325863016248e-05,
      "loss": 1.4875,
      "num_input_tokens_seen": 373504,
      "step": 241
    },
    {
      "epoch": 0.04136045120492224,
      "grad_norm": 1.121070146560669,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 1.0489,
      "num_input_tokens_seen": 374528,
      "step": 242
    },
    {
      "epoch": 0.04153136216031448,
      "grad_norm": 1.1989063024520874,
      "learning_rate": 4.694957438569951e-05,
      "loss": 1.7031,
      "num_input_tokens_seen": 375936,
      "step": 243
    },
    {
      "epoch": 0.04170227311570672,
      "grad_norm": 1.686545968055725,
      "learning_rate": 4.690766700109659e-05,
      "loss": 1.0009,
      "num_input_tokens_seen": 376960,
      "step": 244
    },
    {
      "epoch": 0.04187318407109896,
      "grad_norm": 2.2535016536712646,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 1.3291,
      "num_input_tokens_seen": 377856,
      "step": 245
    },
    {
      "epoch": 0.042044095026491195,
      "grad_norm": 2.0019567012786865,
      "learning_rate": 4.682305193174524e-05,
      "loss": 1.6011,
      "num_input_tokens_seen": 379136,
      "step": 246
    },
    {
      "epoch": 0.042215005981883436,
      "grad_norm": 0.9853203892707825,
      "learning_rate": 4.678034527800474e-05,
      "loss": 1.1621,
      "num_input_tokens_seen": 380544,
      "step": 247
    },
    {
      "epoch": 0.042385916937275676,
      "grad_norm": 1.277783751487732,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 1.5361,
      "num_input_tokens_seen": 381952,
      "step": 248
    },
    {
      "epoch": 0.04255682789266792,
      "grad_norm": 1.406646728515625,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.9955,
      "num_input_tokens_seen": 382976,
      "step": 249
    },
    {
      "epoch": 0.04272773884806016,
      "grad_norm": 1.3983416557312012,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.9082,
      "num_input_tokens_seen": 383872,
      "step": 250
    },
    {
      "epoch": 0.0428986498034524,
      "grad_norm": 1.131485104560852,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 1.2518,
      "num_input_tokens_seen": 385280,
      "step": 251
    },
    {
      "epoch": 0.04306956075884464,
      "grad_norm": 1.0953209400177002,
      "learning_rate": 4.656284173018144e-05,
      "loss": 1.3341,
      "num_input_tokens_seen": 387200,
      "step": 252
    },
    {
      "epoch": 0.04324047171423688,
      "grad_norm": 1.443863868713379,
      "learning_rate": 4.65185506750986e-05,
      "loss": 1.3498,
      "num_input_tokens_seen": 388352,
      "step": 253
    },
    {
      "epoch": 0.04341138266962912,
      "grad_norm": 1.4619327783584595,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 1.6584,
      "num_input_tokens_seen": 389888,
      "step": 254
    },
    {
      "epoch": 0.04358229362502136,
      "grad_norm": 1.1309338808059692,
      "learning_rate": 4.642918251755281e-05,
      "loss": 1.6078,
      "num_input_tokens_seen": 391424,
      "step": 255
    },
    {
      "epoch": 0.0437532045804136,
      "grad_norm": 0.7200770974159241,
      "learning_rate": 4.638410650401267e-05,
      "loss": 1.4567,
      "num_input_tokens_seen": 393856,
      "step": 256
    },
    {
      "epoch": 0.043924115535805844,
      "grad_norm": 1.4722461700439453,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 1.3162,
      "num_input_tokens_seen": 395136,
      "step": 257
    },
    {
      "epoch": 0.044095026491198085,
      "grad_norm": 1.1144517660140991,
      "learning_rate": 4.629317335357619e-05,
      "loss": 1.6717,
      "num_input_tokens_seen": 396928,
      "step": 258
    },
    {
      "epoch": 0.044265937446590325,
      "grad_norm": 1.3883264064788818,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 1.2954,
      "num_input_tokens_seen": 398464,
      "step": 259
    },
    {
      "epoch": 0.044436848401982566,
      "grad_norm": 1.3202756643295288,
      "learning_rate": 4.620120240391065e-05,
      "loss": 1.1652,
      "num_input_tokens_seen": 399488,
      "step": 260
    },
    {
      "epoch": 0.04460775935737481,
      "grad_norm": 1.2338930368423462,
      "learning_rate": 4.615482915318911e-05,
      "loss": 1.1752,
      "num_input_tokens_seen": 401024,
      "step": 261
    },
    {
      "epoch": 0.04477867031276705,
      "grad_norm": 0.9379393458366394,
      "learning_rate": 4.610819813755038e-05,
      "loss": 1.5592,
      "num_input_tokens_seen": 402560,
      "step": 262
    },
    {
      "epoch": 0.04494958126815929,
      "grad_norm": 1.2147589921951294,
      "learning_rate": 4.606130992517869e-05,
      "loss": 1.4388,
      "num_input_tokens_seen": 403840,
      "step": 263
    },
    {
      "epoch": 0.04512049222355153,
      "grad_norm": 0.9817895293235779,
      "learning_rate": 4.601416508739211e-05,
      "loss": 1.4825,
      "num_input_tokens_seen": 405120,
      "step": 264
    },
    {
      "epoch": 0.04529140317894377,
      "grad_norm": 0.7684583067893982,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 1.5939,
      "num_input_tokens_seen": 407424,
      "step": 265
    },
    {
      "epoch": 0.04546231413433601,
      "grad_norm": 1.132185697555542,
      "learning_rate": 4.591910783647404e-05,
      "loss": 1.3629,
      "num_input_tokens_seen": 408704,
      "step": 266
    },
    {
      "epoch": 0.04563322508972825,
      "grad_norm": 1.338149905204773,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 1.0794,
      "num_input_tokens_seen": 409984,
      "step": 267
    },
    {
      "epoch": 0.04580413604512049,
      "grad_norm": 1.0775296688079834,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 1.2033,
      "num_input_tokens_seen": 411392,
      "step": 268
    },
    {
      "epoch": 0.045975047000512734,
      "grad_norm": 0.9750938415527344,
      "learning_rate": 4.577461173185821e-05,
      "loss": 1.0872,
      "num_input_tokens_seen": 414080,
      "step": 269
    },
    {
      "epoch": 0.046145957955904975,
      "grad_norm": 1.2092121839523315,
      "learning_rate": 4.572593931387604e-05,
      "loss": 1.1725,
      "num_input_tokens_seen": 414976,
      "step": 270
    },
    {
      "epoch": 0.046316868911297215,
      "grad_norm": 1.2218600511550903,
      "learning_rate": 4.567701435686404e-05,
      "loss": 1.4686,
      "num_input_tokens_seen": 415872,
      "step": 271
    },
    {
      "epoch": 0.046487779866689456,
      "grad_norm": 1.1105107069015503,
      "learning_rate": 4.562783745695738e-05,
      "loss": 1.3864,
      "num_input_tokens_seen": 417152,
      "step": 272
    },
    {
      "epoch": 0.0466586908220817,
      "grad_norm": 1.1976479291915894,
      "learning_rate": 4.557840921336105e-05,
      "loss": 1.2384,
      "num_input_tokens_seen": 418304,
      "step": 273
    },
    {
      "epoch": 0.04682960177747394,
      "grad_norm": 1.6538084745407104,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 1.2446,
      "num_input_tokens_seen": 419328,
      "step": 274
    },
    {
      "epoch": 0.04700051273286618,
      "grad_norm": 1.0621060132980347,
      "learning_rate": 4.54788011072248e-05,
      "loss": 1.6363,
      "num_input_tokens_seen": 421376,
      "step": 275
    },
    {
      "epoch": 0.04717142368825842,
      "grad_norm": 1.0977247953414917,
      "learning_rate": 4.542862245837821e-05,
      "loss": 1.3105,
      "num_input_tokens_seen": 422400,
      "step": 276
    },
    {
      "epoch": 0.04734233464365066,
      "grad_norm": 0.8955913186073303,
      "learning_rate": 4.537819489321386e-05,
      "loss": 1.302,
      "num_input_tokens_seen": 424832,
      "step": 277
    },
    {
      "epoch": 0.0475132455990429,
      "grad_norm": 1.1718910932540894,
      "learning_rate": 4.532751902617569e-05,
      "loss": 1.2986,
      "num_input_tokens_seen": 425984,
      "step": 278
    },
    {
      "epoch": 0.04768415655443514,
      "grad_norm": 1.4848597049713135,
      "learning_rate": 4.527659547473317e-05,
      "loss": 1.473,
      "num_input_tokens_seen": 427392,
      "step": 279
    },
    {
      "epoch": 0.04785506750982738,
      "grad_norm": 0.9632925987243652,
      "learning_rate": 4.522542485937369e-05,
      "loss": 1.1539,
      "num_input_tokens_seen": 432128,
      "step": 280
    },
    {
      "epoch": 0.048025978465219624,
      "grad_norm": 1.0872085094451904,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 1.3791,
      "num_input_tokens_seen": 433920,
      "step": 281
    },
    {
      "epoch": 0.048196889420611864,
      "grad_norm": 1.1819838285446167,
      "learning_rate": 4.512234493389785e-05,
      "loss": 1.0045,
      "num_input_tokens_seen": 435072,
      "step": 282
    },
    {
      "epoch": 0.048367800376004105,
      "grad_norm": 1.098101258277893,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 1.1817,
      "num_input_tokens_seen": 436736,
      "step": 283
    },
    {
      "epoch": 0.04853871133139634,
      "grad_norm": 1.123747706413269,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 1.3346,
      "num_input_tokens_seen": 438016,
      "step": 284
    },
    {
      "epoch": 0.04870962228678858,
      "grad_norm": 1.2023637294769287,
      "learning_rate": 4.496588775118232e-05,
      "loss": 1.4485,
      "num_input_tokens_seen": 439296,
      "step": 285
    },
    {
      "epoch": 0.04888053324218082,
      "grad_norm": 1.1430031061172485,
      "learning_rate": 4.491324795060491e-05,
      "loss": 1.1529,
      "num_input_tokens_seen": 440704,
      "step": 286
    },
    {
      "epoch": 0.04905144419757306,
      "grad_norm": 1.2747255563735962,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.8688,
      "num_input_tokens_seen": 441856,
      "step": 287
    },
    {
      "epoch": 0.0492223551529653,
      "grad_norm": 1.1850054264068604,
      "learning_rate": 4.480724108387977e-05,
      "loss": 1.7551,
      "num_input_tokens_seen": 443136,
      "step": 288
    },
    {
      "epoch": 0.04939326610835754,
      "grad_norm": 1.3216322660446167,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 1.4308,
      "num_input_tokens_seen": 445312,
      "step": 289
    },
    {
      "epoch": 0.049564177063749784,
      "grad_norm": 1.6860663890838623,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 1.146,
      "num_input_tokens_seen": 446592,
      "step": 290
    },
    {
      "epoch": 0.049735088019142025,
      "grad_norm": 0.9536709189414978,
      "learning_rate": 4.464642232938505e-05,
      "loss": 1.1657,
      "num_input_tokens_seen": 448128,
      "step": 291
    },
    {
      "epoch": 0.049905998974534266,
      "grad_norm": 1.2624051570892334,
      "learning_rate": 4.4592336433146e-05,
      "loss": 1.0698,
      "num_input_tokens_seen": 450432,
      "step": 292
    },
    {
      "epoch": 0.05007690992992651,
      "grad_norm": 1.2469404935836792,
      "learning_rate": 4.453801181047047e-05,
      "loss": 1.6064,
      "num_input_tokens_seen": 451584,
      "step": 293
    },
    {
      "epoch": 0.05024782088531875,
      "grad_norm": 1.3458151817321777,
      "learning_rate": 4.448344912328686e-05,
      "loss": 1.5736,
      "num_input_tokens_seen": 453120,
      "step": 294
    },
    {
      "epoch": 0.05041873184071099,
      "grad_norm": 0.9786531925201416,
      "learning_rate": 4.442864903642428e-05,
      "loss": 1.4863,
      "num_input_tokens_seen": 455552,
      "step": 295
    },
    {
      "epoch": 0.05058964279610323,
      "grad_norm": 1.1595826148986816,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 1.2131,
      "num_input_tokens_seen": 456704,
      "step": 296
    },
    {
      "epoch": 0.05076055375149547,
      "grad_norm": 1.3944300413131714,
      "learning_rate": 4.431833933743378e-05,
      "loss": 1.0137,
      "num_input_tokens_seen": 457728,
      "step": 297
    },
    {
      "epoch": 0.05093146470688771,
      "grad_norm": 0.9568564891815186,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.9835,
      "num_input_tokens_seen": 460416,
      "step": 298
    },
    {
      "epoch": 0.05110237566227995,
      "grad_norm": 0.8809798955917358,
      "learning_rate": 4.420708808983809e-05,
      "loss": 1.3048,
      "num_input_tokens_seen": 463616,
      "step": 299
    },
    {
      "epoch": 0.05127328661767219,
      "grad_norm": 1.041897177696228,
      "learning_rate": 4.415111107797445e-05,
      "loss": 1.5192,
      "num_input_tokens_seen": 465664,
      "step": 300
    },
    {
      "epoch": 0.05144419757306443,
      "grad_norm": 1.1186052560806274,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.9045,
      "num_input_tokens_seen": 467328,
      "step": 301
    },
    {
      "epoch": 0.051615108528456674,
      "grad_norm": 1.5201902389526367,
      "learning_rate": 4.403845768841842e-05,
      "loss": 1.964,
      "num_input_tokens_seen": 468864,
      "step": 302
    },
    {
      "epoch": 0.051786019483848915,
      "grad_norm": 0.9211037158966064,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 1.4078,
      "num_input_tokens_seen": 470144,
      "step": 303
    },
    {
      "epoch": 0.051956930439241156,
      "grad_norm": 0.9092609286308289,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 1.3189,
      "num_input_tokens_seen": 471424,
      "step": 304
    },
    {
      "epoch": 0.0521278413946334,
      "grad_norm": 2.260321617126465,
      "learning_rate": 4.386773950556931e-05,
      "loss": 1.1412,
      "num_input_tokens_seen": 472320,
      "step": 305
    },
    {
      "epoch": 0.05229875235002564,
      "grad_norm": 0.8221091628074646,
      "learning_rate": 4.381037272239311e-05,
      "loss": 1.2617,
      "num_input_tokens_seen": 473728,
      "step": 306
    },
    {
      "epoch": 0.05246966330541788,
      "grad_norm": 0.9732173085212708,
      "learning_rate": 4.375277674076149e-05,
      "loss": 1.2943,
      "num_input_tokens_seen": 474880,
      "step": 307
    },
    {
      "epoch": 0.05264057426081012,
      "grad_norm": 0.958909809589386,
      "learning_rate": 4.36949522624633e-05,
      "loss": 1.329,
      "num_input_tokens_seen": 476288,
      "step": 308
    },
    {
      "epoch": 0.05281148521620236,
      "grad_norm": 1.5420953035354614,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.7652,
      "num_input_tokens_seen": 477568,
      "step": 309
    },
    {
      "epoch": 0.0529823961715946,
      "grad_norm": 0.7725672721862793,
      "learning_rate": 4.357862063693486e-05,
      "loss": 1.1517,
      "num_input_tokens_seen": 480512,
      "step": 310
    },
    {
      "epoch": 0.05315330712698684,
      "grad_norm": 1.98581063747406,
      "learning_rate": 4.352011490716875e-05,
      "loss": 1.435,
      "num_input_tokens_seen": 481536,
      "step": 311
    },
    {
      "epoch": 0.05332421808237908,
      "grad_norm": 0.8134300112724304,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 1.2799,
      "num_input_tokens_seen": 483712,
      "step": 312
    },
    {
      "epoch": 0.05349512903777132,
      "grad_norm": 0.8789598345756531,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 1.4928,
      "num_input_tokens_seen": 486144,
      "step": 313
    },
    {
      "epoch": 0.053666039993163564,
      "grad_norm": 1.0536342859268188,
      "learning_rate": 4.334324661257191e-05,
      "loss": 1.4142,
      "num_input_tokens_seen": 487808,
      "step": 314
    },
    {
      "epoch": 0.053836950948555805,
      "grad_norm": 1.1584173440933228,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 1.229,
      "num_input_tokens_seen": 489088,
      "step": 315
    },
    {
      "epoch": 0.054007861903948046,
      "grad_norm": 1.0175397396087646,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 0.9533,
      "num_input_tokens_seen": 490624,
      "step": 316
    },
    {
      "epoch": 0.054178772859340286,
      "grad_norm": 1.1944398880004883,
      "learning_rate": 4.31643667742744e-05,
      "loss": 1.3465,
      "num_input_tokens_seen": 492160,
      "step": 317
    },
    {
      "epoch": 0.05434968381473253,
      "grad_norm": 1.8372286558151245,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.9923,
      "num_input_tokens_seen": 493440,
      "step": 318
    },
    {
      "epoch": 0.05452059477012477,
      "grad_norm": 1.1609925031661987,
      "learning_rate": 4.304400570245906e-05,
      "loss": 1.4225,
      "num_input_tokens_seen": 494976,
      "step": 319
    },
    {
      "epoch": 0.05469150572551701,
      "grad_norm": 1.2264988422393799,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 1.3134,
      "num_input_tokens_seen": 496384,
      "step": 320
    },
    {
      "epoch": 0.05486241668090924,
      "grad_norm": 1.3681330680847168,
      "learning_rate": 4.292276519126207e-05,
      "loss": 1.0198,
      "num_input_tokens_seen": 497536,
      "step": 321
    },
    {
      "epoch": 0.055033327636301484,
      "grad_norm": 1.6835094690322876,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 1.2788,
      "num_input_tokens_seen": 498816,
      "step": 322
    },
    {
      "epoch": 0.055204238591693724,
      "grad_norm": 1.096444845199585,
      "learning_rate": 4.280065114977492e-05,
      "loss": 1.6014,
      "num_input_tokens_seen": 501504,
      "step": 323
    },
    {
      "epoch": 0.055375149547085965,
      "grad_norm": 1.1579769849777222,
      "learning_rate": 4.273926841341302e-05,
      "loss": 1.5461,
      "num_input_tokens_seen": 503424,
      "step": 324
    },
    {
      "epoch": 0.055546060502478206,
      "grad_norm": 1.043822169303894,
      "learning_rate": 4.267766952966369e-05,
      "loss": 1.218,
      "num_input_tokens_seen": 504832,
      "step": 325
    },
    {
      "epoch": 0.05571697145787045,
      "grad_norm": 0.9884098768234253,
      "learning_rate": 4.261585524908987e-05,
      "loss": 1.1391,
      "num_input_tokens_seen": 506496,
      "step": 326
    },
    {
      "epoch": 0.05588788241326269,
      "grad_norm": 0.9877035617828369,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 1.2125,
      "num_input_tokens_seen": 508032,
      "step": 327
    },
    {
      "epoch": 0.05605879336865493,
      "grad_norm": 1.1853971481323242,
      "learning_rate": 4.249158351283414e-05,
      "loss": 1.0358,
      "num_input_tokens_seen": 509568,
      "step": 328
    },
    {
      "epoch": 0.05622970432404717,
      "grad_norm": 1.1017589569091797,
      "learning_rate": 4.242912757136412e-05,
      "loss": 1.3761,
      "num_input_tokens_seen": 510848,
      "step": 329
    },
    {
      "epoch": 0.05640061527943941,
      "grad_norm": 0.9285961985588074,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 1.3962,
      "num_input_tokens_seen": 513664,
      "step": 330
    },
    {
      "epoch": 0.05657152623483165,
      "grad_norm": 0.911115288734436,
      "learning_rate": 4.230357934676017e-05,
      "loss": 1.236,
      "num_input_tokens_seen": 514944,
      "step": 331
    },
    {
      "epoch": 0.05674243719022389,
      "grad_norm": 1.8159180879592896,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.8304,
      "num_input_tokens_seen": 515968,
      "step": 332
    },
    {
      "epoch": 0.05691334814561613,
      "grad_norm": 0.6588620543479919,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.4808,
      "num_input_tokens_seen": 520320,
      "step": 333
    },
    {
      "epoch": 0.057084259101008374,
      "grad_norm": 0.906249463558197,
      "learning_rate": 4.211367764821722e-05,
      "loss": 1.4054,
      "num_input_tokens_seen": 522880,
      "step": 334
    },
    {
      "epoch": 0.057255170056400614,
      "grad_norm": 1.4077109098434448,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 1.3668,
      "num_input_tokens_seen": 524160,
      "step": 335
    },
    {
      "epoch": 0.057426081011792855,
      "grad_norm": 1.1103355884552002,
      "learning_rate": 4.198603260653792e-05,
      "loss": 1.4751,
      "num_input_tokens_seen": 526336,
      "step": 336
    },
    {
      "epoch": 0.057596991967185096,
      "grad_norm": 0.9403926730155945,
      "learning_rate": 4.192189924206652e-05,
      "loss": 1.5703,
      "num_input_tokens_seen": 528384,
      "step": 337
    },
    {
      "epoch": 0.05776790292257734,
      "grad_norm": 1.5141994953155518,
      "learning_rate": 4.185755968959308e-05,
      "loss": 1.2787,
      "num_input_tokens_seen": 529664,
      "step": 338
    },
    {
      "epoch": 0.05793881387796958,
      "grad_norm": 1.2510337829589844,
      "learning_rate": 4.179301473307476e-05,
      "loss": 1.3431,
      "num_input_tokens_seen": 530944,
      "step": 339
    },
    {
      "epoch": 0.05810972483336182,
      "grad_norm": 1.0812115669250488,
      "learning_rate": 4.172826515897146e-05,
      "loss": 1.006,
      "num_input_tokens_seen": 532352,
      "step": 340
    },
    {
      "epoch": 0.05828063578875406,
      "grad_norm": 0.9966895580291748,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.5051,
      "num_input_tokens_seen": 534144,
      "step": 341
    },
    {
      "epoch": 0.0584515467441463,
      "grad_norm": 0.7467116713523865,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 1.0099,
      "num_input_tokens_seen": 537472,
      "step": 342
    },
    {
      "epoch": 0.05862245769953854,
      "grad_norm": 1.0510014295578003,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 1.1253,
      "num_input_tokens_seen": 539776,
      "step": 343
    },
    {
      "epoch": 0.05879336865493078,
      "grad_norm": 1.6306171417236328,
      "learning_rate": 4.146723650296701e-05,
      "loss": 1.598,
      "num_input_tokens_seen": 541184,
      "step": 344
    },
    {
      "epoch": 0.05896427961032302,
      "grad_norm": 1.252044916152954,
      "learning_rate": 4.140147572476268e-05,
      "loss": 1.3318,
      "num_input_tokens_seen": 542336,
      "step": 345
    },
    {
      "epoch": 0.059135190565715263,
      "grad_norm": 1.1614630222320557,
      "learning_rate": 4.133551509975264e-05,
      "loss": 1.2135,
      "num_input_tokens_seen": 543488,
      "step": 346
    },
    {
      "epoch": 0.059306101521107504,
      "grad_norm": 1.3566601276397705,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 1.3263,
      "num_input_tokens_seen": 544768,
      "step": 347
    },
    {
      "epoch": 0.059477012476499745,
      "grad_norm": 1.2357780933380127,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 1.458,
      "num_input_tokens_seen": 546688,
      "step": 348
    },
    {
      "epoch": 0.059647923431891986,
      "grad_norm": 1.277052402496338,
      "learning_rate": 4.113644219309877e-05,
      "loss": 1.4054,
      "num_input_tokens_seen": 547840,
      "step": 349
    },
    {
      "epoch": 0.05981883438728423,
      "grad_norm": 1.4927446842193604,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 1.5225,
      "num_input_tokens_seen": 548992,
      "step": 350
    },
    {
      "epoch": 0.05998974534267647,
      "grad_norm": 1.0024715662002563,
      "learning_rate": 4.100274248712389e-05,
      "loss": 1.1117,
      "num_input_tokens_seen": 551168,
      "step": 351
    },
    {
      "epoch": 0.06016065629806871,
      "grad_norm": 1.1886438131332397,
      "learning_rate": 4.093559974371725e-05,
      "loss": 1.2012,
      "num_input_tokens_seen": 552576,
      "step": 352
    },
    {
      "epoch": 0.06033156725346095,
      "grad_norm": 1.0095033645629883,
      "learning_rate": 4.086826283005669e-05,
      "loss": 1.2182,
      "num_input_tokens_seen": 553728,
      "step": 353
    },
    {
      "epoch": 0.06050247820885319,
      "grad_norm": 1.2736419439315796,
      "learning_rate": 4.080073256662127e-05,
      "loss": 1.6581,
      "num_input_tokens_seen": 556544,
      "step": 354
    },
    {
      "epoch": 0.06067338916424543,
      "grad_norm": 1.265134572982788,
      "learning_rate": 4.073300977624594e-05,
      "loss": 1.5932,
      "num_input_tokens_seen": 557824,
      "step": 355
    },
    {
      "epoch": 0.06084430011963767,
      "grad_norm": 1.0852231979370117,
      "learning_rate": 4.066509528411152e-05,
      "loss": 1.227,
      "num_input_tokens_seen": 559488,
      "step": 356
    },
    {
      "epoch": 0.06101521107502991,
      "grad_norm": 1.0866012573242188,
      "learning_rate": 4.059698991773466e-05,
      "loss": 1.3366,
      "num_input_tokens_seen": 560896,
      "step": 357
    },
    {
      "epoch": 0.06118612203042215,
      "grad_norm": 1.3021012544631958,
      "learning_rate": 4.052869450695776e-05,
      "loss": 1.0049,
      "num_input_tokens_seen": 561920,
      "step": 358
    },
    {
      "epoch": 0.06135703298581439,
      "grad_norm": 1.345383882522583,
      "learning_rate": 4.046020988393885e-05,
      "loss": 1.5435,
      "num_input_tokens_seen": 563072,
      "step": 359
    },
    {
      "epoch": 0.06152794394120663,
      "grad_norm": 1.4994995594024658,
      "learning_rate": 4.039153688314145e-05,
      "loss": 1.0302,
      "num_input_tokens_seen": 564480,
      "step": 360
    },
    {
      "epoch": 0.06169885489659887,
      "grad_norm": 1.1347566843032837,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 1.275,
      "num_input_tokens_seen": 565760,
      "step": 361
    },
    {
      "epoch": 0.06186976585199111,
      "grad_norm": 1.6658309698104858,
      "learning_rate": 4.02536290975317e-05,
      "loss": 1.2497,
      "num_input_tokens_seen": 566784,
      "step": 362
    },
    {
      "epoch": 0.06204067680738335,
      "grad_norm": 0.8747075200080872,
      "learning_rate": 4.018439599308217e-05,
      "loss": 1.6226,
      "num_input_tokens_seen": 569088,
      "step": 363
    },
    {
      "epoch": 0.06221158776277559,
      "grad_norm": 0.9225448369979858,
      "learning_rate": 4.011497787155938e-05,
      "loss": 1.271,
      "num_input_tokens_seen": 572288,
      "step": 364
    },
    {
      "epoch": 0.06238249871816783,
      "grad_norm": 1.4061386585235596,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 1.483,
      "num_input_tokens_seen": 573824,
      "step": 365
    },
    {
      "epoch": 0.06255340967356007,
      "grad_norm": 1.3178304433822632,
      "learning_rate": 3.997558996288965e-05,
      "loss": 1.2564,
      "num_input_tokens_seen": 574976,
      "step": 366
    },
    {
      "epoch": 0.06272432062895232,
      "grad_norm": 1.0378998517990112,
      "learning_rate": 3.99056218741404e-05,
      "loss": 1.0685,
      "num_input_tokens_seen": 576512,
      "step": 367
    },
    {
      "epoch": 0.06289523158434455,
      "grad_norm": 0.907175600528717,
      "learning_rate": 3.983547216509254e-05,
      "loss": 1.5231,
      "num_input_tokens_seen": 578432,
      "step": 368
    },
    {
      "epoch": 0.0630661425397368,
      "grad_norm": 0.9977318048477173,
      "learning_rate": 3.976514169049814e-05,
      "loss": 1.451,
      "num_input_tokens_seen": 579968,
      "step": 369
    },
    {
      "epoch": 0.06323705349512904,
      "grad_norm": 1.0102801322937012,
      "learning_rate": 3.969463130731183e-05,
      "loss": 1.5435,
      "num_input_tokens_seen": 582400,
      "step": 370
    },
    {
      "epoch": 0.06340796445052128,
      "grad_norm": 1.4458380937576294,
      "learning_rate": 3.962394187468039e-05,
      "loss": 1.8805,
      "num_input_tokens_seen": 583936,
      "step": 371
    },
    {
      "epoch": 0.06357887540591352,
      "grad_norm": 1.32124924659729,
      "learning_rate": 3.955307425393224e-05,
      "loss": 1.6393,
      "num_input_tokens_seen": 585088,
      "step": 372
    },
    {
      "epoch": 0.06374978636130577,
      "grad_norm": 1.281894326210022,
      "learning_rate": 3.948202930856697e-05,
      "loss": 1.8838,
      "num_input_tokens_seen": 587008,
      "step": 373
    },
    {
      "epoch": 0.063920697316698,
      "grad_norm": 1.9483245611190796,
      "learning_rate": 3.941080790424484e-05,
      "loss": 1.1421,
      "num_input_tokens_seen": 587904,
      "step": 374
    },
    {
      "epoch": 0.06409160827209025,
      "grad_norm": 1.2797924280166626,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.6723,
      "num_input_tokens_seen": 589312,
      "step": 375
    },
    {
      "epoch": 0.06426251922748248,
      "grad_norm": 1.311789870262146,
      "learning_rate": 3.92678391921108e-05,
      "loss": 1.3473,
      "num_input_tokens_seen": 590464,
      "step": 376
    },
    {
      "epoch": 0.06443343018287473,
      "grad_norm": 1.12346351146698,
      "learning_rate": 3.919609362632753e-05,
      "loss": 1.3328,
      "num_input_tokens_seen": 592128,
      "step": 377
    },
    {
      "epoch": 0.06460434113826696,
      "grad_norm": 0.9740382432937622,
      "learning_rate": 3.912417508562345e-05,
      "loss": 1.188,
      "num_input_tokens_seen": 593664,
      "step": 378
    },
    {
      "epoch": 0.0647752520936592,
      "grad_norm": 1.1389797925949097,
      "learning_rate": 3.905208444630327e-05,
      "loss": 1.013,
      "num_input_tokens_seen": 594816,
      "step": 379
    },
    {
      "epoch": 0.06494616304905144,
      "grad_norm": 1.1053717136383057,
      "learning_rate": 3.897982258676867e-05,
      "loss": 1.0259,
      "num_input_tokens_seen": 595968,
      "step": 380
    },
    {
      "epoch": 0.06511707400444368,
      "grad_norm": 1.6476004123687744,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 1.3985,
      "num_input_tokens_seen": 597120,
      "step": 381
    },
    {
      "epoch": 0.06528798495983593,
      "grad_norm": 1.4090474843978882,
      "learning_rate": 3.883478873108361e-05,
      "loss": 1.2735,
      "num_input_tokens_seen": 599424,
      "step": 382
    },
    {
      "epoch": 0.06545889591522816,
      "grad_norm": 1.3761489391326904,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 1.161,
      "num_input_tokens_seen": 600576,
      "step": 383
    },
    {
      "epoch": 0.06562980687062041,
      "grad_norm": 1.4676347970962524,
      "learning_rate": 3.868908058731376e-05,
      "loss": 1.3929,
      "num_input_tokens_seen": 601984,
      "step": 384
    },
    {
      "epoch": 0.06580071782601264,
      "grad_norm": 1.1292017698287964,
      "learning_rate": 3.861597587537568e-05,
      "loss": 1.3194,
      "num_input_tokens_seen": 603264,
      "step": 385
    },
    {
      "epoch": 0.06597162878140489,
      "grad_norm": 1.2004152536392212,
      "learning_rate": 3.85427052570685e-05,
      "loss": 1.3254,
      "num_input_tokens_seen": 605184,
      "step": 386
    },
    {
      "epoch": 0.06614253973679712,
      "grad_norm": 1.339856505393982,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 1.3192,
      "num_input_tokens_seen": 606848,
      "step": 387
    },
    {
      "epoch": 0.06631345069218937,
      "grad_norm": 1.3101437091827393,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 1.7516,
      "num_input_tokens_seen": 608768,
      "step": 388
    },
    {
      "epoch": 0.0664843616475816,
      "grad_norm": 1.4069277048110962,
      "learning_rate": 3.832190690176825e-05,
      "loss": 1.2039,
      "num_input_tokens_seen": 610176,
      "step": 389
    },
    {
      "epoch": 0.06665527260297385,
      "grad_norm": 0.9916775822639465,
      "learning_rate": 3.824798160583012e-05,
      "loss": 1.3447,
      "num_input_tokens_seen": 611840,
      "step": 390
    },
    {
      "epoch": 0.06682618355836609,
      "grad_norm": 1.380793809890747,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 1.1554,
      "num_input_tokens_seen": 613248,
      "step": 391
    },
    {
      "epoch": 0.06699709451375833,
      "grad_norm": 1.0895178318023682,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 1.0862,
      "num_input_tokens_seen": 614912,
      "step": 392
    },
    {
      "epoch": 0.06716800546915057,
      "grad_norm": 1.729443907737732,
      "learning_rate": 3.802524079601442e-05,
      "loss": 1.213,
      "num_input_tokens_seen": 615936,
      "step": 393
    },
    {
      "epoch": 0.06733891642454282,
      "grad_norm": 1.063840627670288,
      "learning_rate": 3.795067523432826e-05,
      "loss": 1.1134,
      "num_input_tokens_seen": 617344,
      "step": 394
    },
    {
      "epoch": 0.06750982737993505,
      "grad_norm": 1.2330548763275146,
      "learning_rate": 3.787595187275136e-05,
      "loss": 1.4592,
      "num_input_tokens_seen": 619008,
      "step": 395
    },
    {
      "epoch": 0.0676807383353273,
      "grad_norm": 1.0778703689575195,
      "learning_rate": 3.780107162176429e-05,
      "loss": 1.3695,
      "num_input_tokens_seen": 620544,
      "step": 396
    },
    {
      "epoch": 0.06785164929071953,
      "grad_norm": 0.9391964077949524,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 1.3663,
      "num_input_tokens_seen": 622464,
      "step": 397
    },
    {
      "epoch": 0.06802256024611178,
      "grad_norm": 1.6082991361618042,
      "learning_rate": 3.765084410302909e-05,
      "loss": 1.3733,
      "num_input_tokens_seen": 623872,
      "step": 398
    },
    {
      "epoch": 0.06819347120150401,
      "grad_norm": 1.0598469972610474,
      "learning_rate": 3.757549866575588e-05,
      "loss": 1.5471,
      "num_input_tokens_seen": 625792,
      "step": 399
    },
    {
      "epoch": 0.06836438215689626,
      "grad_norm": 0.9738578796386719,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.8509,
      "num_input_tokens_seen": 627456,
      "step": 400
    },
    {
      "epoch": 0.0685352931122885,
      "grad_norm": 1.0584675073623657,
      "learning_rate": 3.742434902568889e-05,
      "loss": 1.2671,
      "num_input_tokens_seen": 628736,
      "step": 401
    },
    {
      "epoch": 0.06870620406768074,
      "grad_norm": 1.266519546508789,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 1.3093,
      "num_input_tokens_seen": 630144,
      "step": 402
    },
    {
      "epoch": 0.06887711502307298,
      "grad_norm": 1.0605206489562988,
      "learning_rate": 3.727259384037852e-05,
      "loss": 1.3684,
      "num_input_tokens_seen": 631936,
      "step": 403
    },
    {
      "epoch": 0.06904802597846522,
      "grad_norm": 1.0508137941360474,
      "learning_rate": 3.719649147846832e-05,
      "loss": 1.2316,
      "num_input_tokens_seen": 633344,
      "step": 404
    },
    {
      "epoch": 0.06921893693385746,
      "grad_norm": 1.0562338829040527,
      "learning_rate": 3.712024050615843e-05,
      "loss": 1.769,
      "num_input_tokens_seen": 636032,
      "step": 405
    },
    {
      "epoch": 0.0693898478892497,
      "grad_norm": 1.9485269784927368,
      "learning_rate": 3.704384185254288e-05,
      "loss": 1.4919,
      "num_input_tokens_seen": 637568,
      "step": 406
    },
    {
      "epoch": 0.06956075884464194,
      "grad_norm": 0.6849764585494995,
      "learning_rate": 3.696729644851518e-05,
      "loss": 1.0212,
      "num_input_tokens_seen": 642048,
      "step": 407
    },
    {
      "epoch": 0.06973166980003419,
      "grad_norm": 1.4045311212539673,
      "learning_rate": 3.689060522675689e-05,
      "loss": 1.5233,
      "num_input_tokens_seen": 643072,
      "step": 408
    },
    {
      "epoch": 0.06990258075542642,
      "grad_norm": 0.9665870070457458,
      "learning_rate": 3.681376912172636e-05,
      "loss": 1.3107,
      "num_input_tokens_seen": 645248,
      "step": 409
    },
    {
      "epoch": 0.07007349171081867,
      "grad_norm": 1.520004153251648,
      "learning_rate": 3.673678906964727e-05,
      "loss": 1.581,
      "num_input_tokens_seen": 646528,
      "step": 410
    },
    {
      "epoch": 0.0702444026662109,
      "grad_norm": 0.9982793927192688,
      "learning_rate": 3.665966600849728e-05,
      "loss": 1.3413,
      "num_input_tokens_seen": 648448,
      "step": 411
    },
    {
      "epoch": 0.07041531362160315,
      "grad_norm": 1.0779573917388916,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 1.2066,
      "num_input_tokens_seen": 649856,
      "step": 412
    },
    {
      "epoch": 0.07058622457699538,
      "grad_norm": 1.3743102550506592,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 1.2058,
      "num_input_tokens_seen": 651392,
      "step": 413
    },
    {
      "epoch": 0.07075713553238763,
      "grad_norm": 1.9362211227416992,
      "learning_rate": 3.642744817646736e-05,
      "loss": 1.0805,
      "num_input_tokens_seen": 652416,
      "step": 414
    },
    {
      "epoch": 0.07092804648777987,
      "grad_norm": 0.948117196559906,
      "learning_rate": 3.634976249348867e-05,
      "loss": 1.6516,
      "num_input_tokens_seen": 653952,
      "step": 415
    },
    {
      "epoch": 0.0710989574431721,
      "grad_norm": 2.6435322761535645,
      "learning_rate": 3.627193851723577e-05,
      "loss": 1.1809,
      "num_input_tokens_seen": 654976,
      "step": 416
    },
    {
      "epoch": 0.07126986839856435,
      "grad_norm": 1.0608670711517334,
      "learning_rate": 3.619397719596924e-05,
      "loss": 1.3293,
      "num_input_tokens_seen": 656384,
      "step": 417
    },
    {
      "epoch": 0.07144077935395658,
      "grad_norm": 1.0649120807647705,
      "learning_rate": 3.611587947962319e-05,
      "loss": 1.3112,
      "num_input_tokens_seen": 657792,
      "step": 418
    },
    {
      "epoch": 0.07161169030934883,
      "grad_norm": 0.9533461928367615,
      "learning_rate": 3.603764631979363e-05,
      "loss": 1.3536,
      "num_input_tokens_seen": 661760,
      "step": 419
    },
    {
      "epoch": 0.07178260126474106,
      "grad_norm": 1.3469878435134888,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.939,
      "num_input_tokens_seen": 663424,
      "step": 420
    },
    {
      "epoch": 0.07195351222013331,
      "grad_norm": 1.7069233655929565,
      "learning_rate": 3.588077748430819e-05,
      "loss": 1.6707,
      "num_input_tokens_seen": 664960,
      "step": 421
    },
    {
      "epoch": 0.07212442317552555,
      "grad_norm": 1.2416623830795288,
      "learning_rate": 3.580214372004956e-05,
      "loss": 1.2033,
      "num_input_tokens_seen": 666112,
      "step": 422
    },
    {
      "epoch": 0.0722953341309178,
      "grad_norm": 1.0065237283706665,
      "learning_rate": 3.572337833507865e-05,
      "loss": 1.5908,
      "num_input_tokens_seen": 667776,
      "step": 423
    },
    {
      "epoch": 0.07246624508631003,
      "grad_norm": 0.9658154249191284,
      "learning_rate": 3.564448228912682e-05,
      "loss": 1.3434,
      "num_input_tokens_seen": 669184,
      "step": 424
    },
    {
      "epoch": 0.07263715604170227,
      "grad_norm": 1.0530915260314941,
      "learning_rate": 3.556545654351749e-05,
      "loss": 1.3727,
      "num_input_tokens_seen": 670720,
      "step": 425
    },
    {
      "epoch": 0.07280806699709451,
      "grad_norm": 1.3460097312927246,
      "learning_rate": 3.548630206115443e-05,
      "loss": 1.6328,
      "num_input_tokens_seen": 673664,
      "step": 426
    },
    {
      "epoch": 0.07297897795248676,
      "grad_norm": 1.5682042837142944,
      "learning_rate": 3.540701980651003e-05,
      "loss": 1.367,
      "num_input_tokens_seen": 674816,
      "step": 427
    },
    {
      "epoch": 0.07314988890787899,
      "grad_norm": 0.9478664398193359,
      "learning_rate": 3.532761074561355e-05,
      "loss": 1.294,
      "num_input_tokens_seen": 677632,
      "step": 428
    },
    {
      "epoch": 0.07332079986327124,
      "grad_norm": 1.0294626951217651,
      "learning_rate": 3.524807584603932e-05,
      "loss": 1.3118,
      "num_input_tokens_seen": 679680,
      "step": 429
    },
    {
      "epoch": 0.07349171081866347,
      "grad_norm": 0.9960530996322632,
      "learning_rate": 3.516841607689501e-05,
      "loss": 1.2193,
      "num_input_tokens_seen": 680960,
      "step": 430
    },
    {
      "epoch": 0.07366262177405572,
      "grad_norm": 1.163150429725647,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 1.1363,
      "num_input_tokens_seen": 682112,
      "step": 431
    },
    {
      "epoch": 0.07383353272944795,
      "grad_norm": 1.2167682647705078,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.9382,
      "num_input_tokens_seen": 683648,
      "step": 432
    },
    {
      "epoch": 0.0740044436848402,
      "grad_norm": 1.0608776807785034,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 1.2149,
      "num_input_tokens_seen": 685568,
      "step": 433
    },
    {
      "epoch": 0.07417535464023244,
      "grad_norm": 1.257960557937622,
      "learning_rate": 3.484854773977378e-05,
      "loss": 1.2424,
      "num_input_tokens_seen": 686720,
      "step": 434
    },
    {
      "epoch": 0.07434626559562468,
      "grad_norm": 1.3928608894348145,
      "learning_rate": 3.476827821223184e-05,
      "loss": 1.2515,
      "num_input_tokens_seen": 688768,
      "step": 435
    },
    {
      "epoch": 0.07451717655101692,
      "grad_norm": 0.9963674545288086,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 1.2496,
      "num_input_tokens_seen": 690560,
      "step": 436
    },
    {
      "epoch": 0.07468808750640916,
      "grad_norm": 0.9634340405464172,
      "learning_rate": 3.460738306649509e-05,
      "loss": 1.4991,
      "num_input_tokens_seen": 693888,
      "step": 437
    },
    {
      "epoch": 0.0748589984618014,
      "grad_norm": 1.198439598083496,
      "learning_rate": 3.452675940875686e-05,
      "loss": 1.2592,
      "num_input_tokens_seen": 695296,
      "step": 438
    },
    {
      "epoch": 0.07502990941719365,
      "grad_norm": 1.0903185606002808,
      "learning_rate": 3.444601967046168e-05,
      "loss": 1.3106,
      "num_input_tokens_seen": 697216,
      "step": 439
    },
    {
      "epoch": 0.07520082037258588,
      "grad_norm": 1.1880677938461304,
      "learning_rate": 3.436516483539781e-05,
      "loss": 1.4823,
      "num_input_tokens_seen": 699008,
      "step": 440
    },
    {
      "epoch": 0.07537173132797813,
      "grad_norm": 0.8801599740982056,
      "learning_rate": 3.428419588875588e-05,
      "loss": 1.1051,
      "num_input_tokens_seen": 701184,
      "step": 441
    },
    {
      "epoch": 0.07554264228337036,
      "grad_norm": 1.275736927986145,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 1.1707,
      "num_input_tokens_seen": 702208,
      "step": 442
    },
    {
      "epoch": 0.07571355323876261,
      "grad_norm": 1.1048011779785156,
      "learning_rate": 3.412191960844049e-05,
      "loss": 1.2814,
      "num_input_tokens_seen": 703872,
      "step": 443
    },
    {
      "epoch": 0.07588446419415484,
      "grad_norm": 0.9503735303878784,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 1.1754,
      "num_input_tokens_seen": 705024,
      "step": 444
    },
    {
      "epoch": 0.07605537514954709,
      "grad_norm": 1.4254251718521118,
      "learning_rate": 3.39591987386325e-05,
      "loss": 1.1917,
      "num_input_tokens_seen": 706176,
      "step": 445
    },
    {
      "epoch": 0.07622628610493933,
      "grad_norm": 2.681914806365967,
      "learning_rate": 3.387767406020343e-05,
      "loss": 1.169,
      "num_input_tokens_seen": 707072,
      "step": 446
    },
    {
      "epoch": 0.07639719706033157,
      "grad_norm": 1.4607446193695068,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 1.4135,
      "num_input_tokens_seen": 708480,
      "step": 447
    },
    {
      "epoch": 0.07656810801572381,
      "grad_norm": 1.5916938781738281,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 1.2371,
      "num_input_tokens_seen": 709632,
      "step": 448
    },
    {
      "epoch": 0.07673901897111605,
      "grad_norm": 1.093766689300537,
      "learning_rate": 3.363245497496337e-05,
      "loss": 1.203,
      "num_input_tokens_seen": 711680,
      "step": 449
    },
    {
      "epoch": 0.07690992992650829,
      "grad_norm": 1.3670686483383179,
      "learning_rate": 3.355050358314172e-05,
      "loss": 1.3526,
      "num_input_tokens_seen": 712832,
      "step": 450
    },
    {
      "epoch": 0.07708084088190054,
      "grad_norm": 1.1937499046325684,
      "learning_rate": 3.346844800613229e-05,
      "loss": 1.363,
      "num_input_tokens_seen": 714240,
      "step": 451
    },
    {
      "epoch": 0.07725175183729277,
      "grad_norm": 1.0995358228683472,
      "learning_rate": 3.338628924375638e-05,
      "loss": 1.2974,
      "num_input_tokens_seen": 716032,
      "step": 452
    },
    {
      "epoch": 0.077422662792685,
      "grad_norm": 0.9689955711364746,
      "learning_rate": 3.330402829709258e-05,
      "loss": 1.2083,
      "num_input_tokens_seen": 717568,
      "step": 453
    },
    {
      "epoch": 0.07759357374807725,
      "grad_norm": 1.2385294437408447,
      "learning_rate": 3.322166616846458e-05,
      "loss": 1.5265,
      "num_input_tokens_seen": 718720,
      "step": 454
    },
    {
      "epoch": 0.07776448470346949,
      "grad_norm": 1.226272702217102,
      "learning_rate": 3.313920386142892e-05,
      "loss": 1.2803,
      "num_input_tokens_seen": 719872,
      "step": 455
    },
    {
      "epoch": 0.07793539565886173,
      "grad_norm": 1.2677836418151855,
      "learning_rate": 3.305664238076278e-05,
      "loss": 1.7226,
      "num_input_tokens_seen": 721664,
      "step": 456
    },
    {
      "epoch": 0.07810630661425397,
      "grad_norm": 0.7960153818130493,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 1.5285,
      "num_input_tokens_seen": 726144,
      "step": 457
    },
    {
      "epoch": 0.07827721756964622,
      "grad_norm": 1.2333368062973022,
      "learning_rate": 3.289122592367757e-05,
      "loss": 1.3386,
      "num_input_tokens_seen": 727424,
      "step": 458
    },
    {
      "epoch": 0.07844812852503845,
      "grad_norm": 1.38233482837677,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 1.506,
      "num_input_tokens_seen": 728960,
      "step": 459
    },
    {
      "epoch": 0.0786190394804307,
      "grad_norm": 1.2296773195266724,
      "learning_rate": 3.272542485937369e-05,
      "loss": 1.2101,
      "num_input_tokens_seen": 730624,
      "step": 460
    },
    {
      "epoch": 0.07878995043582293,
      "grad_norm": 1.3082292079925537,
      "learning_rate": 3.264238262407764e-05,
      "loss": 1.527,
      "num_input_tokens_seen": 732672,
      "step": 461
    },
    {
      "epoch": 0.07896086139121518,
      "grad_norm": 1.701676607131958,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 1.2207,
      "num_input_tokens_seen": 733952,
      "step": 462
    },
    {
      "epoch": 0.07913177234660741,
      "grad_norm": 0.7465097904205322,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.8625,
      "num_input_tokens_seen": 736512,
      "step": 463
    },
    {
      "epoch": 0.07930268330199966,
      "grad_norm": 1.4886306524276733,
      "learning_rate": 3.239270125110117e-05,
      "loss": 1.331,
      "num_input_tokens_seen": 737920,
      "step": 464
    },
    {
      "epoch": 0.0794735942573919,
      "grad_norm": 1.050298810005188,
      "learning_rate": 3.230929261806842e-05,
      "loss": 1.0319,
      "num_input_tokens_seen": 739968,
      "step": 465
    },
    {
      "epoch": 0.07964450521278414,
      "grad_norm": 1.1778405904769897,
      "learning_rate": 3.222579492361179e-05,
      "loss": 1.3684,
      "num_input_tokens_seen": 741760,
      "step": 466
    },
    {
      "epoch": 0.07981541616817638,
      "grad_norm": 1.212634563446045,
      "learning_rate": 3.214220918512434e-05,
      "loss": 1.0994,
      "num_input_tokens_seen": 743296,
      "step": 467
    },
    {
      "epoch": 0.07998632712356862,
      "grad_norm": 1.1994328498840332,
      "learning_rate": 3.205853642107192e-05,
      "loss": 1.4115,
      "num_input_tokens_seen": 744704,
      "step": 468
    },
    {
      "epoch": 0.08015723807896086,
      "grad_norm": 2.066033124923706,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 1.4001,
      "num_input_tokens_seen": 745856,
      "step": 469
    },
    {
      "epoch": 0.0803281490343531,
      "grad_norm": 1.2001874446868896,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 1.395,
      "num_input_tokens_seen": 747136,
      "step": 470
    },
    {
      "epoch": 0.08049905998974534,
      "grad_norm": 1.4698678255081177,
      "learning_rate": 3.180700617601436e-05,
      "loss": 1.3119,
      "num_input_tokens_seen": 748160,
      "step": 471
    },
    {
      "epoch": 0.08066997094513759,
      "grad_norm": 1.069612979888916,
      "learning_rate": 3.172299551538164e-05,
      "loss": 1.5543,
      "num_input_tokens_seen": 749824,
      "step": 472
    },
    {
      "epoch": 0.08084088190052982,
      "grad_norm": 1.294314980506897,
      "learning_rate": 3.163890293717022e-05,
      "loss": 1.1773,
      "num_input_tokens_seen": 751616,
      "step": 473
    },
    {
      "epoch": 0.08101179285592207,
      "grad_norm": 1.1755603551864624,
      "learning_rate": 3.155472946602162e-05,
      "loss": 1.1462,
      "num_input_tokens_seen": 752896,
      "step": 474
    },
    {
      "epoch": 0.0811827038113143,
      "grad_norm": 1.2339527606964111,
      "learning_rate": 3.147047612756302e-05,
      "loss": 1.3102,
      "num_input_tokens_seen": 754176,
      "step": 475
    },
    {
      "epoch": 0.08135361476670655,
      "grad_norm": 1.0128952264785767,
      "learning_rate": 3.138614394839476e-05,
      "loss": 1.2026,
      "num_input_tokens_seen": 755840,
      "step": 476
    },
    {
      "epoch": 0.08152452572209878,
      "grad_norm": 0.8745014667510986,
      "learning_rate": 3.130173395607785e-05,
      "loss": 1.1038,
      "num_input_tokens_seen": 757632,
      "step": 477
    },
    {
      "epoch": 0.08169543667749103,
      "grad_norm": 1.0477858781814575,
      "learning_rate": 3.121724717912138e-05,
      "loss": 1.4345,
      "num_input_tokens_seen": 759168,
      "step": 478
    },
    {
      "epoch": 0.08186634763288327,
      "grad_norm": 1.5331828594207764,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 1.3028,
      "num_input_tokens_seen": 760320,
      "step": 479
    },
    {
      "epoch": 0.08203725858827551,
      "grad_norm": 2.7154595851898193,
      "learning_rate": 3.104804738999169e-05,
      "loss": 1.1878,
      "num_input_tokens_seen": 761088,
      "step": 480
    },
    {
      "epoch": 0.08220816954366775,
      "grad_norm": 2.301790952682495,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 1.3269,
      "num_input_tokens_seen": 762112,
      "step": 481
    },
    {
      "epoch": 0.08237908049906,
      "grad_norm": 1.243520975112915,
      "learning_rate": 3.087855282756475e-05,
      "loss": 1.5544,
      "num_input_tokens_seen": 763392,
      "step": 482
    },
    {
      "epoch": 0.08254999145445223,
      "grad_norm": 1.0473334789276123,
      "learning_rate": 3.079369758735393e-05,
      "loss": 1.3377,
      "num_input_tokens_seen": 765056,
      "step": 483
    },
    {
      "epoch": 0.08272090240984448,
      "grad_norm": 1.0841411352157593,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 1.5681,
      "num_input_tokens_seen": 767360,
      "step": 484
    },
    {
      "epoch": 0.08289181336523671,
      "grad_norm": 1.2339277267456055,
      "learning_rate": 3.062377635859663e-05,
      "loss": 1.1849,
      "num_input_tokens_seen": 768384,
      "step": 485
    },
    {
      "epoch": 0.08306272432062896,
      "grad_norm": 1.243688941001892,
      "learning_rate": 3.053871244048669e-05,
      "loss": 1.2202,
      "num_input_tokens_seen": 769664,
      "step": 486
    },
    {
      "epoch": 0.08323363527602119,
      "grad_norm": 1.2321761846542358,
      "learning_rate": 3.045358103491357e-05,
      "loss": 1.1363,
      "num_input_tokens_seen": 771200,
      "step": 487
    },
    {
      "epoch": 0.08340454623141344,
      "grad_norm": 2.2877020835876465,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 1.8375,
      "num_input_tokens_seen": 772224,
      "step": 488
    },
    {
      "epoch": 0.08357545718680567,
      "grad_norm": 1.387783408164978,
      "learning_rate": 3.028311991138472e-05,
      "loss": 1.4233,
      "num_input_tokens_seen": 773632,
      "step": 489
    },
    {
      "epoch": 0.08374636814219792,
      "grad_norm": 1.3464401960372925,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 1.5371,
      "num_input_tokens_seen": 775296,
      "step": 490
    },
    {
      "epoch": 0.08391727909759016,
      "grad_norm": 1.5157991647720337,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 1.2327,
      "num_input_tokens_seen": 776576,
      "step": 491
    },
    {
      "epoch": 0.08408819005298239,
      "grad_norm": 2.181100368499756,
      "learning_rate": 3.002694802864912e-05,
      "loss": 1.3394,
      "num_input_tokens_seen": 777344,
      "step": 492
    },
    {
      "epoch": 0.08425910100837464,
      "grad_norm": 1.3370262384414673,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 1.1238,
      "num_input_tokens_seen": 778752,
      "step": 493
    },
    {
      "epoch": 0.08443001196376687,
      "grad_norm": 1.419992446899414,
      "learning_rate": 2.98558587804993e-05,
      "loss": 1.3413,
      "num_input_tokens_seen": 780032,
      "step": 494
    },
    {
      "epoch": 0.08460092291915912,
      "grad_norm": 1.305991530418396,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 1.2422,
      "num_input_tokens_seen": 781184,
      "step": 495
    },
    {
      "epoch": 0.08477183387455135,
      "grad_norm": 1.1314910650253296,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 1.1932,
      "num_input_tokens_seen": 783488,
      "step": 496
    },
    {
      "epoch": 0.0849427448299436,
      "grad_norm": 1.7882778644561768,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.8833,
      "num_input_tokens_seen": 784768,
      "step": 497
    },
    {
      "epoch": 0.08511365578533583,
      "grad_norm": 1.0985405445098877,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 1.1774,
      "num_input_tokens_seen": 786304,
      "step": 498
    },
    {
      "epoch": 0.08528456674072808,
      "grad_norm": 1.4248557090759277,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.8537,
      "num_input_tokens_seen": 787456,
      "step": 499
    },
    {
      "epoch": 0.08545547769612032,
      "grad_norm": 1.4204181432724,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 1.228,
      "num_input_tokens_seen": 788608,
      "step": 500
    },
    {
      "epoch": 0.08545547769612032,
      "eval_loss": 1.2713937759399414,
      "eval_runtime": 94.5053,
      "eval_samples_per_second": 55.034,
      "eval_steps_per_second": 6.888,
      "num_input_tokens_seen": 788608,
      "step": 500
    },
    {
      "epoch": 0.08562638865151256,
      "grad_norm": 0.9435201287269592,
      "learning_rate": 2.9255237479150816e-05,
      "loss": 0.8098,
      "num_input_tokens_seen": 790784,
      "step": 501
    },
    {
      "epoch": 0.0857972996069048,
      "grad_norm": 1.5942968130111694,
      "learning_rate": 2.916921866790256e-05,
      "loss": 1.1291,
      "num_input_tokens_seen": 791936,
      "step": 502
    },
    {
      "epoch": 0.08596821056229705,
      "grad_norm": 1.622069239616394,
      "learning_rate": 2.908314905604056e-05,
      "loss": 1.5954,
      "num_input_tokens_seen": 793472,
      "step": 503
    },
    {
      "epoch": 0.08613912151768928,
      "grad_norm": 1.6610897779464722,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 1.2803,
      "num_input_tokens_seen": 794496,
      "step": 504
    },
    {
      "epoch": 0.08631003247308153,
      "grad_norm": 1.1070635318756104,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 1.2393,
      "num_input_tokens_seen": 796928,
      "step": 505
    },
    {
      "epoch": 0.08648094342847376,
      "grad_norm": 1.3076180219650269,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 1.3103,
      "num_input_tokens_seen": 798720,
      "step": 506
    },
    {
      "epoch": 0.08665185438386601,
      "grad_norm": 1.2901500463485718,
      "learning_rate": 2.8738383586092745e-05,
      "loss": 1.362,
      "num_input_tokens_seen": 801024,
      "step": 507
    },
    {
      "epoch": 0.08682276533925824,
      "grad_norm": 1.0787369012832642,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 1.4225,
      "num_input_tokens_seen": 802432,
      "step": 508
    },
    {
      "epoch": 0.08699367629465049,
      "grad_norm": 1.7602185010910034,
      "learning_rate": 2.8565723342637796e-05,
      "loss": 1.3006,
      "num_input_tokens_seen": 803584,
      "step": 509
    },
    {
      "epoch": 0.08716458725004272,
      "grad_norm": 1.2483274936676025,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 1.2966,
      "num_input_tokens_seen": 804864,
      "step": 510
    },
    {
      "epoch": 0.08733549820543497,
      "grad_norm": 1.5651098489761353,
      "learning_rate": 2.8392889310857612e-05,
      "loss": 1.0733,
      "num_input_tokens_seen": 806272,
      "step": 511
    },
    {
      "epoch": 0.0875064091608272,
      "grad_norm": 0.8296051621437073,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 0.6841,
      "num_input_tokens_seen": 808832,
      "step": 512
    },
    {
      "epoch": 0.08767732011621945,
      "grad_norm": 1.6594685316085815,
      "learning_rate": 2.8219889914439074e-05,
      "loss": 1.769,
      "num_input_tokens_seen": 809984,
      "step": 513
    },
    {
      "epoch": 0.08784823107161169,
      "grad_norm": 1.1502628326416016,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 1.4217,
      "num_input_tokens_seen": 811904,
      "step": 514
    },
    {
      "epoch": 0.08801914202700394,
      "grad_norm": 1.1246191263198853,
      "learning_rate": 2.8046733585128687e-05,
      "loss": 0.9871,
      "num_input_tokens_seen": 813056,
      "step": 515
    },
    {
      "epoch": 0.08819005298239617,
      "grad_norm": 0.9974589347839355,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 1.5171,
      "num_input_tokens_seen": 815616,
      "step": 516
    },
    {
      "epoch": 0.08836096393778842,
      "grad_norm": 0.64259934425354,
      "learning_rate": 2.787342876232167e-05,
      "loss": 0.9968,
      "num_input_tokens_seen": 818816,
      "step": 517
    },
    {
      "epoch": 0.08853187489318065,
      "grad_norm": 1.318171739578247,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 1.4368,
      "num_input_tokens_seen": 820096,
      "step": 518
    },
    {
      "epoch": 0.0887027858485729,
      "grad_norm": 1.1985678672790527,
      "learning_rate": 2.7699983892650573e-05,
      "loss": 1.5969,
      "num_input_tokens_seen": 821632,
      "step": 519
    },
    {
      "epoch": 0.08887369680396513,
      "grad_norm": 0.978938102722168,
      "learning_rate": 2.761321158169134e-05,
      "loss": 1.4715,
      "num_input_tokens_seen": 823168,
      "step": 520
    },
    {
      "epoch": 0.08904460775935738,
      "grad_norm": 1.1694082021713257,
      "learning_rate": 2.7526407429573657e-05,
      "loss": 1.372,
      "num_input_tokens_seen": 824704,
      "step": 521
    },
    {
      "epoch": 0.08921551871474961,
      "grad_norm": 0.630893886089325,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 0.7563,
      "num_input_tokens_seen": 828544,
      "step": 522
    },
    {
      "epoch": 0.08938642967014186,
      "grad_norm": 1.4611320495605469,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 1.1372,
      "num_input_tokens_seen": 829440,
      "step": 523
    },
    {
      "epoch": 0.0895573406255341,
      "grad_norm": 1.3230888843536377,
      "learning_rate": 2.726581450494451e-05,
      "loss": 1.1059,
      "num_input_tokens_seen": 830848,
      "step": 524
    },
    {
      "epoch": 0.08972825158092634,
      "grad_norm": 1.3029413223266602,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.9889,
      "num_input_tokens_seen": 832128,
      "step": 525
    },
    {
      "epoch": 0.08989916253631858,
      "grad_norm": 1.7314943075180054,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 1.568,
      "num_input_tokens_seen": 833664,
      "step": 526
    },
    {
      "epoch": 0.09007007349171083,
      "grad_norm": 1.7178184986114502,
      "learning_rate": 2.7004973108221472e-05,
      "loss": 1.0745,
      "num_input_tokens_seen": 834816,
      "step": 527
    },
    {
      "epoch": 0.09024098444710306,
      "grad_norm": 1.002094030380249,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 1.5493,
      "num_input_tokens_seen": 837504,
      "step": 528
    },
    {
      "epoch": 0.0904118954024953,
      "grad_norm": 1.3845213651657104,
      "learning_rate": 2.6830954928190794e-05,
      "loss": 1.5135,
      "num_input_tokens_seen": 838656,
      "step": 529
    },
    {
      "epoch": 0.09058280635788754,
      "grad_norm": 1.297532081604004,
      "learning_rate": 2.674391184360313e-05,
      "loss": 1.2161,
      "num_input_tokens_seen": 840192,
      "step": 530
    },
    {
      "epoch": 0.09075371731327977,
      "grad_norm": 0.9695016145706177,
      "learning_rate": 2.6656847510000012e-05,
      "loss": 1.6072,
      "num_input_tokens_seen": 841984,
      "step": 531
    },
    {
      "epoch": 0.09092462826867202,
      "grad_norm": 0.947200357913971,
      "learning_rate": 2.656976298823284e-05,
      "loss": 1.0635,
      "num_input_tokens_seen": 844288,
      "step": 532
    },
    {
      "epoch": 0.09109553922406426,
      "grad_norm": 1.7799469232559204,
      "learning_rate": 2.6482659339399045e-05,
      "loss": 1.1921,
      "num_input_tokens_seen": 845696,
      "step": 533
    },
    {
      "epoch": 0.0912664501794565,
      "grad_norm": 1.1313903331756592,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 1.3163,
      "num_input_tokens_seen": 846848,
      "step": 534
    },
    {
      "epoch": 0.09143736113484874,
      "grad_norm": 1.0307281017303467,
      "learning_rate": 2.63083989060736e-05,
      "loss": 1.2868,
      "num_input_tokens_seen": 848768,
      "step": 535
    },
    {
      "epoch": 0.09160827209024099,
      "grad_norm": 1.2592157125473022,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 1.0092,
      "num_input_tokens_seen": 849792,
      "step": 536
    },
    {
      "epoch": 0.09177918304563322,
      "grad_norm": 1.1399788856506348,
      "learning_rate": 2.6134074703231344e-05,
      "loss": 1.3466,
      "num_input_tokens_seen": 851456,
      "step": 537
    },
    {
      "epoch": 0.09195009400102547,
      "grad_norm": 0.9796627759933472,
      "learning_rate": 2.604689134322999e-05,
      "loss": 1.0995,
      "num_input_tokens_seen": 853632,
      "step": 538
    },
    {
      "epoch": 0.0921210049564177,
      "grad_norm": 1.4835779666900635,
      "learning_rate": 2.5959695227188004e-05,
      "loss": 1.5259,
      "num_input_tokens_seen": 854912,
      "step": 539
    },
    {
      "epoch": 0.09229191591180995,
      "grad_norm": 1.332071304321289,
      "learning_rate": 2.587248741756253e-05,
      "loss": 1.4225,
      "num_input_tokens_seen": 856832,
      "step": 540
    },
    {
      "epoch": 0.09246282686720218,
      "grad_norm": 0.972199559211731,
      "learning_rate": 2.578526897695321e-05,
      "loss": 1.4049,
      "num_input_tokens_seen": 858368,
      "step": 541
    },
    {
      "epoch": 0.09263373782259443,
      "grad_norm": 1.412385106086731,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 1.3695,
      "num_input_tokens_seen": 859520,
      "step": 542
    },
    {
      "epoch": 0.09280464877798666,
      "grad_norm": 2.0965516567230225,
      "learning_rate": 2.5610804453816333e-05,
      "loss": 1.0995,
      "num_input_tokens_seen": 860544,
      "step": 543
    },
    {
      "epoch": 0.09297555973337891,
      "grad_norm": 1.461508870124817,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 1.4447,
      "num_input_tokens_seen": 861568,
      "step": 544
    },
    {
      "epoch": 0.09314647068877115,
      "grad_norm": 1.2964274883270264,
      "learning_rate": 2.5436310160932092e-05,
      "loss": 1.3969,
      "num_input_tokens_seen": 862976,
      "step": 545
    },
    {
      "epoch": 0.0933173816441634,
      "grad_norm": 1.2783985137939453,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 1.458,
      "num_input_tokens_seen": 864384,
      "step": 546
    },
    {
      "epoch": 0.09348829259955563,
      "grad_norm": 1.532828688621521,
      "learning_rate": 2.5261794602906145e-05,
      "loss": 1.7431,
      "num_input_tokens_seen": 866048,
      "step": 547
    },
    {
      "epoch": 0.09365920355494788,
      "grad_norm": 1.098285436630249,
      "learning_rate": 2.517453150744904e-05,
      "loss": 1.0869,
      "num_input_tokens_seen": 867328,
      "step": 548
    },
    {
      "epoch": 0.09383011451034011,
      "grad_norm": 0.5284131169319153,
      "learning_rate": 2.5087266285380596e-05,
      "loss": 0.7241,
      "num_input_tokens_seen": 872320,
      "step": 549
    },
    {
      "epoch": 0.09400102546573236,
      "grad_norm": 1.1318472623825073,
      "learning_rate": 2.5e-05,
      "loss": 1.3764,
      "num_input_tokens_seen": 874240,
      "step": 550
    },
    {
      "epoch": 0.09417193642112459,
      "grad_norm": 1.06272292137146,
      "learning_rate": 2.4912733714619417e-05,
      "loss": 1.1768,
      "num_input_tokens_seen": 875776,
      "step": 551
    },
    {
      "epoch": 0.09434284737651684,
      "grad_norm": 0.9299125671386719,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 1.2516,
      "num_input_tokens_seen": 877952,
      "step": 552
    },
    {
      "epoch": 0.09451375833190907,
      "grad_norm": 1.0536737442016602,
      "learning_rate": 2.4738205397093864e-05,
      "loss": 1.1364,
      "num_input_tokens_seen": 879360,
      "step": 553
    },
    {
      "epoch": 0.09468466928730132,
      "grad_norm": 0.8628309369087219,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 1.2655,
      "num_input_tokens_seen": 882304,
      "step": 554
    },
    {
      "epoch": 0.09485558024269355,
      "grad_norm": 1.4758273363113403,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.4489,
      "num_input_tokens_seen": 884096,
      "step": 555
    },
    {
      "epoch": 0.0950264911980858,
      "grad_norm": 1.0510971546173096,
      "learning_rate": 2.447643950291608e-05,
      "loss": 1.3062,
      "num_input_tokens_seen": 886272,
      "step": 556
    },
    {
      "epoch": 0.09519740215347804,
      "grad_norm": 1.3384654521942139,
      "learning_rate": 2.4389195546183673e-05,
      "loss": 1.4109,
      "num_input_tokens_seen": 887936,
      "step": 557
    },
    {
      "epoch": 0.09536831310887028,
      "grad_norm": 1.178639531135559,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 1.296,
      "num_input_tokens_seen": 889088,
      "step": 558
    },
    {
      "epoch": 0.09553922406426252,
      "grad_norm": 0.8505715131759644,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 1.7778,
      "num_input_tokens_seen": 891520,
      "step": 559
    },
    {
      "epoch": 0.09571013501965477,
      "grad_norm": 1.0355310440063477,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 1.2042,
      "num_input_tokens_seen": 893184,
      "step": 560
    },
    {
      "epoch": 0.095881045975047,
      "grad_norm": 1.797632098197937,
      "learning_rate": 2.4040304772812002e-05,
      "loss": 1.4942,
      "num_input_tokens_seen": 894336,
      "step": 561
    },
    {
      "epoch": 0.09605195693043925,
      "grad_norm": 0.9499807357788086,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 1.3852,
      "num_input_tokens_seen": 896768,
      "step": 562
    },
    {
      "epoch": 0.09622286788583148,
      "grad_norm": 1.5266753435134888,
      "learning_rate": 2.386592529676866e-05,
      "loss": 1.3447,
      "num_input_tokens_seen": 897792,
      "step": 563
    },
    {
      "epoch": 0.09639377884122373,
      "grad_norm": 1.5097604990005493,
      "learning_rate": 2.377875575510967e-05,
      "loss": 1.2861,
      "num_input_tokens_seen": 899072,
      "step": 564
    },
    {
      "epoch": 0.09656468979661596,
      "grad_norm": 1.5918703079223633,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 1.293,
      "num_input_tokens_seen": 900224,
      "step": 565
    },
    {
      "epoch": 0.09673560075200821,
      "grad_norm": 1.4949287176132202,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 1.2599,
      "num_input_tokens_seen": 901888,
      "step": 566
    },
    {
      "epoch": 0.09690651170740044,
      "grad_norm": 1.2148983478546143,
      "learning_rate": 2.3517340660600964e-05,
      "loss": 1.5607,
      "num_input_tokens_seen": 903552,
      "step": 567
    },
    {
      "epoch": 0.09707742266279268,
      "grad_norm": 0.9718085527420044,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 1.3694,
      "num_input_tokens_seen": 904832,
      "step": 568
    },
    {
      "epoch": 0.09724833361818493,
      "grad_norm": 1.0432908535003662,
      "learning_rate": 2.3343152490000004e-05,
      "loss": 1.4044,
      "num_input_tokens_seen": 907776,
      "step": 569
    },
    {
      "epoch": 0.09741924457357716,
      "grad_norm": 1.1604228019714355,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 1.1774,
      "num_input_tokens_seen": 909312,
      "step": 570
    },
    {
      "epoch": 0.09759015552896941,
      "grad_norm": 1.2084988355636597,
      "learning_rate": 2.3169045071809215e-05,
      "loss": 1.2665,
      "num_input_tokens_seen": 910976,
      "step": 571
    },
    {
      "epoch": 0.09776106648436164,
      "grad_norm": 1.0177490711212158,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 0.8388,
      "num_input_tokens_seen": 912640,
      "step": 572
    },
    {
      "epoch": 0.09793197743975389,
      "grad_norm": 1.1910157203674316,
      "learning_rate": 2.299502689177853e-05,
      "loss": 1.4722,
      "num_input_tokens_seen": 914304,
      "step": 573
    },
    {
      "epoch": 0.09810288839514612,
      "grad_norm": 1.2723486423492432,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 1.1211,
      "num_input_tokens_seen": 915712,
      "step": 574
    },
    {
      "epoch": 0.09827379935053837,
      "grad_norm": 1.2404265403747559,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 1.1557,
      "num_input_tokens_seen": 917120,
      "step": 575
    },
    {
      "epoch": 0.0984447103059306,
      "grad_norm": 1.5114257335662842,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 1.0828,
      "num_input_tokens_seen": 918528,
      "step": 576
    },
    {
      "epoch": 0.09861562126132285,
      "grad_norm": 0.8088451623916626,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 0.7406,
      "num_input_tokens_seen": 921984,
      "step": 577
    },
    {
      "epoch": 0.09878653221671509,
      "grad_norm": 2.008183240890503,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 1.1954,
      "num_input_tokens_seen": 922880,
      "step": 578
    },
    {
      "epoch": 0.09895744317210733,
      "grad_norm": 1.185408592224121,
      "learning_rate": 2.247359257042634e-05,
      "loss": 1.2041,
      "num_input_tokens_seen": 924672,
      "step": 579
    },
    {
      "epoch": 0.09912835412749957,
      "grad_norm": 1.1163522005081177,
      "learning_rate": 2.238678841830867e-05,
      "loss": 1.368,
      "num_input_tokens_seen": 926208,
      "step": 580
    },
    {
      "epoch": 0.09929926508289182,
      "grad_norm": 1.237580418586731,
      "learning_rate": 2.230001610734943e-05,
      "loss": 1.3498,
      "num_input_tokens_seen": 927616,
      "step": 581
    },
    {
      "epoch": 0.09947017603828405,
      "grad_norm": 1.4763119220733643,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 1.2805,
      "num_input_tokens_seen": 929024,
      "step": 582
    },
    {
      "epoch": 0.0996410869936763,
      "grad_norm": 1.6211227178573608,
      "learning_rate": 2.212657123767834e-05,
      "loss": 1.2487,
      "num_input_tokens_seen": 930048,
      "step": 583
    },
    {
      "epoch": 0.09981199794906853,
      "grad_norm": 1.2847617864608765,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 1.516,
      "num_input_tokens_seen": 931712,
      "step": 584
    },
    {
      "epoch": 0.09998290890446078,
      "grad_norm": 1.588565707206726,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.9255,
      "num_input_tokens_seen": 932864,
      "step": 585
    },
    {
      "epoch": 0.10015381985985301,
      "grad_norm": 1.1654152870178223,
      "learning_rate": 2.186666916089239e-05,
      "loss": 1.7865,
      "num_input_tokens_seen": 934656,
      "step": 586
    },
    {
      "epoch": 0.10032473081524526,
      "grad_norm": 1.1776905059814453,
      "learning_rate": 2.1780110085560935e-05,
      "loss": 1.349,
      "num_input_tokens_seen": 936064,
      "step": 587
    },
    {
      "epoch": 0.1004956417706375,
      "grad_norm": 1.591984510421753,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 1.3275,
      "num_input_tokens_seen": 937344,
      "step": 588
    },
    {
      "epoch": 0.10066655272602974,
      "grad_norm": 1.31329345703125,
      "learning_rate": 2.1607110689142393e-05,
      "loss": 1.377,
      "num_input_tokens_seen": 938624,
      "step": 589
    },
    {
      "epoch": 0.10083746368142198,
      "grad_norm": 0.9792755842208862,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 1.1728,
      "num_input_tokens_seen": 940928,
      "step": 590
    },
    {
      "epoch": 0.10100837463681422,
      "grad_norm": 1.5139389038085938,
      "learning_rate": 2.1434276657362213e-05,
      "loss": 1.2462,
      "num_input_tokens_seen": 942336,
      "step": 591
    },
    {
      "epoch": 0.10117928559220646,
      "grad_norm": 1.2313576936721802,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 1.5762,
      "num_input_tokens_seen": 943744,
      "step": 592
    },
    {
      "epoch": 0.1013501965475987,
      "grad_norm": 1.3691296577453613,
      "learning_rate": 2.1261616413907265e-05,
      "loss": 1.3728,
      "num_input_tokens_seen": 945536,
      "step": 593
    },
    {
      "epoch": 0.10152110750299094,
      "grad_norm": 1.270127534866333,
      "learning_rate": 2.117535409289905e-05,
      "loss": 1.1993,
      "num_input_tokens_seen": 946944,
      "step": 594
    },
    {
      "epoch": 0.10169201845838319,
      "grad_norm": 1.1039906740188599,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 1.3504,
      "num_input_tokens_seen": 948352,
      "step": 595
    },
    {
      "epoch": 0.10186292941377542,
      "grad_norm": 1.2766711711883545,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 1.3207,
      "num_input_tokens_seen": 950016,
      "step": 596
    },
    {
      "epoch": 0.10203384036916767,
      "grad_norm": 0.9672445058822632,
      "learning_rate": 2.0916850943959452e-05,
      "loss": 1.2456,
      "num_input_tokens_seen": 951552,
      "step": 597
    },
    {
      "epoch": 0.1022047513245599,
      "grad_norm": 1.6653071641921997,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 1.2404,
      "num_input_tokens_seen": 952704,
      "step": 598
    },
    {
      "epoch": 0.10237566227995215,
      "grad_norm": 1.0695490837097168,
      "learning_rate": 2.0744762520849193e-05,
      "loss": 1.199,
      "num_input_tokens_seen": 954112,
      "step": 599
    },
    {
      "epoch": 0.10254657323534438,
      "grad_norm": 0.9235876798629761,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 1.5369,
      "num_input_tokens_seen": 961536,
      "step": 600
    },
    {
      "epoch": 0.10271748419073663,
      "grad_norm": 0.9918472170829773,
      "learning_rate": 2.057288149201042e-05,
      "loss": 1.4282,
      "num_input_tokens_seen": 963072,
      "step": 601
    },
    {
      "epoch": 0.10288839514612887,
      "grad_norm": 1.161167025566101,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 1.0964,
      "num_input_tokens_seen": 964352,
      "step": 602
    },
    {
      "epoch": 0.10305930610152111,
      "grad_norm": 1.0260716676712036,
      "learning_rate": 2.0401216234681995e-05,
      "loss": 1.3517,
      "num_input_tokens_seen": 965760,
      "step": 603
    },
    {
      "epoch": 0.10323021705691335,
      "grad_norm": 0.8468192219734192,
      "learning_rate": 2.031546713535688e-05,
      "loss": 1.4266,
      "num_input_tokens_seen": 968192,
      "step": 604
    },
    {
      "epoch": 0.10340112801230558,
      "grad_norm": 1.4060041904449463,
      "learning_rate": 2.022977511558638e-05,
      "loss": 1.4354,
      "num_input_tokens_seen": 969472,
      "step": 605
    },
    {
      "epoch": 0.10357203896769783,
      "grad_norm": 1.617035984992981,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 1.5943,
      "num_input_tokens_seen": 970624,
      "step": 606
    },
    {
      "epoch": 0.10374294992309006,
      "grad_norm": 1.1065733432769775,
      "learning_rate": 2.0058566490521847e-05,
      "loss": 1.0489,
      "num_input_tokens_seen": 972544,
      "step": 607
    },
    {
      "epoch": 0.10391386087848231,
      "grad_norm": 1.357657790184021,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 1.0624,
      "num_input_tokens_seen": 973696,
      "step": 608
    },
    {
      "epoch": 0.10408477183387455,
      "grad_norm": 1.1065057516098022,
      "learning_rate": 1.9887598703955242e-05,
      "loss": 1.1602,
      "num_input_tokens_seen": 974848,
      "step": 609
    },
    {
      "epoch": 0.1042556827892668,
      "grad_norm": 1.3477582931518555,
      "learning_rate": 1.980220772955602e-05,
      "loss": 1.2696,
      "num_input_tokens_seen": 976256,
      "step": 610
    },
    {
      "epoch": 0.10442659374465903,
      "grad_norm": 1.3934171199798584,
      "learning_rate": 1.9716880088615285e-05,
      "loss": 1.2048,
      "num_input_tokens_seen": 977536,
      "step": 611
    },
    {
      "epoch": 0.10459750470005127,
      "grad_norm": 1.9246220588684082,
      "learning_rate": 1.963161682082342e-05,
      "loss": 1.0442,
      "num_input_tokens_seen": 978432,
      "step": 612
    },
    {
      "epoch": 0.10476841565544351,
      "grad_norm": 1.2233449220657349,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 1.4823,
      "num_input_tokens_seen": 979968,
      "step": 613
    },
    {
      "epoch": 0.10493932661083576,
      "grad_norm": 1.6480357646942139,
      "learning_rate": 1.946128755951332e-05,
      "loss": 1.6453,
      "num_input_tokens_seen": 980992,
      "step": 614
    },
    {
      "epoch": 0.10511023756622799,
      "grad_norm": 1.403598427772522,
      "learning_rate": 1.937622364140338e-05,
      "loss": 1.6529,
      "num_input_tokens_seen": 982656,
      "step": 615
    },
    {
      "epoch": 0.10528114852162024,
      "grad_norm": 1.3892121315002441,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 1.358,
      "num_input_tokens_seen": 984576,
      "step": 616
    },
    {
      "epoch": 0.10545205947701247,
      "grad_norm": 1.1073213815689087,
      "learning_rate": 1.920630241264607e-05,
      "loss": 1.2677,
      "num_input_tokens_seen": 985984,
      "step": 617
    },
    {
      "epoch": 0.10562297043240472,
      "grad_norm": 1.4297939538955688,
      "learning_rate": 1.912144717243525e-05,
      "loss": 1.5432,
      "num_input_tokens_seen": 987136,
      "step": 618
    },
    {
      "epoch": 0.10579388138779695,
      "grad_norm": 1.0541497468948364,
      "learning_rate": 1.9036663560535483e-05,
      "loss": 1.5883,
      "num_input_tokens_seen": 989696,
      "step": 619
    },
    {
      "epoch": 0.1059647923431892,
      "grad_norm": 0.9101724028587341,
      "learning_rate": 1.895195261000831e-05,
      "loss": 1.3441,
      "num_input_tokens_seen": 991104,
      "step": 620
    },
    {
      "epoch": 0.10613570329858144,
      "grad_norm": 1.4926962852478027,
      "learning_rate": 1.8867315353029935e-05,
      "loss": 1.4269,
      "num_input_tokens_seen": 992896,
      "step": 621
    },
    {
      "epoch": 0.10630661425397368,
      "grad_norm": 1.2964332103729248,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 1.4366,
      "num_input_tokens_seen": 995200,
      "step": 622
    },
    {
      "epoch": 0.10647752520936592,
      "grad_norm": 1.1922842264175415,
      "learning_rate": 1.869826604392216e-05,
      "loss": 1.5106,
      "num_input_tokens_seen": 997248,
      "step": 623
    },
    {
      "epoch": 0.10664843616475816,
      "grad_norm": 0.9891872406005859,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 0.9826,
      "num_input_tokens_seen": 998912,
      "step": 624
    },
    {
      "epoch": 0.1068193471201504,
      "grad_norm": 1.547795057296753,
      "learning_rate": 1.852952387243698e-05,
      "loss": 1.4587,
      "num_input_tokens_seen": 1000448,
      "step": 625
    },
    {
      "epoch": 0.10699025807554265,
      "grad_norm": 1.3257553577423096,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 1.0987,
      "num_input_tokens_seen": 1001600,
      "step": 626
    },
    {
      "epoch": 0.10716116903093488,
      "grad_norm": 1.2558335065841675,
      "learning_rate": 1.8361097062829778e-05,
      "loss": 1.4123,
      "num_input_tokens_seen": 1003264,
      "step": 627
    },
    {
      "epoch": 0.10733207998632713,
      "grad_norm": 1.7761154174804688,
      "learning_rate": 1.827700448461836e-05,
      "loss": 1.4771,
      "num_input_tokens_seen": 1004416,
      "step": 628
    },
    {
      "epoch": 0.10750299094171936,
      "grad_norm": 1.0228434801101685,
      "learning_rate": 1.8192993823985643e-05,
      "loss": 1.4215,
      "num_input_tokens_seen": 1006080,
      "step": 629
    },
    {
      "epoch": 0.10767390189711161,
      "grad_norm": 1.438772201538086,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 1.3993,
      "num_input_tokens_seen": 1007616,
      "step": 630
    },
    {
      "epoch": 0.10784481285250384,
      "grad_norm": 1.0083057880401611,
      "learning_rate": 1.802522234901927e-05,
      "loss": 1.3203,
      "num_input_tokens_seen": 1009536,
      "step": 631
    },
    {
      "epoch": 0.10801572380789609,
      "grad_norm": 1.4458520412445068,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 1.1621,
      "num_input_tokens_seen": 1010944,
      "step": 632
    },
    {
      "epoch": 0.10818663476328833,
      "grad_norm": 1.319189429283142,
      "learning_rate": 1.7857790814875663e-05,
      "loss": 1.1524,
      "num_input_tokens_seen": 1012352,
      "step": 633
    },
    {
      "epoch": 0.10835754571868057,
      "grad_norm": 1.471014142036438,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 1.4067,
      "num_input_tokens_seen": 1013632,
      "step": 634
    },
    {
      "epoch": 0.1085284566740728,
      "grad_norm": 1.4747174978256226,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 1.2748,
      "num_input_tokens_seen": 1015040,
      "step": 635
    },
    {
      "epoch": 0.10869936762946505,
      "grad_norm": 1.2345759868621826,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 1.2164,
      "num_input_tokens_seen": 1016320,
      "step": 636
    },
    {
      "epoch": 0.10887027858485729,
      "grad_norm": 1.1618794202804565,
      "learning_rate": 1.7523980193597836e-05,
      "loss": 0.9677,
      "num_input_tokens_seen": 1017728,
      "step": 637
    },
    {
      "epoch": 0.10904118954024954,
      "grad_norm": 1.6214568614959717,
      "learning_rate": 1.744075273123889e-05,
      "loss": 1.2446,
      "num_input_tokens_seen": 1019136,
      "step": 638
    },
    {
      "epoch": 0.10921210049564177,
      "grad_norm": 1.3339771032333374,
      "learning_rate": 1.735761737592236e-05,
      "loss": 1.1489,
      "num_input_tokens_seen": 1020544,
      "step": 639
    },
    {
      "epoch": 0.10938301145103402,
      "grad_norm": 1.5254807472229004,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 1.2662,
      "num_input_tokens_seen": 1021696,
      "step": 640
    },
    {
      "epoch": 0.10955392240642625,
      "grad_norm": 1.2224225997924805,
      "learning_rate": 1.7191627037194186e-05,
      "loss": 1.3068,
      "num_input_tokens_seen": 1023232,
      "step": 641
    },
    {
      "epoch": 0.10972483336181849,
      "grad_norm": 0.9965255260467529,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 1.2849,
      "num_input_tokens_seen": 1024896,
      "step": 642
    },
    {
      "epoch": 0.10989574431721073,
      "grad_norm": 1.0401102304458618,
      "learning_rate": 1.702601726754825e-05,
      "loss": 1.4098,
      "num_input_tokens_seen": 1026304,
      "step": 643
    },
    {
      "epoch": 0.11006665527260297,
      "grad_norm": 1.4526801109313965,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 1.4457,
      "num_input_tokens_seen": 1027584,
      "step": 644
    },
    {
      "epoch": 0.11023756622799522,
      "grad_norm": 1.0784152746200562,
      "learning_rate": 1.686079613857109e-05,
      "loss": 1.4423,
      "num_input_tokens_seen": 1029504,
      "step": 645
    },
    {
      "epoch": 0.11040847718338745,
      "grad_norm": 0.9229816794395447,
      "learning_rate": 1.677833383153542e-05,
      "loss": 0.9476,
      "num_input_tokens_seen": 1032576,
      "step": 646
    },
    {
      "epoch": 0.1105793881387797,
      "grad_norm": 1.1531165838241577,
      "learning_rate": 1.6695971702907426e-05,
      "loss": 1.2861,
      "num_input_tokens_seen": 1033856,
      "step": 647
    },
    {
      "epoch": 0.11075029909417193,
      "grad_norm": 1.1383639574050903,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 1.1628,
      "num_input_tokens_seen": 1035008,
      "step": 648
    },
    {
      "epoch": 0.11092121004956418,
      "grad_norm": 0.8012166619300842,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 1.3227,
      "num_input_tokens_seen": 1038336,
      "step": 649
    },
    {
      "epoch": 0.11109212100495641,
      "grad_norm": 1.1785551309585571,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 1.1829,
      "num_input_tokens_seen": 1040512,
      "step": 650
    },
    {
      "epoch": 0.11126303196034866,
      "grad_norm": 1.500640869140625,
      "learning_rate": 1.6367545025036636e-05,
      "loss": 1.3953,
      "num_input_tokens_seen": 1042304,
      "step": 651
    },
    {
      "epoch": 0.1114339429157409,
      "grad_norm": 1.0249502658843994,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 1.2723,
      "num_input_tokens_seen": 1044608,
      "step": 652
    },
    {
      "epoch": 0.11160485387113314,
      "grad_norm": 1.6419132947921753,
      "learning_rate": 1.6203958789882456e-05,
      "loss": 1.3075,
      "num_input_tokens_seen": 1045504,
      "step": 653
    },
    {
      "epoch": 0.11177576482652538,
      "grad_norm": 1.325859546661377,
      "learning_rate": 1.612232593979658e-05,
      "loss": 1.3643,
      "num_input_tokens_seen": 1047168,
      "step": 654
    },
    {
      "epoch": 0.11194667578191762,
      "grad_norm": 1.0605961084365845,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 1.3491,
      "num_input_tokens_seen": 1049216,
      "step": 655
    },
    {
      "epoch": 0.11211758673730986,
      "grad_norm": 1.2981783151626587,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 1.4421,
      "num_input_tokens_seen": 1050752,
      "step": 656
    },
    {
      "epoch": 0.1122884976927021,
      "grad_norm": 2.109013795852661,
      "learning_rate": 1.5878080391559508e-05,
      "loss": 1.2836,
      "num_input_tokens_seen": 1051648,
      "step": 657
    },
    {
      "epoch": 0.11245940864809434,
      "grad_norm": 1.0177870988845825,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 0.6844,
      "num_input_tokens_seen": 1053056,
      "step": 658
    },
    {
      "epoch": 0.11263031960348659,
      "grad_norm": 0.8575263023376465,
      "learning_rate": 1.5715804111244137e-05,
      "loss": 1.0228,
      "num_input_tokens_seen": 1054848,
      "step": 659
    },
    {
      "epoch": 0.11280123055887882,
      "grad_norm": 1.5244112014770508,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.9414,
      "num_input_tokens_seen": 1056384,
      "step": 660
    },
    {
      "epoch": 0.11297214151427107,
      "grad_norm": 1.3835312128067017,
      "learning_rate": 1.5553980329538326e-05,
      "loss": 1.2256,
      "num_input_tokens_seen": 1057536,
      "step": 661
    },
    {
      "epoch": 0.1131430524696633,
      "grad_norm": 1.3551915884017944,
      "learning_rate": 1.547324059124315e-05,
      "loss": 0.9657,
      "num_input_tokens_seen": 1058688,
      "step": 662
    },
    {
      "epoch": 0.11331396342505555,
      "grad_norm": 1.659938931465149,
      "learning_rate": 1.539261693350491e-05,
      "loss": 1.3143,
      "num_input_tokens_seen": 1059968,
      "step": 663
    },
    {
      "epoch": 0.11348487438044778,
      "grad_norm": 1.4172837734222412,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 1.4329,
      "num_input_tokens_seen": 1061376,
      "step": 664
    },
    {
      "epoch": 0.11365578533584003,
      "grad_norm": 1.284745216369629,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.9277,
      "num_input_tokens_seen": 1062784,
      "step": 665
    },
    {
      "epoch": 0.11382669629123227,
      "grad_norm": 1.4969379901885986,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 1.4497,
      "num_input_tokens_seen": 1065088,
      "step": 666
    },
    {
      "epoch": 0.11399760724662451,
      "grad_norm": 1.4705729484558105,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 1.6013,
      "num_input_tokens_seen": 1066368,
      "step": 667
    },
    {
      "epoch": 0.11416851820201675,
      "grad_norm": 1.075469732284546,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 1.3647,
      "num_input_tokens_seen": 1068800,
      "step": 668
    },
    {
      "epoch": 0.114339429157409,
      "grad_norm": 1.1814297437667847,
      "learning_rate": 1.4911367591190248e-05,
      "loss": 1.655,
      "num_input_tokens_seen": 1070464,
      "step": 669
    },
    {
      "epoch": 0.11451034011280123,
      "grad_norm": 1.2877715826034546,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 1.5648,
      "num_input_tokens_seen": 1072640,
      "step": 670
    },
    {
      "epoch": 0.11468125106819348,
      "grad_norm": 1.0169984102249146,
      "learning_rate": 1.475192415396068e-05,
      "loss": 1.4637,
      "num_input_tokens_seen": 1075328,
      "step": 671
    },
    {
      "epoch": 0.11485216202358571,
      "grad_norm": 1.527280569076538,
      "learning_rate": 1.467238925438646e-05,
      "loss": 1.3999,
      "num_input_tokens_seen": 1076608,
      "step": 672
    },
    {
      "epoch": 0.11502307297897796,
      "grad_norm": 1.291388750076294,
      "learning_rate": 1.4592980193489975e-05,
      "loss": 1.3635,
      "num_input_tokens_seen": 1077888,
      "step": 673
    },
    {
      "epoch": 0.11519398393437019,
      "grad_norm": 1.3838521242141724,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 1.3868,
      "num_input_tokens_seen": 1079296,
      "step": 674
    },
    {
      "epoch": 0.11536489488976244,
      "grad_norm": 1.1320810317993164,
      "learning_rate": 1.443454345648252e-05,
      "loss": 1.243,
      "num_input_tokens_seen": 1080704,
      "step": 675
    },
    {
      "epoch": 0.11553580584515467,
      "grad_norm": 0.8380499482154846,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 1.4454,
      "num_input_tokens_seen": 1083648,
      "step": 676
    },
    {
      "epoch": 0.11570671680054692,
      "grad_norm": 1.0963497161865234,
      "learning_rate": 1.4276621664921357e-05,
      "loss": 1.3141,
      "num_input_tokens_seen": 1085312,
      "step": 677
    },
    {
      "epoch": 0.11587762775593916,
      "grad_norm": 1.1410123109817505,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 1.4158,
      "num_input_tokens_seen": 1086848,
      "step": 678
    },
    {
      "epoch": 0.1160485387113314,
      "grad_norm": 1.2661707401275635,
      "learning_rate": 1.4119222515691816e-05,
      "loss": 1.5083,
      "num_input_tokens_seen": 1088256,
      "step": 679
    },
    {
      "epoch": 0.11621944966672364,
      "grad_norm": 1.1897021532058716,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.9872,
      "num_input_tokens_seen": 1090048,
      "step": 680
    },
    {
      "epoch": 0.11639036062211587,
      "grad_norm": 1.4536880254745483,
      "learning_rate": 1.3962353680206373e-05,
      "loss": 1.4424,
      "num_input_tokens_seen": 1091840,
      "step": 681
    },
    {
      "epoch": 0.11656127157750812,
      "grad_norm": 1.347129464149475,
      "learning_rate": 1.388412052037682e-05,
      "loss": 1.4098,
      "num_input_tokens_seen": 1093120,
      "step": 682
    },
    {
      "epoch": 0.11673218253290035,
      "grad_norm": 1.2368675470352173,
      "learning_rate": 1.380602280403076e-05,
      "loss": 1.2387,
      "num_input_tokens_seen": 1094784,
      "step": 683
    },
    {
      "epoch": 0.1169030934882926,
      "grad_norm": 0.8726116418838501,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 0.8631,
      "num_input_tokens_seen": 1096448,
      "step": 684
    },
    {
      "epoch": 0.11707400444368483,
      "grad_norm": 1.5856844186782837,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 1.5311,
      "num_input_tokens_seen": 1097472,
      "step": 685
    },
    {
      "epoch": 0.11724491539907708,
      "grad_norm": 0.9871749877929688,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 1.1686,
      "num_input_tokens_seen": 1099008,
      "step": 686
    },
    {
      "epoch": 0.11741582635446932,
      "grad_norm": 1.0502396821975708,
      "learning_rate": 1.349500538040371e-05,
      "loss": 1.2378,
      "num_input_tokens_seen": 1100800,
      "step": 687
    },
    {
      "epoch": 0.11758673730986156,
      "grad_norm": 1.5493625402450562,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 1.4838,
      "num_input_tokens_seen": 1102080,
      "step": 688
    },
    {
      "epoch": 0.1177576482652538,
      "grad_norm": 1.162373661994934,
      "learning_rate": 1.3340333991502724e-05,
      "loss": 1.3365,
      "num_input_tokens_seen": 1103744,
      "step": 689
    },
    {
      "epoch": 0.11792855922064605,
      "grad_norm": 1.7563554048538208,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 1.5194,
      "num_input_tokens_seen": 1104768,
      "step": 690
    },
    {
      "epoch": 0.11809947017603828,
      "grad_norm": 1.5500811338424683,
      "learning_rate": 1.3186230878273653e-05,
      "loss": 1.2011,
      "num_input_tokens_seen": 1106304,
      "step": 691
    },
    {
      "epoch": 0.11827038113143053,
      "grad_norm": 1.1013985872268677,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 1.4205,
      "num_input_tokens_seen": 1107840,
      "step": 692
    },
    {
      "epoch": 0.11844129208682276,
      "grad_norm": 1.0171910524368286,
      "learning_rate": 1.3032703551484832e-05,
      "loss": 1.3721,
      "num_input_tokens_seen": 1110528,
      "step": 693
    },
    {
      "epoch": 0.11861220304221501,
      "grad_norm": 0.9964315295219421,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 1.2709,
      "num_input_tokens_seen": 1112832,
      "step": 694
    },
    {
      "epoch": 0.11878311399760724,
      "grad_norm": 1.6083248853683472,
      "learning_rate": 1.2879759493841575e-05,
      "loss": 1.3508,
      "num_input_tokens_seen": 1114752,
      "step": 695
    },
    {
      "epoch": 0.11895402495299949,
      "grad_norm": 0.9933610558509827,
      "learning_rate": 1.280350852153168e-05,
      "loss": 1.3252,
      "num_input_tokens_seen": 1117184,
      "step": 696
    },
    {
      "epoch": 0.11912493590839172,
      "grad_norm": 1.1770957708358765,
      "learning_rate": 1.272740615962148e-05,
      "loss": 0.8943,
      "num_input_tokens_seen": 1118336,
      "step": 697
    },
    {
      "epoch": 0.11929584686378397,
      "grad_norm": 1.12267005443573,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 1.1257,
      "num_input_tokens_seen": 1120000,
      "step": 698
    },
    {
      "epoch": 0.1194667578191762,
      "grad_norm": 2.245288610458374,
      "learning_rate": 1.2575650974311119e-05,
      "loss": 1.0814,
      "num_input_tokens_seen": 1120896,
      "step": 699
    },
    {
      "epoch": 0.11963766877456845,
      "grad_norm": 1.5521953105926514,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 1.3008,
      "num_input_tokens_seen": 1122304,
      "step": 700
    },
    {
      "epoch": 0.11980857972996069,
      "grad_norm": 0.9776777625083923,
      "learning_rate": 1.2424501334244123e-05,
      "loss": 1.0047,
      "num_input_tokens_seen": 1124864,
      "step": 701
    },
    {
      "epoch": 0.11997949068535294,
      "grad_norm": 1.1953985691070557,
      "learning_rate": 1.234915589697091e-05,
      "loss": 1.1082,
      "num_input_tokens_seen": 1126400,
      "step": 702
    },
    {
      "epoch": 0.12015040164074517,
      "grad_norm": 1.4796253442764282,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 1.1687,
      "num_input_tokens_seen": 1127552,
      "step": 703
    },
    {
      "epoch": 0.12032131259613742,
      "grad_norm": 1.1682941913604736,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 1.482,
      "num_input_tokens_seen": 1129344,
      "step": 704
    },
    {
      "epoch": 0.12049222355152965,
      "grad_norm": 1.471303939819336,
      "learning_rate": 1.2124048127248644e-05,
      "loss": 1.1888,
      "num_input_tokens_seen": 1130368,
      "step": 705
    },
    {
      "epoch": 0.1206631345069219,
      "grad_norm": 1.3847638368606567,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 1.716,
      "num_input_tokens_seen": 1131776,
      "step": 706
    },
    {
      "epoch": 0.12083404546231413,
      "grad_norm": 1.6496864557266235,
      "learning_rate": 1.19747592039856e-05,
      "loss": 1.2302,
      "num_input_tokens_seen": 1132800,
      "step": 707
    },
    {
      "epoch": 0.12100495641770638,
      "grad_norm": 1.407829761505127,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 1.4256,
      "num_input_tokens_seen": 1134208,
      "step": 708
    },
    {
      "epoch": 0.12117586737309861,
      "grad_norm": 1.2217650413513184,
      "learning_rate": 1.1826105112583061e-05,
      "loss": 1.2228,
      "num_input_tokens_seen": 1135616,
      "step": 709
    },
    {
      "epoch": 0.12134677832849086,
      "grad_norm": 1.0453135967254639,
      "learning_rate": 1.175201839416988e-05,
      "loss": 1.1616,
      "num_input_tokens_seen": 1137920,
      "step": 710
    },
    {
      "epoch": 0.1215176892838831,
      "grad_norm": 1.2075927257537842,
      "learning_rate": 1.167809309823175e-05,
      "loss": 1.428,
      "num_input_tokens_seen": 1139584,
      "step": 711
    },
    {
      "epoch": 0.12168860023927534,
      "grad_norm": 1.8834657669067383,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 1.338,
      "num_input_tokens_seen": 1140992,
      "step": 712
    },
    {
      "epoch": 0.12185951119466758,
      "grad_norm": 1.3408688306808472,
      "learning_rate": 1.1530730374828422e-05,
      "loss": 1.4084,
      "num_input_tokens_seen": 1142400,
      "step": 713
    },
    {
      "epoch": 0.12203042215005983,
      "grad_norm": 1.9438209533691406,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 1.1795,
      "num_input_tokens_seen": 1143552,
      "step": 714
    },
    {
      "epoch": 0.12220133310545206,
      "grad_norm": 0.7423321008682251,
      "learning_rate": 1.1384024124624324e-05,
      "loss": 1.5257,
      "num_input_tokens_seen": 1146752,
      "step": 715
    },
    {
      "epoch": 0.1223722440608443,
      "grad_norm": 0.9299507141113281,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 1.2779,
      "num_input_tokens_seen": 1149440,
      "step": 716
    },
    {
      "epoch": 0.12254315501623654,
      "grad_norm": 1.5625783205032349,
      "learning_rate": 1.123798149787511e-05,
      "loss": 1.0636,
      "num_input_tokens_seen": 1150464,
      "step": 717
    },
    {
      "epoch": 0.12271406597162877,
      "grad_norm": 1.3238623142242432,
      "learning_rate": 1.11652112689164e-05,
      "loss": 1.0363,
      "num_input_tokens_seen": 1151744,
      "step": 718
    },
    {
      "epoch": 0.12288497692702102,
      "grad_norm": 1.721549153327942,
      "learning_rate": 1.109260961249238e-05,
      "loss": 1.0088,
      "num_input_tokens_seen": 1152896,
      "step": 719
    },
    {
      "epoch": 0.12305588788241326,
      "grad_norm": 1.606882929801941,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 1.1641,
      "num_input_tokens_seen": 1154048,
      "step": 720
    },
    {
      "epoch": 0.1232267988378055,
      "grad_norm": 1.0093653202056885,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 1.1961,
      "num_input_tokens_seen": 1156096,
      "step": 721
    },
    {
      "epoch": 0.12339770979319774,
      "grad_norm": 1.9638255834579468,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 1.5346,
      "num_input_tokens_seen": 1157376,
      "step": 722
    },
    {
      "epoch": 0.12356862074858999,
      "grad_norm": 1.4030635356903076,
      "learning_rate": 1.0803906373672476e-05,
      "loss": 1.3196,
      "num_input_tokens_seen": 1158784,
      "step": 723
    },
    {
      "epoch": 0.12373953170398222,
      "grad_norm": 2.373340129852295,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 0.999,
      "num_input_tokens_seen": 1159808,
      "step": 724
    },
    {
      "epoch": 0.12391044265937447,
      "grad_norm": 1.0334622859954834,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 1.0773,
      "num_input_tokens_seen": 1161600,
      "step": 725
    },
    {
      "epoch": 0.1240813536147667,
      "grad_norm": 0.856156587600708,
      "learning_rate": 1.058919209575517e-05,
      "loss": 1.3704,
      "num_input_tokens_seen": 1164800,
      "step": 726
    },
    {
      "epoch": 0.12425226457015895,
      "grad_norm": 1.2265903949737549,
      "learning_rate": 1.0517970691433035e-05,
      "loss": 1.2002,
      "num_input_tokens_seen": 1166336,
      "step": 727
    },
    {
      "epoch": 0.12442317552555118,
      "grad_norm": 1.421107292175293,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 1.2089,
      "num_input_tokens_seen": 1167616,
      "step": 728
    },
    {
      "epoch": 0.12459408648094343,
      "grad_norm": 1.1234277486801147,
      "learning_rate": 1.0376058125319613e-05,
      "loss": 1.2768,
      "num_input_tokens_seen": 1169152,
      "step": 729
    },
    {
      "epoch": 0.12476499743633566,
      "grad_norm": 1.2909152507781982,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 1.3682,
      "num_input_tokens_seen": 1170560,
      "step": 730
    },
    {
      "epoch": 0.12493590839172791,
      "grad_norm": 1.3442028760910034,
      "learning_rate": 1.0234858309501862e-05,
      "loss": 1.1847,
      "num_input_tokens_seen": 1171968,
      "step": 731
    },
    {
      "epoch": 0.12510681934712015,
      "grad_norm": 0.7610854506492615,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 1.3441,
      "num_input_tokens_seen": 1174656,
      "step": 732
    },
    {
      "epoch": 0.12527773030251238,
      "grad_norm": 1.2540740966796875,
      "learning_rate": 1.0094378125859602e-05,
      "loss": 1.6075,
      "num_input_tokens_seen": 1176320,
      "step": 733
    },
    {
      "epoch": 0.12544864125790464,
      "grad_norm": 1.157147765159607,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 1.4551,
      "num_input_tokens_seen": 1177728,
      "step": 734
    },
    {
      "epoch": 0.12561955221329688,
      "grad_norm": 0.8779391646385193,
      "learning_rate": 9.954624421198792e-06,
      "loss": 1.5882,
      "num_input_tokens_seen": 1179520,
      "step": 735
    },
    {
      "epoch": 0.1257904631686891,
      "grad_norm": 1.2403241395950317,
      "learning_rate": 9.88502212844063e-06,
      "loss": 1.1213,
      "num_input_tokens_seen": 1180672,
      "step": 736
    },
    {
      "epoch": 0.12596137412408134,
      "grad_norm": 0.9053582549095154,
      "learning_rate": 9.815604006917839e-06,
      "loss": 1.6425,
      "num_input_tokens_seen": 1182848,
      "step": 737
    },
    {
      "epoch": 0.1261322850794736,
      "grad_norm": 1.5042582750320435,
      "learning_rate": 9.746370902468311e-06,
      "loss": 1.5481,
      "num_input_tokens_seen": 1184768,
      "step": 738
    },
    {
      "epoch": 0.12630319603486584,
      "grad_norm": 1.4559415578842163,
      "learning_rate": 9.677323658675594e-06,
      "loss": 1.4168,
      "num_input_tokens_seen": 1186304,
      "step": 739
    },
    {
      "epoch": 0.12647410699025807,
      "grad_norm": 1.1749237775802612,
      "learning_rate": 9.608463116858542e-06,
      "loss": 1.4041,
      "num_input_tokens_seen": 1187584,
      "step": 740
    },
    {
      "epoch": 0.1266450179456503,
      "grad_norm": 1.353429913520813,
      "learning_rate": 9.539790116061151e-06,
      "loss": 1.3074,
      "num_input_tokens_seen": 1188992,
      "step": 741
    },
    {
      "epoch": 0.12681592890104257,
      "grad_norm": 1.2949681282043457,
      "learning_rate": 9.471305493042243e-06,
      "loss": 1.2236,
      "num_input_tokens_seen": 1190272,
      "step": 742
    },
    {
      "epoch": 0.1269868398564348,
      "grad_norm": 1.3229597806930542,
      "learning_rate": 9.403010082265351e-06,
      "loss": 1.7545,
      "num_input_tokens_seen": 1192064,
      "step": 743
    },
    {
      "epoch": 0.12715775081182704,
      "grad_norm": 0.9179220199584961,
      "learning_rate": 9.334904715888495e-06,
      "loss": 1.236,
      "num_input_tokens_seen": 1193984,
      "step": 744
    },
    {
      "epoch": 0.12732866176721927,
      "grad_norm": 1.0979444980621338,
      "learning_rate": 9.266990223754069e-06,
      "loss": 1.0922,
      "num_input_tokens_seen": 1195264,
      "step": 745
    },
    {
      "epoch": 0.12749957272261153,
      "grad_norm": 1.4876583814620972,
      "learning_rate": 9.199267433378727e-06,
      "loss": 1.3805,
      "num_input_tokens_seen": 1196416,
      "step": 746
    },
    {
      "epoch": 0.12767048367800377,
      "grad_norm": 1.5479142665863037,
      "learning_rate": 9.131737169943314e-06,
      "loss": 1.2654,
      "num_input_tokens_seen": 1197824,
      "step": 747
    },
    {
      "epoch": 0.127841394633396,
      "grad_norm": 0.9928240180015564,
      "learning_rate": 9.064400256282757e-06,
      "loss": 1.1301,
      "num_input_tokens_seen": 1200768,
      "step": 748
    },
    {
      "epoch": 0.12801230558878823,
      "grad_norm": 1.0777939558029175,
      "learning_rate": 8.997257512876108e-06,
      "loss": 1.5302,
      "num_input_tokens_seen": 1202560,
      "step": 749
    },
    {
      "epoch": 0.1281832165441805,
      "grad_norm": 2.1472179889678955,
      "learning_rate": 8.930309757836517e-06,
      "loss": 1.1756,
      "num_input_tokens_seen": 1203712,
      "step": 750
    },
    {
      "epoch": 0.12835412749957273,
      "grad_norm": 1.625246286392212,
      "learning_rate": 8.863557806901233e-06,
      "loss": 1.4685,
      "num_input_tokens_seen": 1204992,
      "step": 751
    },
    {
      "epoch": 0.12852503845496496,
      "grad_norm": 1.290174126625061,
      "learning_rate": 8.797002473421728e-06,
      "loss": 1.2136,
      "num_input_tokens_seen": 1206144,
      "step": 752
    },
    {
      "epoch": 0.1286959494103572,
      "grad_norm": 1.4268299341201782,
      "learning_rate": 8.73064456835373e-06,
      "loss": 1.6703,
      "num_input_tokens_seen": 1207552,
      "step": 753
    },
    {
      "epoch": 0.12886686036574946,
      "grad_norm": 0.9264057874679565,
      "learning_rate": 8.664484900247363e-06,
      "loss": 1.7117,
      "num_input_tokens_seen": 1211264,
      "step": 754
    },
    {
      "epoch": 0.1290377713211417,
      "grad_norm": 1.1656503677368164,
      "learning_rate": 8.598524275237322e-06,
      "loss": 1.3677,
      "num_input_tokens_seen": 1212928,
      "step": 755
    },
    {
      "epoch": 0.12920868227653393,
      "grad_norm": 1.7683355808258057,
      "learning_rate": 8.532763497032987e-06,
      "loss": 1.2479,
      "num_input_tokens_seen": 1214336,
      "step": 756
    },
    {
      "epoch": 0.12937959323192616,
      "grad_norm": 0.8763198852539062,
      "learning_rate": 8.467203366908707e-06,
      "loss": 1.2283,
      "num_input_tokens_seen": 1216640,
      "step": 757
    },
    {
      "epoch": 0.1295505041873184,
      "grad_norm": 1.0671736001968384,
      "learning_rate": 8.40184468369396e-06,
      "loss": 1.4902,
      "num_input_tokens_seen": 1218048,
      "step": 758
    },
    {
      "epoch": 0.12972141514271066,
      "grad_norm": 1.0200616121292114,
      "learning_rate": 8.33668824376369e-06,
      "loss": 1.5337,
      "num_input_tokens_seen": 1220224,
      "step": 759
    },
    {
      "epoch": 0.1298923260981029,
      "grad_norm": 0.8875368237495422,
      "learning_rate": 8.271734841028553e-06,
      "loss": 1.0351,
      "num_input_tokens_seen": 1223424,
      "step": 760
    },
    {
      "epoch": 0.13006323705349512,
      "grad_norm": 0.9490101933479309,
      "learning_rate": 8.206985266925249e-06,
      "loss": 1.232,
      "num_input_tokens_seen": 1225728,
      "step": 761
    },
    {
      "epoch": 0.13023414800888736,
      "grad_norm": 1.2699040174484253,
      "learning_rate": 8.142440310406924e-06,
      "loss": 1.2068,
      "num_input_tokens_seen": 1227008,
      "step": 762
    },
    {
      "epoch": 0.13040505896427962,
      "grad_norm": 1.5364089012145996,
      "learning_rate": 8.078100757933485e-06,
      "loss": 1.2791,
      "num_input_tokens_seen": 1228800,
      "step": 763
    },
    {
      "epoch": 0.13057596991967185,
      "grad_norm": 0.7191441059112549,
      "learning_rate": 8.013967393462094e-06,
      "loss": 1.1798,
      "num_input_tokens_seen": 1231616,
      "step": 764
    },
    {
      "epoch": 0.1307468808750641,
      "grad_norm": 1.3384076356887817,
      "learning_rate": 7.950040998437542e-06,
      "loss": 0.7162,
      "num_input_tokens_seen": 1232640,
      "step": 765
    },
    {
      "epoch": 0.13091779183045632,
      "grad_norm": 1.3885738849639893,
      "learning_rate": 7.886322351782783e-06,
      "loss": 1.1372,
      "num_input_tokens_seen": 1233792,
      "step": 766
    },
    {
      "epoch": 0.13108870278584858,
      "grad_norm": 1.0929116010665894,
      "learning_rate": 7.822812229889428e-06,
      "loss": 1.2673,
      "num_input_tokens_seen": 1235328,
      "step": 767
    },
    {
      "epoch": 0.13125961374124082,
      "grad_norm": 1.0824776887893677,
      "learning_rate": 7.759511406608255e-06,
      "loss": 0.9624,
      "num_input_tokens_seen": 1236736,
      "step": 768
    },
    {
      "epoch": 0.13143052469663305,
      "grad_norm": 1.1077066659927368,
      "learning_rate": 7.696420653239833e-06,
      "loss": 1.3457,
      "num_input_tokens_seen": 1238272,
      "step": 769
    },
    {
      "epoch": 0.13160143565202528,
      "grad_norm": 1.3062260150909424,
      "learning_rate": 7.633540738525066e-06,
      "loss": 1.2257,
      "num_input_tokens_seen": 1239680,
      "step": 770
    },
    {
      "epoch": 0.13177234660741755,
      "grad_norm": 1.2696670293807983,
      "learning_rate": 7.570872428635889e-06,
      "loss": 1.3599,
      "num_input_tokens_seen": 1241088,
      "step": 771
    },
    {
      "epoch": 0.13194325756280978,
      "grad_norm": 0.904512882232666,
      "learning_rate": 7.508416487165862e-06,
      "loss": 1.239,
      "num_input_tokens_seen": 1242496,
      "step": 772
    },
    {
      "epoch": 0.132114168518202,
      "grad_norm": 1.3860375881195068,
      "learning_rate": 7.4461736751209405e-06,
      "loss": 1.0176,
      "num_input_tokens_seen": 1243392,
      "step": 773
    },
    {
      "epoch": 0.13228507947359425,
      "grad_norm": 1.4313865900039673,
      "learning_rate": 7.384144750910133e-06,
      "loss": 1.6437,
      "num_input_tokens_seen": 1244928,
      "step": 774
    },
    {
      "epoch": 0.1324559904289865,
      "grad_norm": 1.3330035209655762,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 1.4067,
      "num_input_tokens_seen": 1246080,
      "step": 775
    },
    {
      "epoch": 0.13262690138437874,
      "grad_norm": 1.8910586833953857,
      "learning_rate": 7.260731586586983e-06,
      "loss": 1.2211,
      "num_input_tokens_seen": 1247360,
      "step": 776
    },
    {
      "epoch": 0.13279781233977098,
      "grad_norm": 1.1512120962142944,
      "learning_rate": 7.19934885022509e-06,
      "loss": 1.3444,
      "num_input_tokens_seen": 1248768,
      "step": 777
    },
    {
      "epoch": 0.1329687232951632,
      "grad_norm": 1.2045398950576782,
      "learning_rate": 7.138183009179922e-06,
      "loss": 1.4757,
      "num_input_tokens_seen": 1250816,
      "step": 778
    },
    {
      "epoch": 0.13313963425055547,
      "grad_norm": 1.6882028579711914,
      "learning_rate": 7.0772348087379315e-06,
      "loss": 1.276,
      "num_input_tokens_seen": 1252224,
      "step": 779
    },
    {
      "epoch": 0.1333105452059477,
      "grad_norm": 1.222859263420105,
      "learning_rate": 7.016504991533726e-06,
      "loss": 1.3786,
      "num_input_tokens_seen": 1254400,
      "step": 780
    },
    {
      "epoch": 0.13348145616133994,
      "grad_norm": 1.1429311037063599,
      "learning_rate": 6.9559942975409465e-06,
      "loss": 1.2956,
      "num_input_tokens_seen": 1256192,
      "step": 781
    },
    {
      "epoch": 0.13365236711673217,
      "grad_norm": 1.1389638185501099,
      "learning_rate": 6.895703464063319e-06,
      "loss": 0.9666,
      "num_input_tokens_seen": 1257472,
      "step": 782
    },
    {
      "epoch": 0.13382327807212444,
      "grad_norm": 2.3390161991119385,
      "learning_rate": 6.835633225725605e-06,
      "loss": 0.982,
      "num_input_tokens_seen": 1258624,
      "step": 783
    },
    {
      "epoch": 0.13399418902751667,
      "grad_norm": 0.8141054511070251,
      "learning_rate": 6.775784314464717e-06,
      "loss": 1.2014,
      "num_input_tokens_seen": 1260416,
      "step": 784
    },
    {
      "epoch": 0.1341650999829089,
      "grad_norm": 1.2340580224990845,
      "learning_rate": 6.716157459520739e-06,
      "loss": 1.4478,
      "num_input_tokens_seen": 1261696,
      "step": 785
    },
    {
      "epoch": 0.13433601093830114,
      "grad_norm": 1.523438572883606,
      "learning_rate": 6.656753387428089e-06,
      "loss": 1.1952,
      "num_input_tokens_seen": 1262976,
      "step": 786
    },
    {
      "epoch": 0.1345069218936934,
      "grad_norm": 1.8930535316467285,
      "learning_rate": 6.5975728220066425e-06,
      "loss": 1.2456,
      "num_input_tokens_seen": 1264000,
      "step": 787
    },
    {
      "epoch": 0.13467783284908563,
      "grad_norm": 1.0182385444641113,
      "learning_rate": 6.538616484352902e-06,
      "loss": 0.9753,
      "num_input_tokens_seen": 1265920,
      "step": 788
    },
    {
      "epoch": 0.13484874380447787,
      "grad_norm": 2.133293628692627,
      "learning_rate": 6.47988509283125e-06,
      "loss": 1.8824,
      "num_input_tokens_seen": 1267072,
      "step": 789
    },
    {
      "epoch": 0.1350196547598701,
      "grad_norm": 1.6731667518615723,
      "learning_rate": 6.421379363065142e-06,
      "loss": 1.0316,
      "num_input_tokens_seen": 1268352,
      "step": 790
    },
    {
      "epoch": 0.13519056571526236,
      "grad_norm": 0.8903709650039673,
      "learning_rate": 6.363100007928446e-06,
      "loss": 1.3327,
      "num_input_tokens_seen": 1270272,
      "step": 791
    },
    {
      "epoch": 0.1353614766706546,
      "grad_norm": 1.1502596139907837,
      "learning_rate": 6.305047737536707e-06,
      "loss": 1.4171,
      "num_input_tokens_seen": 1272704,
      "step": 792
    },
    {
      "epoch": 0.13553238762604683,
      "grad_norm": 1.3042970895767212,
      "learning_rate": 6.247223259238511e-06,
      "loss": 1.5807,
      "num_input_tokens_seen": 1274240,
      "step": 793
    },
    {
      "epoch": 0.13570329858143906,
      "grad_norm": 1.1854853630065918,
      "learning_rate": 6.189627277606894e-06,
      "loss": 1.472,
      "num_input_tokens_seen": 1276032,
      "step": 794
    },
    {
      "epoch": 0.1358742095368313,
      "grad_norm": 1.296064853668213,
      "learning_rate": 6.1322604944307e-06,
      "loss": 1.3922,
      "num_input_tokens_seen": 1277696,
      "step": 795
    },
    {
      "epoch": 0.13604512049222356,
      "grad_norm": 1.1411014795303345,
      "learning_rate": 6.075123608706093e-06,
      "loss": 1.0394,
      "num_input_tokens_seen": 1278976,
      "step": 796
    },
    {
      "epoch": 0.1362160314476158,
      "grad_norm": 1.2680301666259766,
      "learning_rate": 6.01821731662798e-06,
      "loss": 1.2944,
      "num_input_tokens_seen": 1280384,
      "step": 797
    },
    {
      "epoch": 0.13638694240300803,
      "grad_norm": 1.1402863264083862,
      "learning_rate": 5.961542311581586e-06,
      "loss": 1.3369,
      "num_input_tokens_seen": 1281664,
      "step": 798
    },
    {
      "epoch": 0.13655785335840026,
      "grad_norm": 0.7598361968994141,
      "learning_rate": 5.905099284133952e-06,
      "loss": 0.8824,
      "num_input_tokens_seen": 1285248,
      "step": 799
    },
    {
      "epoch": 0.13672876431379252,
      "grad_norm": 1.1887848377227783,
      "learning_rate": 5.848888922025553e-06,
      "loss": 1.9292,
      "num_input_tokens_seen": 1287168,
      "step": 800
    },
    {
      "epoch": 0.13689967526918476,
      "grad_norm": 1.5458849668502808,
      "learning_rate": 5.792911910161922e-06,
      "loss": 1.4892,
      "num_input_tokens_seen": 1288576,
      "step": 801
    },
    {
      "epoch": 0.137070586224577,
      "grad_norm": 1.892223596572876,
      "learning_rate": 5.737168930605272e-06,
      "loss": 1.1985,
      "num_input_tokens_seen": 1289984,
      "step": 802
    },
    {
      "epoch": 0.13724149717996922,
      "grad_norm": 1.0335140228271484,
      "learning_rate": 5.681660662566224e-06,
      "loss": 1.3118,
      "num_input_tokens_seen": 1291520,
      "step": 803
    },
    {
      "epoch": 0.13741240813536149,
      "grad_norm": 1.2237935066223145,
      "learning_rate": 5.626387782395512e-06,
      "loss": 1.2007,
      "num_input_tokens_seen": 1293056,
      "step": 804
    },
    {
      "epoch": 0.13758331909075372,
      "grad_norm": 1.299662470817566,
      "learning_rate": 5.571350963575728e-06,
      "loss": 1.532,
      "num_input_tokens_seen": 1295232,
      "step": 805
    },
    {
      "epoch": 0.13775423004614595,
      "grad_norm": 1.7739375829696655,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 0.9079,
      "num_input_tokens_seen": 1296384,
      "step": 806
    },
    {
      "epoch": 0.1379251410015382,
      "grad_norm": 1.0514365434646606,
      "learning_rate": 5.461988189529529e-06,
      "loss": 1.2751,
      "num_input_tokens_seen": 1297792,
      "step": 807
    },
    {
      "epoch": 0.13809605195693045,
      "grad_norm": 1.3919732570648193,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 1.2917,
      "num_input_tokens_seen": 1298944,
      "step": 808
    },
    {
      "epoch": 0.13826696291232268,
      "grad_norm": 2.012439250946045,
      "learning_rate": 5.3535776706149505e-06,
      "loss": 1.2307,
      "num_input_tokens_seen": 1300480,
      "step": 809
    },
    {
      "epoch": 0.13843787386771492,
      "grad_norm": 1.5188066959381104,
      "learning_rate": 5.299731159831953e-06,
      "loss": 1.2916,
      "num_input_tokens_seen": 1301760,
      "step": 810
    },
    {
      "epoch": 0.13860878482310715,
      "grad_norm": 1.3569542169570923,
      "learning_rate": 5.24612469060774e-06,
      "loss": 1.308,
      "num_input_tokens_seen": 1302784,
      "step": 811
    },
    {
      "epoch": 0.1387796957784994,
      "grad_norm": 1.1242380142211914,
      "learning_rate": 5.192758916120236e-06,
      "loss": 1.1265,
      "num_input_tokens_seen": 1304192,
      "step": 812
    },
    {
      "epoch": 0.13895060673389165,
      "grad_norm": 1.2460139989852905,
      "learning_rate": 5.139634486614544e-06,
      "loss": 1.077,
      "num_input_tokens_seen": 1305728,
      "step": 813
    },
    {
      "epoch": 0.13912151768928388,
      "grad_norm": 1.1909270286560059,
      "learning_rate": 5.086752049395094e-06,
      "loss": 1.5646,
      "num_input_tokens_seen": 1307520,
      "step": 814
    },
    {
      "epoch": 0.13929242864467611,
      "grad_norm": 1.091817021369934,
      "learning_rate": 5.034112248817685e-06,
      "loss": 1.4425,
      "num_input_tokens_seen": 1310720,
      "step": 815
    },
    {
      "epoch": 0.13946333960006838,
      "grad_norm": 1.052657127380371,
      "learning_rate": 4.981715726281666e-06,
      "loss": 1.2819,
      "num_input_tokens_seen": 1312000,
      "step": 816
    },
    {
      "epoch": 0.1396342505554606,
      "grad_norm": 1.048698902130127,
      "learning_rate": 4.929563120222141e-06,
      "loss": 0.9721,
      "num_input_tokens_seen": 1313280,
      "step": 817
    },
    {
      "epoch": 0.13980516151085284,
      "grad_norm": 1.2173616886138916,
      "learning_rate": 4.877655066102149e-06,
      "loss": 1.1474,
      "num_input_tokens_seen": 1314432,
      "step": 818
    },
    {
      "epoch": 0.13997607246624508,
      "grad_norm": 1.4542515277862549,
      "learning_rate": 4.825992196404957e-06,
      "loss": 1.4034,
      "num_input_tokens_seen": 1315712,
      "step": 819
    },
    {
      "epoch": 0.14014698342163734,
      "grad_norm": 1.1151410341262817,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 1.5459,
      "num_input_tokens_seen": 1317888,
      "step": 820
    },
    {
      "epoch": 0.14031789437702957,
      "grad_norm": 1.1893155574798584,
      "learning_rate": 4.723404525266839e-06,
      "loss": 1.3709,
      "num_input_tokens_seen": 1319040,
      "step": 821
    },
    {
      "epoch": 0.1404888053324218,
      "grad_norm": 0.8136382102966309,
      "learning_rate": 4.672480973824311e-06,
      "loss": 1.292,
      "num_input_tokens_seen": 1320704,
      "step": 822
    },
    {
      "epoch": 0.14065971628781404,
      "grad_norm": 1.0329256057739258,
      "learning_rate": 4.621805106786142e-06,
      "loss": 1.3544,
      "num_input_tokens_seen": 1322624,
      "step": 823
    },
    {
      "epoch": 0.1408306272432063,
      "grad_norm": 1.1935065984725952,
      "learning_rate": 4.571377541621788e-06,
      "loss": 1.1892,
      "num_input_tokens_seen": 1323776,
      "step": 824
    },
    {
      "epoch": 0.14100153819859854,
      "grad_norm": 2.2583439350128174,
      "learning_rate": 4.521198892775203e-06,
      "loss": 1.4353,
      "num_input_tokens_seen": 1325056,
      "step": 825
    },
    {
      "epoch": 0.14117244915399077,
      "grad_norm": 1.4760998487472534,
      "learning_rate": 4.4712697716574e-06,
      "loss": 1.0497,
      "num_input_tokens_seen": 1326336,
      "step": 826
    },
    {
      "epoch": 0.141343360109383,
      "grad_norm": 0.9099166393280029,
      "learning_rate": 4.421590786638951e-06,
      "loss": 1.2285,
      "num_input_tokens_seen": 1329408,
      "step": 827
    },
    {
      "epoch": 0.14151427106477527,
      "grad_norm": 1.2259130477905273,
      "learning_rate": 4.372162543042624e-06,
      "loss": 1.2944,
      "num_input_tokens_seen": 1330432,
      "step": 828
    },
    {
      "epoch": 0.1416851820201675,
      "grad_norm": 1.0529087781906128,
      "learning_rate": 4.322985643135952e-06,
      "loss": 0.8718,
      "num_input_tokens_seen": 1331968,
      "step": 829
    },
    {
      "epoch": 0.14185609297555973,
      "grad_norm": 0.9349254965782166,
      "learning_rate": 4.274060686123959e-06,
      "loss": 1.0243,
      "num_input_tokens_seen": 1334272,
      "step": 830
    },
    {
      "epoch": 0.14202700393095197,
      "grad_norm": 1.4527184963226318,
      "learning_rate": 4.225388268141797e-06,
      "loss": 1.0129,
      "num_input_tokens_seen": 1336064,
      "step": 831
    },
    {
      "epoch": 0.1421979148863442,
      "grad_norm": 1.406758189201355,
      "learning_rate": 4.176968982247514e-06,
      "loss": 1.3036,
      "num_input_tokens_seen": 1337472,
      "step": 832
    },
    {
      "epoch": 0.14236882584173646,
      "grad_norm": 1.185320258140564,
      "learning_rate": 4.128803418414839e-06,
      "loss": 1.2598,
      "num_input_tokens_seen": 1338880,
      "step": 833
    },
    {
      "epoch": 0.1425397367971287,
      "grad_norm": 1.1411311626434326,
      "learning_rate": 4.08089216352596e-06,
      "loss": 1.002,
      "num_input_tokens_seen": 1340544,
      "step": 834
    },
    {
      "epoch": 0.14271064775252093,
      "grad_norm": 1.1658159494400024,
      "learning_rate": 4.0332358013644016e-06,
      "loss": 1.1409,
      "num_input_tokens_seen": 1341696,
      "step": 835
    },
    {
      "epoch": 0.14288155870791316,
      "grad_norm": 1.3261125087738037,
      "learning_rate": 3.985834912607894e-06,
      "loss": 0.9423,
      "num_input_tokens_seen": 1342976,
      "step": 836
    },
    {
      "epoch": 0.14305246966330543,
      "grad_norm": 1.1009705066680908,
      "learning_rate": 3.938690074821313e-06,
      "loss": 1.2836,
      "num_input_tokens_seen": 1344512,
      "step": 837
    },
    {
      "epoch": 0.14322338061869766,
      "grad_norm": 1.4190592765808105,
      "learning_rate": 3.891801862449629e-06,
      "loss": 1.3118,
      "num_input_tokens_seen": 1345792,
      "step": 838
    },
    {
      "epoch": 0.1433942915740899,
      "grad_norm": 1.0961052179336548,
      "learning_rate": 3.845170846810902e-06,
      "loss": 1.3138,
      "num_input_tokens_seen": 1347072,
      "step": 839
    },
    {
      "epoch": 0.14356520252948213,
      "grad_norm": 1.1882303953170776,
      "learning_rate": 3.798797596089351e-06,
      "loss": 0.8736,
      "num_input_tokens_seen": 1348480,
      "step": 840
    },
    {
      "epoch": 0.1437361134848744,
      "grad_norm": 1.1609426736831665,
      "learning_rate": 3.752682675328406e-06,
      "loss": 1.4598,
      "num_input_tokens_seen": 1350272,
      "step": 841
    },
    {
      "epoch": 0.14390702444026662,
      "grad_norm": 1.265134334564209,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 0.99,
      "num_input_tokens_seen": 1351168,
      "step": 842
    },
    {
      "epoch": 0.14407793539565886,
      "grad_norm": 1.8745195865631104,
      "learning_rate": 3.661230068116811e-06,
      "loss": 1.2843,
      "num_input_tokens_seen": 1352192,
      "step": 843
    },
    {
      "epoch": 0.1442488463510511,
      "grad_norm": 0.9899400472640991,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 0.9237,
      "num_input_tokens_seen": 1355136,
      "step": 844
    },
    {
      "epoch": 0.14441975730644335,
      "grad_norm": 1.0325583219528198,
      "learning_rate": 3.5708174824471947e-06,
      "loss": 1.4218,
      "num_input_tokens_seen": 1357056,
      "step": 845
    },
    {
      "epoch": 0.1445906682618356,
      "grad_norm": 1.0770082473754883,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 1.2995,
      "num_input_tokens_seen": 1358848,
      "step": 846
    },
    {
      "epoch": 0.14476157921722782,
      "grad_norm": 1.5768237113952637,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 1.3013,
      "num_input_tokens_seen": 1360128,
      "step": 847
    },
    {
      "epoch": 0.14493249017262005,
      "grad_norm": 1.1183656454086304,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 0.8908,
      "num_input_tokens_seen": 1361792,
      "step": 848
    },
    {
      "epoch": 0.14510340112801232,
      "grad_norm": 1.3288859128952026,
      "learning_rate": 3.393129951157384e-06,
      "loss": 1.4369,
      "num_input_tokens_seen": 1362944,
      "step": 849
    },
    {
      "epoch": 0.14527431208340455,
      "grad_norm": 1.0003710985183716,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.8429,
      "num_input_tokens_seen": 1364608,
      "step": 850
    },
    {
      "epoch": 0.14544522303879678,
      "grad_norm": 1.083343505859375,
      "learning_rate": 3.305863665776793e-06,
      "loss": 1.3849,
      "num_input_tokens_seen": 1367168,
      "step": 851
    },
    {
      "epoch": 0.14561613399418902,
      "grad_norm": 0.9864150881767273,
      "learning_rate": 3.262626762369525e-06,
      "loss": 1.7759,
      "num_input_tokens_seen": 1369856,
      "step": 852
    },
    {
      "epoch": 0.14578704494958128,
      "grad_norm": 1.1446421146392822,
      "learning_rate": 3.219654721995266e-06,
      "loss": 1.264,
      "num_input_tokens_seen": 1371520,
      "step": 853
    },
    {
      "epoch": 0.1459579559049735,
      "grad_norm": 1.4345283508300781,
      "learning_rate": 3.176948068254762e-06,
      "loss": 1.3294,
      "num_input_tokens_seen": 1372544,
      "step": 854
    },
    {
      "epoch": 0.14612886686036575,
      "grad_norm": 1.154437780380249,
      "learning_rate": 3.1345073215151066e-06,
      "loss": 1.4207,
      "num_input_tokens_seen": 1373952,
      "step": 855
    },
    {
      "epoch": 0.14629977781575798,
      "grad_norm": 1.3557332754135132,
      "learning_rate": 3.092332998903416e-06,
      "loss": 1.2181,
      "num_input_tokens_seen": 1375232,
      "step": 856
    },
    {
      "epoch": 0.14647068877115024,
      "grad_norm": 0.9313439726829529,
      "learning_rate": 3.0504256143004866e-06,
      "loss": 1.2031,
      "num_input_tokens_seen": 1377536,
      "step": 857
    },
    {
      "epoch": 0.14664159972654248,
      "grad_norm": 1.2058906555175781,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 1.3705,
      "num_input_tokens_seen": 1379072,
      "step": 858
    },
    {
      "epoch": 0.1468125106819347,
      "grad_norm": 1.323063850402832,
      "learning_rate": 2.967413698375196e-06,
      "loss": 1.0907,
      "num_input_tokens_seen": 1380608,
      "step": 859
    },
    {
      "epoch": 0.14698342163732694,
      "grad_norm": 1.2123335599899292,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 1.0282,
      "num_input_tokens_seen": 1381888,
      "step": 860
    },
    {
      "epoch": 0.1471543325927192,
      "grad_norm": 1.574333906173706,
      "learning_rate": 2.8854756196229016e-06,
      "loss": 1.3135,
      "num_input_tokens_seen": 1383040,
      "step": 861
    },
    {
      "epoch": 0.14732524354811144,
      "grad_norm": 1.4668247699737549,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 0.9621,
      "num_input_tokens_seen": 1383936,
      "step": 862
    },
    {
      "epoch": 0.14749615450350367,
      "grad_norm": 1.260976791381836,
      "learning_rate": 2.8046153715899692e-06,
      "loss": 1.2175,
      "num_input_tokens_seen": 1385728,
      "step": 863
    },
    {
      "epoch": 0.1476670654588959,
      "grad_norm": 1.382636547088623,
      "learning_rate": 2.764590667717562e-06,
      "loss": 1.5316,
      "num_input_tokens_seen": 1387392,
      "step": 864
    },
    {
      "epoch": 0.14783797641428817,
      "grad_norm": 1.7550935745239258,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 1.4593,
      "num_input_tokens_seen": 1388544,
      "step": 865
    },
    {
      "epoch": 0.1480088873696804,
      "grad_norm": 1.2404913902282715,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 1.2874,
      "num_input_tokens_seen": 1390080,
      "step": 866
    },
    {
      "epoch": 0.14817979832507264,
      "grad_norm": 1.077967643737793,
      "learning_rate": 2.646144079015797e-06,
      "loss": 1.069,
      "num_input_tokens_seen": 1391872,
      "step": 867
    },
    {
      "epoch": 0.14835070928046487,
      "grad_norm": 1.4035272598266602,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 1.1787,
      "num_input_tokens_seen": 1393280,
      "step": 868
    },
    {
      "epoch": 0.1485216202358571,
      "grad_norm": 1.7466624975204468,
      "learning_rate": 2.5685407581417907e-06,
      "loss": 1.3251,
      "num_input_tokens_seen": 1394432,
      "step": 869
    },
    {
      "epoch": 0.14869253119124937,
      "grad_norm": 1.5174888372421265,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 1.1627,
      "num_input_tokens_seen": 1395840,
      "step": 870
    },
    {
      "epoch": 0.1488634421466416,
      "grad_norm": 1.1257425546646118,
      "learning_rate": 2.492030714945162e-06,
      "loss": 1.1311,
      "num_input_tokens_seen": 1397888,
      "step": 871
    },
    {
      "epoch": 0.14903435310203383,
      "grad_norm": 1.6867891550064087,
      "learning_rate": 2.454186839872158e-06,
      "loss": 1.0874,
      "num_input_tokens_seen": 1398912,
      "step": 872
    },
    {
      "epoch": 0.14920526405742607,
      "grad_norm": 1.5649261474609375,
      "learning_rate": 2.4166176784174795e-06,
      "loss": 1.3273,
      "num_input_tokens_seen": 1400064,
      "step": 873
    },
    {
      "epoch": 0.14937617501281833,
      "grad_norm": 1.4104456901550293,
      "learning_rate": 2.379323688349516e-06,
      "loss": 1.3599,
      "num_input_tokens_seen": 1401728,
      "step": 874
    },
    {
      "epoch": 0.14954708596821056,
      "grad_norm": 1.8338252305984497,
      "learning_rate": 2.3423053240837515e-06,
      "loss": 1.1395,
      "num_input_tokens_seen": 1403008,
      "step": 875
    },
    {
      "epoch": 0.1497179969236028,
      "grad_norm": 1.5206533670425415,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 1.3854,
      "num_input_tokens_seen": 1404032,
      "step": 876
    },
    {
      "epoch": 0.14988890787899503,
      "grad_norm": 1.3257313966751099,
      "learning_rate": 2.269097273823287e-06,
      "loss": 1.0362,
      "num_input_tokens_seen": 1405568,
      "step": 877
    },
    {
      "epoch": 0.1500598188343873,
      "grad_norm": 1.4665486812591553,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 1.2833,
      "num_input_tokens_seen": 1406848,
      "step": 878
    },
    {
      "epoch": 0.15023072978977953,
      "grad_norm": 1.1567167043685913,
      "learning_rate": 2.1969970956931762e-06,
      "loss": 1.0544,
      "num_input_tokens_seen": 1408768,
      "step": 879
    },
    {
      "epoch": 0.15040164074517176,
      "grad_norm": 1.1609593629837036,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 0.9072,
      "num_input_tokens_seen": 1410432,
      "step": 880
    },
    {
      "epoch": 0.150572551700564,
      "grad_norm": 1.5156000852584839,
      "learning_rate": 2.1260083037543817e-06,
      "loss": 1.0834,
      "num_input_tokens_seen": 1411712,
      "step": 881
    },
    {
      "epoch": 0.15074346265595626,
      "grad_norm": 2.019737482070923,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 1.3519,
      "num_input_tokens_seen": 1412736,
      "step": 882
    },
    {
      "epoch": 0.1509143736113485,
      "grad_norm": 1.4560739994049072,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 1.2125,
      "num_input_tokens_seen": 1413888,
      "step": 883
    },
    {
      "epoch": 0.15108528456674072,
      "grad_norm": 0.9209890365600586,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 1.5616,
      "num_input_tokens_seen": 1417472,
      "step": 884
    },
    {
      "epoch": 0.15125619552213296,
      "grad_norm": 0.973683774471283,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 1.2197,
      "num_input_tokens_seen": 1420928,
      "step": 885
    },
    {
      "epoch": 0.15142710647752522,
      "grad_norm": 1.1559979915618896,
      "learning_rate": 1.95342121028749e-06,
      "loss": 1.0949,
      "num_input_tokens_seen": 1422720,
      "step": 886
    },
    {
      "epoch": 0.15159801743291745,
      "grad_norm": 1.4323554039001465,
      "learning_rate": 1.9197445721754776e-06,
      "loss": 1.3591,
      "num_input_tokens_seen": 1424000,
      "step": 887
    },
    {
      "epoch": 0.1517689283883097,
      "grad_norm": 1.5616093873977661,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 1.1064,
      "num_input_tokens_seen": 1425664,
      "step": 888
    },
    {
      "epoch": 0.15193983934370192,
      "grad_norm": 0.9332357048988342,
      "learning_rate": 1.8532353797501318e-06,
      "loss": 1.2733,
      "num_input_tokens_seen": 1427712,
      "step": 889
    },
    {
      "epoch": 0.15211075029909418,
      "grad_norm": 1.4383611679077148,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 1.4546,
      "num_input_tokens_seen": 1429888,
      "step": 890
    },
    {
      "epoch": 0.15228166125448642,
      "grad_norm": 1.2602137327194214,
      "learning_rate": 1.787854327977162e-06,
      "loss": 1.3651,
      "num_input_tokens_seen": 1431296,
      "step": 891
    },
    {
      "epoch": 0.15245257220987865,
      "grad_norm": 2.1103150844573975,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 1.2868,
      "num_input_tokens_seen": 1432320,
      "step": 892
    },
    {
      "epoch": 0.15262348316527088,
      "grad_norm": 1.330872654914856,
      "learning_rate": 1.7236046034367958e-06,
      "loss": 1.6355,
      "num_input_tokens_seen": 1434368,
      "step": 893
    },
    {
      "epoch": 0.15279439412066315,
      "grad_norm": 1.5610040426254272,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 1.3348,
      "num_input_tokens_seen": 1435776,
      "step": 894
    },
    {
      "epoch": 0.15296530507605538,
      "grad_norm": 1.4304686784744263,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 1.2828,
      "num_input_tokens_seen": 1437824,
      "step": 895
    },
    {
      "epoch": 0.15313621603144761,
      "grad_norm": 1.1075396537780762,
      "learning_rate": 1.629358090099639e-06,
      "loss": 1.3993,
      "num_input_tokens_seen": 1439360,
      "step": 896
    },
    {
      "epoch": 0.15330712698683985,
      "grad_norm": 1.067120909690857,
      "learning_rate": 1.5985116065256684e-06,
      "loss": 1.0371,
      "num_input_tokens_seen": 1441536,
      "step": 897
    },
    {
      "epoch": 0.1534780379422321,
      "grad_norm": 1.154389500617981,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 1.407,
      "num_input_tokens_seen": 1443200,
      "step": 898
    },
    {
      "epoch": 0.15364894889762434,
      "grad_norm": 1.260058045387268,
      "learning_rate": 1.5376744310111019e-06,
      "loss": 0.8945,
      "num_input_tokens_seen": 1445248,
      "step": 899
    },
    {
      "epoch": 0.15381985985301658,
      "grad_norm": 1.6152087450027466,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 1.2643,
      "num_input_tokens_seen": 1446144,
      "step": 900
    },
    {
      "epoch": 0.1539907708084088,
      "grad_norm": 1.5999239683151245,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 1.2323,
      "num_input_tokens_seen": 1447424,
      "step": 901
    },
    {
      "epoch": 0.15416168176380107,
      "grad_norm": 1.1579269170761108,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 1.1999,
      "num_input_tokens_seen": 1449856,
      "step": 902
    },
    {
      "epoch": 0.1543325927191933,
      "grad_norm": 1.4241220951080322,
      "learning_rate": 1.4194335513099761e-06,
      "loss": 0.96,
      "num_input_tokens_seen": 1451136,
      "step": 903
    },
    {
      "epoch": 0.15450350367458554,
      "grad_norm": 1.506796956062317,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 1.5793,
      "num_input_tokens_seen": 1452416,
      "step": 904
    },
    {
      "epoch": 0.15467441462997777,
      "grad_norm": 0.962185263633728,
      "learning_rate": 1.362035610017079e-06,
      "loss": 1.4072,
      "num_input_tokens_seen": 1454720,
      "step": 905
    },
    {
      "epoch": 0.15484532558537,
      "grad_norm": 1.1414717435836792,
      "learning_rate": 1.333768497107593e-06,
      "loss": 1.2898,
      "num_input_tokens_seen": 1455872,
      "step": 906
    },
    {
      "epoch": 0.15501623654076227,
      "grad_norm": 1.1537418365478516,
      "learning_rate": 1.305789749760361e-06,
      "loss": 1.8085,
      "num_input_tokens_seen": 1457280,
      "step": 907
    },
    {
      "epoch": 0.1551871474961545,
      "grad_norm": 1.2882609367370605,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 1.4916,
      "num_input_tokens_seen": 1458944,
      "step": 908
    },
    {
      "epoch": 0.15535805845154674,
      "grad_norm": 1.0600218772888184,
      "learning_rate": 1.250698711883691e-06,
      "loss": 1.2027,
      "num_input_tokens_seen": 1460864,
      "step": 909
    },
    {
      "epoch": 0.15552896940693897,
      "grad_norm": 1.1306538581848145,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 1.1598,
      "num_input_tokens_seen": 1462144,
      "step": 910
    },
    {
      "epoch": 0.15569988036233123,
      "grad_norm": 2.1188416481018066,
      "learning_rate": 1.1967651814465354e-06,
      "loss": 1.006,
      "num_input_tokens_seen": 1463040,
      "step": 911
    },
    {
      "epoch": 0.15587079131772347,
      "grad_norm": 1.2094056606292725,
      "learning_rate": 1.170233305176327e-06,
      "loss": 1.4963,
      "num_input_tokens_seen": 1466240,
      "step": 912
    },
    {
      "epoch": 0.1560417022731157,
      "grad_norm": 1.3615549802780151,
      "learning_rate": 1.1439917870930793e-06,
      "loss": 1.2969,
      "num_input_tokens_seen": 1467392,
      "step": 913
    },
    {
      "epoch": 0.15621261322850793,
      "grad_norm": 1.3788602352142334,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 1.1526,
      "num_input_tokens_seen": 1469312,
      "step": 914
    },
    {
      "epoch": 0.1563835241839002,
      "grad_norm": 1.141424536705017,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 1.2743,
      "num_input_tokens_seen": 1471360,
      "step": 915
    },
    {
      "epoch": 0.15655443513929243,
      "grad_norm": 1.008150339126587,
      "learning_rate": 1.067012561698319e-06,
      "loss": 1.5091,
      "num_input_tokens_seen": 1473152,
      "step": 916
    },
    {
      "epoch": 0.15672534609468466,
      "grad_norm": 2.2813055515289307,
      "learning_rate": 1.0419356383716688e-06,
      "loss": 1.2373,
      "num_input_tokens_seen": 1474176,
      "step": 917
    },
    {
      "epoch": 0.1568962570500769,
      "grad_norm": 1.5979217290878296,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 1.1056,
      "num_input_tokens_seen": 1476224,
      "step": 918
    },
    {
      "epoch": 0.15706716800546916,
      "grad_norm": 1.7322977781295776,
      "learning_rate": 9.926578580764234e-07,
      "loss": 1.4204,
      "num_input_tokens_seen": 1477632,
      "step": 919
    },
    {
      "epoch": 0.1572380789608614,
      "grad_norm": 1.2666244506835938,
      "learning_rate": 9.684576015420278e-07,
      "loss": 1.3838,
      "num_input_tokens_seen": 1479296,
      "step": 920
    },
    {
      "epoch": 0.15740898991625363,
      "grad_norm": 1.452539086341858,
      "learning_rate": 9.445501617678654e-07,
      "loss": 1.1975,
      "num_input_tokens_seen": 1480448,
      "step": 921
    },
    {
      "epoch": 0.15757990087164586,
      "grad_norm": 1.0013576745986938,
      "learning_rate": 9.209358300585474e-07,
      "loss": 1.235,
      "num_input_tokens_seen": 1481984,
      "step": 922
    },
    {
      "epoch": 0.15775081182703812,
      "grad_norm": 1.3361754417419434,
      "learning_rate": 8.976148941472501e-07,
      "loss": 1.5232,
      "num_input_tokens_seen": 1483648,
      "step": 923
    },
    {
      "epoch": 0.15792172278243036,
      "grad_norm": 1.1421899795532227,
      "learning_rate": 8.745876381922147e-07,
      "loss": 1.2178,
      "num_input_tokens_seen": 1485184,
      "step": 924
    },
    {
      "epoch": 0.1580926337378226,
      "grad_norm": 1.2340621948242188,
      "learning_rate": 8.51854342773295e-07,
      "loss": 1.182,
      "num_input_tokens_seen": 1487104,
      "step": 925
    },
    {
      "epoch": 0.15826354469321482,
      "grad_norm": 1.4174124002456665,
      "learning_rate": 8.294152848885157e-07,
      "loss": 0.8548,
      "num_input_tokens_seen": 1489280,
      "step": 926
    },
    {
      "epoch": 0.1584344556486071,
      "grad_norm": 0.8303067088127136,
      "learning_rate": 8.072707379507216e-07,
      "loss": 1.2448,
      "num_input_tokens_seen": 1492352,
      "step": 927
    },
    {
      "epoch": 0.15860536660399932,
      "grad_norm": 1.1004095077514648,
      "learning_rate": 7.854209717842231e-07,
      "loss": 1.6061,
      "num_input_tokens_seen": 1494400,
      "step": 928
    },
    {
      "epoch": 0.15877627755939155,
      "grad_norm": 1.4981874227523804,
      "learning_rate": 7.638662526215284e-07,
      "loss": 1.4106,
      "num_input_tokens_seen": 1495808,
      "step": 929
    },
    {
      "epoch": 0.1589471885147838,
      "grad_norm": 1.1548348665237427,
      "learning_rate": 7.426068431000882e-07,
      "loss": 1.5781,
      "num_input_tokens_seen": 1497728,
      "step": 930
    },
    {
      "epoch": 0.15911809947017605,
      "grad_norm": 1.2835031747817993,
      "learning_rate": 7.216430022591008e-07,
      "loss": 1.512,
      "num_input_tokens_seen": 1499264,
      "step": 931
    },
    {
      "epoch": 0.15928901042556828,
      "grad_norm": 1.5727182626724243,
      "learning_rate": 7.009749855363456e-07,
      "loss": 1.2915,
      "num_input_tokens_seen": 1500672,
      "step": 932
    },
    {
      "epoch": 0.15945992138096052,
      "grad_norm": 1.3856523036956787,
      "learning_rate": 6.806030447650879e-07,
      "loss": 1.2246,
      "num_input_tokens_seen": 1501824,
      "step": 933
    },
    {
      "epoch": 0.15963083233635275,
      "grad_norm": 1.1907247304916382,
      "learning_rate": 6.605274281709928e-07,
      "loss": 1.1058,
      "num_input_tokens_seen": 1505408,
      "step": 934
    },
    {
      "epoch": 0.159801743291745,
      "grad_norm": 1.0002636909484863,
      "learning_rate": 6.407483803691216e-07,
      "loss": 1.0798,
      "num_input_tokens_seen": 1507456,
      "step": 935
    },
    {
      "epoch": 0.15997265424713725,
      "grad_norm": 1.2332655191421509,
      "learning_rate": 6.212661423609184e-07,
      "loss": 1.7517,
      "num_input_tokens_seen": 1509120,
      "step": 936
    },
    {
      "epoch": 0.16014356520252948,
      "grad_norm": 0.8006249070167542,
      "learning_rate": 6.020809515313142e-07,
      "loss": 1.334,
      "num_input_tokens_seen": 1511936,
      "step": 937
    },
    {
      "epoch": 0.16031447615792171,
      "grad_norm": 1.5673917531967163,
      "learning_rate": 5.83193041645802e-07,
      "loss": 1.2044,
      "num_input_tokens_seen": 1513472,
      "step": 938
    },
    {
      "epoch": 0.16048538711331398,
      "grad_norm": 0.9158459305763245,
      "learning_rate": 5.646026428476031e-07,
      "loss": 1.2454,
      "num_input_tokens_seen": 1516032,
      "step": 939
    },
    {
      "epoch": 0.1606562980687062,
      "grad_norm": 1.1032164096832275,
      "learning_rate": 5.463099816548579e-07,
      "loss": 1.3572,
      "num_input_tokens_seen": 1517440,
      "step": 940
    },
    {
      "epoch": 0.16082720902409844,
      "grad_norm": 1.2415801286697388,
      "learning_rate": 5.283152809578751e-07,
      "loss": 1.2867,
      "num_input_tokens_seen": 1519104,
      "step": 941
    },
    {
      "epoch": 0.16099811997949068,
      "grad_norm": 1.3244779109954834,
      "learning_rate": 5.106187600163987e-07,
      "loss": 1.6282,
      "num_input_tokens_seen": 1521536,
      "step": 942
    },
    {
      "epoch": 0.16116903093488294,
      "grad_norm": 1.2828481197357178,
      "learning_rate": 4.932206344569562e-07,
      "loss": 1.2269,
      "num_input_tokens_seen": 1523200,
      "step": 943
    },
    {
      "epoch": 0.16133994189027517,
      "grad_norm": 1.6713334321975708,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 1.0685,
      "num_input_tokens_seen": 1524224,
      "step": 944
    },
    {
      "epoch": 0.1615108528456674,
      "grad_norm": 1.537893533706665,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 1.4561,
      "num_input_tokens_seen": 1525760,
      "step": 945
    },
    {
      "epoch": 0.16168176380105964,
      "grad_norm": 1.3506205081939697,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 1.5339,
      "num_input_tokens_seen": 1527168,
      "step": 946
    },
    {
      "epoch": 0.16185267475645188,
      "grad_norm": 1.3097453117370605,
      "learning_rate": 4.26616271261146e-07,
      "loss": 1.3408,
      "num_input_tokens_seen": 1528704,
      "step": 947
    },
    {
      "epoch": 0.16202358571184414,
      "grad_norm": 1.7368944883346558,
      "learning_rate": 4.107132296653549e-07,
      "loss": 1.4181,
      "num_input_tokens_seen": 1529984,
      "step": 948
    },
    {
      "epoch": 0.16219449666723637,
      "grad_norm": 1.619661569595337,
      "learning_rate": 3.95109800768953e-07,
      "loss": 1.0761,
      "num_input_tokens_seen": 1530880,
      "step": 949
    },
    {
      "epoch": 0.1623654076226286,
      "grad_norm": 0.9828883409500122,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 1.486,
      "num_input_tokens_seen": 1532288,
      "step": 950
    },
    {
      "epoch": 0.16253631857802084,
      "grad_norm": 1.1927214860916138,
      "learning_rate": 3.6480253791274786e-07,
      "loss": 1.6793,
      "num_input_tokens_seen": 1533568,
      "step": 951
    },
    {
      "epoch": 0.1627072295334131,
      "grad_norm": 1.3982822895050049,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 1.5878,
      "num_input_tokens_seen": 1535488,
      "step": 952
    },
    {
      "epoch": 0.16287814048880533,
      "grad_norm": 1.1162917613983154,
      "learning_rate": 3.3569595982576583e-07,
      "loss": 1.2299,
      "num_input_tokens_seen": 1536896,
      "step": 953
    },
    {
      "epoch": 0.16304905144419757,
      "grad_norm": 1.6629667282104492,
      "learning_rate": 3.215933731753024e-07,
      "loss": 1.5065,
      "num_input_tokens_seen": 1538176,
      "step": 954
    },
    {
      "epoch": 0.1632199623995898,
      "grad_norm": 1.96835458278656,
      "learning_rate": 3.077914851215585e-07,
      "loss": 1.1598,
      "num_input_tokens_seen": 1539456,
      "step": 955
    },
    {
      "epoch": 0.16339087335498206,
      "grad_norm": 1.1779505014419556,
      "learning_rate": 2.942904638361804e-07,
      "loss": 1.5969,
      "num_input_tokens_seen": 1541120,
      "step": 956
    },
    {
      "epoch": 0.1635617843103743,
      "grad_norm": 1.5550580024719238,
      "learning_rate": 2.810904738248549e-07,
      "loss": 1.2684,
      "num_input_tokens_seen": 1542400,
      "step": 957
    },
    {
      "epoch": 0.16373269526576653,
      "grad_norm": 1.3353073596954346,
      "learning_rate": 2.681916759252917e-07,
      "loss": 1.104,
      "num_input_tokens_seen": 1543808,
      "step": 958
    },
    {
      "epoch": 0.16390360622115877,
      "grad_norm": 1.5523662567138672,
      "learning_rate": 2.555942273052753e-07,
      "loss": 1.9073,
      "num_input_tokens_seen": 1545088,
      "step": 959
    },
    {
      "epoch": 0.16407451717655103,
      "grad_norm": 1.5049338340759277,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 1.265,
      "num_input_tokens_seen": 1546112,
      "step": 960
    },
    {
      "epoch": 0.16424542813194326,
      "grad_norm": 1.1157705783843994,
      "learning_rate": 2.3130398821391007e-07,
      "loss": 1.0975,
      "num_input_tokens_seen": 1548160,
      "step": 961
    },
    {
      "epoch": 0.1644163390873355,
      "grad_norm": 1.1394641399383545,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 1.2424,
      "num_input_tokens_seen": 1549824,
      "step": 962
    },
    {
      "epoch": 0.16458725004272773,
      "grad_norm": 1.2125030755996704,
      "learning_rate": 2.0822094042274032e-07,
      "loss": 1.0891,
      "num_input_tokens_seen": 1551360,
      "step": 963
    },
    {
      "epoch": 0.16475816099812,
      "grad_norm": 1.270798683166504,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 1.0462,
      "num_input_tokens_seen": 1552512,
      "step": 964
    },
    {
      "epoch": 0.16492907195351222,
      "grad_norm": 1.1480530500411987,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 1.5325,
      "num_input_tokens_seen": 1554560,
      "step": 965
    },
    {
      "epoch": 0.16509998290890446,
      "grad_norm": 1.7388248443603516,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 1.2945,
      "num_input_tokens_seen": 1555584,
      "step": 966
    },
    {
      "epoch": 0.1652708938642967,
      "grad_norm": 1.5328916311264038,
      "learning_rate": 1.6568085999008888e-07,
      "loss": 1.216,
      "num_input_tokens_seen": 1556992,
      "step": 967
    },
    {
      "epoch": 0.16544180481968895,
      "grad_norm": 1.3016353845596313,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 1.4656,
      "num_input_tokens_seen": 1558400,
      "step": 968
    },
    {
      "epoch": 0.1656127157750812,
      "grad_norm": 0.8944463133811951,
      "learning_rate": 1.4622590069211516e-07,
      "loss": 1.8059,
      "num_input_tokens_seen": 1561984,
      "step": 969
    },
    {
      "epoch": 0.16578362673047342,
      "grad_norm": 1.6411018371582031,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 1.6339,
      "num_input_tokens_seen": 1563136,
      "step": 970
    },
    {
      "epoch": 0.16595453768586566,
      "grad_norm": 1.7390810251235962,
      "learning_rate": 1.2798227928029482e-07,
      "loss": 1.6679,
      "num_input_tokens_seen": 1564416,
      "step": 971
    },
    {
      "epoch": 0.16612544864125792,
      "grad_norm": 1.213734745979309,
      "learning_rate": 1.193150004542204e-07,
      "loss": 1.244,
      "num_input_tokens_seen": 1566336,
      "step": 972
    },
    {
      "epoch": 0.16629635959665015,
      "grad_norm": 1.4376020431518555,
      "learning_rate": 1.109508849230001e-07,
      "loss": 1.2677,
      "num_input_tokens_seen": 1567744,
      "step": 973
    },
    {
      "epoch": 0.16646727055204238,
      "grad_norm": 1.146178960800171,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 1.2814,
      "num_input_tokens_seen": 1568896,
      "step": 974
    },
    {
      "epoch": 0.16663818150743462,
      "grad_norm": 1.6299996376037598,
      "learning_rate": 9.513254770636137e-08,
      "loss": 1.2745,
      "num_input_tokens_seen": 1569920,
      "step": 975
    },
    {
      "epoch": 0.16680909246282688,
      "grad_norm": 1.0718154907226562,
      "learning_rate": 8.767851876239074e-08,
      "loss": 1.2548,
      "num_input_tokens_seen": 1571712,
      "step": 976
    },
    {
      "epoch": 0.16698000341821911,
      "grad_norm": 1.2630144357681274,
      "learning_rate": 8.052803859382174e-08,
      "loss": 1.3598,
      "num_input_tokens_seen": 1573120,
      "step": 977
    },
    {
      "epoch": 0.16715091437361135,
      "grad_norm": 1.6164922714233398,
      "learning_rate": 7.368119432699383e-08,
      "loss": 1.3245,
      "num_input_tokens_seen": 1575040,
      "step": 978
    },
    {
      "epoch": 0.16732182532900358,
      "grad_norm": 1.3498270511627197,
      "learning_rate": 6.71380693885476e-08,
      "loss": 1.398,
      "num_input_tokens_seen": 1576064,
      "step": 979
    },
    {
      "epoch": 0.16749273628439584,
      "grad_norm": 1.0805009603500366,
      "learning_rate": 6.089874350439506e-08,
      "loss": 1.3146,
      "num_input_tokens_seen": 1578112,
      "step": 980
    },
    {
      "epoch": 0.16766364723978808,
      "grad_norm": 1.5432918071746826,
      "learning_rate": 5.496329269875089e-08,
      "loss": 1.3696,
      "num_input_tokens_seen": 1579648,
      "step": 981
    },
    {
      "epoch": 0.1678345581951803,
      "grad_norm": 1.3887919187545776,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 1.0438,
      "num_input_tokens_seen": 1580672,
      "step": 982
    },
    {
      "epoch": 0.16800546915057255,
      "grad_norm": 1.2019280195236206,
      "learning_rate": 4.400430190586724e-08,
      "loss": 1.3362,
      "num_input_tokens_seen": 1584512,
      "step": 983
    },
    {
      "epoch": 0.16817638010596478,
      "grad_norm": 1.1166672706604004,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 1.4486,
      "num_input_tokens_seen": 1586048,
      "step": 984
    },
    {
      "epoch": 0.16834729106135704,
      "grad_norm": 1.2349090576171875,
      "learning_rate": 3.426163113565417e-08,
      "loss": 1.2894,
      "num_input_tokens_seen": 1587456,
      "step": 985
    },
    {
      "epoch": 0.16851820201674927,
      "grad_norm": 1.0185251235961914,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 1.3152,
      "num_input_tokens_seen": 1589888,
      "step": 986
    },
    {
      "epoch": 0.1686891129721415,
      "grad_norm": 1.5424748659133911,
      "learning_rate": 2.5735755232134118e-08,
      "loss": 1.3883,
      "num_input_tokens_seen": 1591040,
      "step": 987
    },
    {
      "epoch": 0.16886002392753374,
      "grad_norm": 1.2455328702926636,
      "learning_rate": 2.192924752854042e-08,
      "loss": 1.2044,
      "num_input_tokens_seen": 1592704,
      "step": 988
    },
    {
      "epoch": 0.169030934882926,
      "grad_norm": 1.498356819152832,
      "learning_rate": 1.842708973447127e-08,
      "loss": 1.5381,
      "num_input_tokens_seen": 1593728,
      "step": 989
    },
    {
      "epoch": 0.16920184583831824,
      "grad_norm": 1.1277154684066772,
      "learning_rate": 1.522932452260595e-08,
      "loss": 1.177,
      "num_input_tokens_seen": 1595008,
      "step": 990
    },
    {
      "epoch": 0.16937275679371047,
      "grad_norm": 1.4371259212493896,
      "learning_rate": 1.233599085671e-08,
      "loss": 1.5395,
      "num_input_tokens_seen": 1596544,
      "step": 991
    },
    {
      "epoch": 0.1695436677491027,
      "grad_norm": 1.2638682126998901,
      "learning_rate": 9.747123991141194e-09,
      "loss": 1.6281,
      "num_input_tokens_seen": 1598848,
      "step": 992
    },
    {
      "epoch": 0.16971457870449497,
      "grad_norm": 1.4004957675933838,
      "learning_rate": 7.462755470422078e-09,
      "loss": 1.1513,
      "num_input_tokens_seen": 1600640,
      "step": 993
    },
    {
      "epoch": 0.1698854896598872,
      "grad_norm": 0.9166178107261658,
      "learning_rate": 5.48291312886251e-09,
      "loss": 1.4956,
      "num_input_tokens_seen": 1603072,
      "step": 994
    },
    {
      "epoch": 0.17005640061527943,
      "grad_norm": 0.9018672704696655,
      "learning_rate": 3.807621090218261e-09,
      "loss": 0.6941,
      "num_input_tokens_seen": 1604992,
      "step": 995
    },
    {
      "epoch": 0.17022731157067167,
      "grad_norm": 1.235129714012146,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 1.2224,
      "num_input_tokens_seen": 1606912,
      "step": 996
    },
    {
      "epoch": 0.17039822252606393,
      "grad_norm": 1.3711905479431152,
      "learning_rate": 1.3707658621964215e-09,
      "loss": 1.8584,
      "num_input_tokens_seen": 1608448,
      "step": 997
    },
    {
      "epoch": 0.17056913348145616,
      "grad_norm": 1.5582702159881592,
      "learning_rate": 6.092323651313292e-10,
      "loss": 1.1195,
      "num_input_tokens_seen": 1609728,
      "step": 998
    },
    {
      "epoch": 0.1707400444368484,
      "grad_norm": 0.9311224818229675,
      "learning_rate": 1.5230855524017708e-10,
      "loss": 1.2119,
      "num_input_tokens_seen": 1611264,
      "step": 999
    },
    {
      "epoch": 0.17091095539224063,
      "grad_norm": 1.306238055229187,
      "learning_rate": 0.0,
      "loss": 1.5051,
      "num_input_tokens_seen": 1613184,
      "step": 1000
    },
    {
      "epoch": 0.17091095539224063,
      "eval_loss": 1.2649707794189453,
      "eval_runtime": 94.6847,
      "eval_samples_per_second": 54.93,
      "eval_steps_per_second": 6.875,
      "num_input_tokens_seen": 1613184,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 1613184,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.267393155996058e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
