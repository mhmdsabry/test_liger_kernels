{
  "best_metric": 0.4565405547618866,
  "best_model_checkpoint": "exp/liger_test_llama_1k_learning_steps_8_bs_4096_seqlen/liger_tau_commonsense_qa/checkpoint-1000",
  "epoch": 0.8210180623973727,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008210180623973727,
      "grad_norm": 10.750598907470703,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.0816,
      "num_input_tokens_seen": 768,
      "step": 1
    },
    {
      "epoch": 0.0016420361247947454,
      "grad_norm": 9.2435884475708,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.7905,
      "num_input_tokens_seen": 1536,
      "step": 2
    },
    {
      "epoch": 0.0024630541871921183,
      "grad_norm": 8.207642555236816,
      "learning_rate": 1.5e-06,
      "loss": 1.779,
      "num_input_tokens_seen": 2304,
      "step": 3
    },
    {
      "epoch": 0.003284072249589491,
      "grad_norm": 5.70974588394165,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7822,
      "num_input_tokens_seen": 3200,
      "step": 4
    },
    {
      "epoch": 0.004105090311986864,
      "grad_norm": 5.426308631896973,
      "learning_rate": 2.5e-06,
      "loss": 1.5269,
      "num_input_tokens_seen": 3968,
      "step": 5
    },
    {
      "epoch": 0.0049261083743842365,
      "grad_norm": 9.175739288330078,
      "learning_rate": 3e-06,
      "loss": 1.3909,
      "num_input_tokens_seen": 4864,
      "step": 6
    },
    {
      "epoch": 0.005747126436781609,
      "grad_norm": 8.30392074584961,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.6412,
      "num_input_tokens_seen": 5632,
      "step": 7
    },
    {
      "epoch": 0.006568144499178982,
      "grad_norm": 7.168707370758057,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.9154,
      "num_input_tokens_seen": 6528,
      "step": 8
    },
    {
      "epoch": 0.007389162561576354,
      "grad_norm": 10.750738143920898,
      "learning_rate": 4.5e-06,
      "loss": 1.014,
      "num_input_tokens_seen": 7552,
      "step": 9
    },
    {
      "epoch": 0.008210180623973728,
      "grad_norm": 7.0806427001953125,
      "learning_rate": 5e-06,
      "loss": 1.1987,
      "num_input_tokens_seen": 8320,
      "step": 10
    },
    {
      "epoch": 0.0090311986863711,
      "grad_norm": 9.545406341552734,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.3724,
      "num_input_tokens_seen": 9344,
      "step": 11
    },
    {
      "epoch": 0.009852216748768473,
      "grad_norm": 8.930278778076172,
      "learning_rate": 6e-06,
      "loss": 2.0888,
      "num_input_tokens_seen": 10112,
      "step": 12
    },
    {
      "epoch": 0.010673234811165846,
      "grad_norm": 7.296545028686523,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.2586,
      "num_input_tokens_seen": 10880,
      "step": 13
    },
    {
      "epoch": 0.011494252873563218,
      "grad_norm": 7.860019683837891,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2286,
      "num_input_tokens_seen": 11648,
      "step": 14
    },
    {
      "epoch": 0.012315270935960592,
      "grad_norm": 6.020633697509766,
      "learning_rate": 7.5e-06,
      "loss": 1.4025,
      "num_input_tokens_seen": 12416,
      "step": 15
    },
    {
      "epoch": 0.013136288998357963,
      "grad_norm": 8.231178283691406,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.3669,
      "num_input_tokens_seen": 13184,
      "step": 16
    },
    {
      "epoch": 0.013957307060755337,
      "grad_norm": 6.909144401550293,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.9397,
      "num_input_tokens_seen": 14080,
      "step": 17
    },
    {
      "epoch": 0.014778325123152709,
      "grad_norm": 7.214262008666992,
      "learning_rate": 9e-06,
      "loss": 0.8677,
      "num_input_tokens_seen": 14848,
      "step": 18
    },
    {
      "epoch": 0.015599343185550082,
      "grad_norm": 7.615847587585449,
      "learning_rate": 9.5e-06,
      "loss": 1.4291,
      "num_input_tokens_seen": 15616,
      "step": 19
    },
    {
      "epoch": 0.016420361247947456,
      "grad_norm": 10.660218238830566,
      "learning_rate": 1e-05,
      "loss": 2.4134,
      "num_input_tokens_seen": 16512,
      "step": 20
    },
    {
      "epoch": 0.017241379310344827,
      "grad_norm": 9.430850982666016,
      "learning_rate": 1.05e-05,
      "loss": 2.4519,
      "num_input_tokens_seen": 17280,
      "step": 21
    },
    {
      "epoch": 0.0180623973727422,
      "grad_norm": 10.296548843383789,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.8349,
      "num_input_tokens_seen": 18048,
      "step": 22
    },
    {
      "epoch": 0.018883415435139574,
      "grad_norm": 9.492511749267578,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.9934,
      "num_input_tokens_seen": 18816,
      "step": 23
    },
    {
      "epoch": 0.019704433497536946,
      "grad_norm": 4.974617004394531,
      "learning_rate": 1.2e-05,
      "loss": 0.4989,
      "num_input_tokens_seen": 19584,
      "step": 24
    },
    {
      "epoch": 0.020525451559934318,
      "grad_norm": 7.59859037399292,
      "learning_rate": 1.25e-05,
      "loss": 0.8806,
      "num_input_tokens_seen": 20352,
      "step": 25
    },
    {
      "epoch": 0.021346469622331693,
      "grad_norm": 9.929797172546387,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.9213,
      "num_input_tokens_seen": 21248,
      "step": 26
    },
    {
      "epoch": 0.022167487684729065,
      "grad_norm": 6.639834880828857,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 1.1123,
      "num_input_tokens_seen": 22016,
      "step": 27
    },
    {
      "epoch": 0.022988505747126436,
      "grad_norm": 5.6020050048828125,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.4561,
      "num_input_tokens_seen": 22784,
      "step": 28
    },
    {
      "epoch": 0.023809523809523808,
      "grad_norm": 7.07780122756958,
      "learning_rate": 1.45e-05,
      "loss": 1.1629,
      "num_input_tokens_seen": 23680,
      "step": 29
    },
    {
      "epoch": 0.024630541871921183,
      "grad_norm": 8.791866302490234,
      "learning_rate": 1.5e-05,
      "loss": 1.625,
      "num_input_tokens_seen": 24448,
      "step": 30
    },
    {
      "epoch": 0.025451559934318555,
      "grad_norm": 12.756072044372559,
      "learning_rate": 1.55e-05,
      "loss": 1.5246,
      "num_input_tokens_seen": 25216,
      "step": 31
    },
    {
      "epoch": 0.026272577996715927,
      "grad_norm": 12.723733901977539,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.8168,
      "num_input_tokens_seen": 25984,
      "step": 32
    },
    {
      "epoch": 0.027093596059113302,
      "grad_norm": 8.787097930908203,
      "learning_rate": 1.65e-05,
      "loss": 1.8195,
      "num_input_tokens_seen": 26752,
      "step": 33
    },
    {
      "epoch": 0.027914614121510674,
      "grad_norm": 13.234707832336426,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.3857,
      "num_input_tokens_seen": 27776,
      "step": 34
    },
    {
      "epoch": 0.028735632183908046,
      "grad_norm": 6.273129463195801,
      "learning_rate": 1.75e-05,
      "loss": 1.002,
      "num_input_tokens_seen": 28544,
      "step": 35
    },
    {
      "epoch": 0.029556650246305417,
      "grad_norm": 6.074653148651123,
      "learning_rate": 1.8e-05,
      "loss": 0.6859,
      "num_input_tokens_seen": 29312,
      "step": 36
    },
    {
      "epoch": 0.030377668308702793,
      "grad_norm": 12.164575576782227,
      "learning_rate": 1.85e-05,
      "loss": 1.4439,
      "num_input_tokens_seen": 30080,
      "step": 37
    },
    {
      "epoch": 0.031198686371100164,
      "grad_norm": 7.945065021514893,
      "learning_rate": 1.9e-05,
      "loss": 1.7304,
      "num_input_tokens_seen": 30848,
      "step": 38
    },
    {
      "epoch": 0.03201970443349754,
      "grad_norm": 4.238689422607422,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.3603,
      "num_input_tokens_seen": 31744,
      "step": 39
    },
    {
      "epoch": 0.03284072249589491,
      "grad_norm": 7.421785354614258,
      "learning_rate": 2e-05,
      "loss": 1.2158,
      "num_input_tokens_seen": 32768,
      "step": 40
    },
    {
      "epoch": 0.03366174055829228,
      "grad_norm": 6.8422346115112305,
      "learning_rate": 2.05e-05,
      "loss": 0.4359,
      "num_input_tokens_seen": 33536,
      "step": 41
    },
    {
      "epoch": 0.034482758620689655,
      "grad_norm": 8.012866973876953,
      "learning_rate": 2.1e-05,
      "loss": 0.7968,
      "num_input_tokens_seen": 34304,
      "step": 42
    },
    {
      "epoch": 0.035303776683087026,
      "grad_norm": 5.980032444000244,
      "learning_rate": 2.15e-05,
      "loss": 0.8343,
      "num_input_tokens_seen": 35200,
      "step": 43
    },
    {
      "epoch": 0.0361247947454844,
      "grad_norm": 5.017906665802002,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4861,
      "num_input_tokens_seen": 35968,
      "step": 44
    },
    {
      "epoch": 0.03694581280788178,
      "grad_norm": 1.691826581954956,
      "learning_rate": 2.25e-05,
      "loss": 0.1013,
      "num_input_tokens_seen": 36736,
      "step": 45
    },
    {
      "epoch": 0.03776683087027915,
      "grad_norm": 8.307939529418945,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.8879,
      "num_input_tokens_seen": 37504,
      "step": 46
    },
    {
      "epoch": 0.03858784893267652,
      "grad_norm": 1.835070252418518,
      "learning_rate": 2.35e-05,
      "loss": 0.1041,
      "num_input_tokens_seen": 38272,
      "step": 47
    },
    {
      "epoch": 0.03940886699507389,
      "grad_norm": 4.903926372528076,
      "learning_rate": 2.4e-05,
      "loss": 0.3521,
      "num_input_tokens_seen": 39040,
      "step": 48
    },
    {
      "epoch": 0.040229885057471264,
      "grad_norm": 5.068355560302734,
      "learning_rate": 2.45e-05,
      "loss": 0.5937,
      "num_input_tokens_seen": 39808,
      "step": 49
    },
    {
      "epoch": 0.041050903119868636,
      "grad_norm": 4.362441539764404,
      "learning_rate": 2.5e-05,
      "loss": 0.6235,
      "num_input_tokens_seen": 40704,
      "step": 50
    },
    {
      "epoch": 0.04187192118226601,
      "grad_norm": 8.58091926574707,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.3271,
      "num_input_tokens_seen": 41472,
      "step": 51
    },
    {
      "epoch": 0.042692939244663386,
      "grad_norm": 5.328819274902344,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.5185,
      "num_input_tokens_seen": 42240,
      "step": 52
    },
    {
      "epoch": 0.04351395730706076,
      "grad_norm": 5.777352333068848,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.6244,
      "num_input_tokens_seen": 43008,
      "step": 53
    },
    {
      "epoch": 0.04433497536945813,
      "grad_norm": 12.560769081115723,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.8578,
      "num_input_tokens_seen": 43776,
      "step": 54
    },
    {
      "epoch": 0.0451559934318555,
      "grad_norm": 13.23491382598877,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.5071,
      "num_input_tokens_seen": 44544,
      "step": 55
    },
    {
      "epoch": 0.04597701149425287,
      "grad_norm": 6.492510795593262,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.8179,
      "num_input_tokens_seen": 45312,
      "step": 56
    },
    {
      "epoch": 0.046798029556650245,
      "grad_norm": 8.601272583007812,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.6978,
      "num_input_tokens_seen": 46080,
      "step": 57
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.4563087224960327,
      "learning_rate": 2.9e-05,
      "loss": 0.0668,
      "num_input_tokens_seen": 46848,
      "step": 58
    },
    {
      "epoch": 0.048440065681444995,
      "grad_norm": 6.777072906494141,
      "learning_rate": 2.95e-05,
      "loss": 0.6687,
      "num_input_tokens_seen": 47616,
      "step": 59
    },
    {
      "epoch": 0.04926108374384237,
      "grad_norm": 9.753912925720215,
      "learning_rate": 3e-05,
      "loss": 1.3637,
      "num_input_tokens_seen": 48512,
      "step": 60
    },
    {
      "epoch": 0.05008210180623974,
      "grad_norm": 11.317591667175293,
      "learning_rate": 3.05e-05,
      "loss": 1.0756,
      "num_input_tokens_seen": 49280,
      "step": 61
    },
    {
      "epoch": 0.05090311986863711,
      "grad_norm": 0.9776474833488464,
      "learning_rate": 3.1e-05,
      "loss": 0.0404,
      "num_input_tokens_seen": 50048,
      "step": 62
    },
    {
      "epoch": 0.05172413793103448,
      "grad_norm": 6.6228485107421875,
      "learning_rate": 3.15e-05,
      "loss": 1.1192,
      "num_input_tokens_seen": 50816,
      "step": 63
    },
    {
      "epoch": 0.052545155993431854,
      "grad_norm": 8.245331764221191,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4232,
      "num_input_tokens_seen": 51840,
      "step": 64
    },
    {
      "epoch": 0.053366174055829226,
      "grad_norm": 6.265161037445068,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.2888,
      "num_input_tokens_seen": 52608,
      "step": 65
    },
    {
      "epoch": 0.054187192118226604,
      "grad_norm": 7.404114723205566,
      "learning_rate": 3.3e-05,
      "loss": 0.4482,
      "num_input_tokens_seen": 53376,
      "step": 66
    },
    {
      "epoch": 0.055008210180623976,
      "grad_norm": 6.610565662384033,
      "learning_rate": 3.35e-05,
      "loss": 0.5413,
      "num_input_tokens_seen": 54144,
      "step": 67
    },
    {
      "epoch": 0.05582922824302135,
      "grad_norm": 10.021830558776855,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.8384,
      "num_input_tokens_seen": 54912,
      "step": 68
    },
    {
      "epoch": 0.05665024630541872,
      "grad_norm": 7.7081475257873535,
      "learning_rate": 3.45e-05,
      "loss": 0.9989,
      "num_input_tokens_seen": 55680,
      "step": 69
    },
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 13.117938041687012,
      "learning_rate": 3.5e-05,
      "loss": 0.8505,
      "num_input_tokens_seen": 56448,
      "step": 70
    },
    {
      "epoch": 0.05829228243021346,
      "grad_norm": 4.930790424346924,
      "learning_rate": 3.55e-05,
      "loss": 0.5102,
      "num_input_tokens_seen": 57216,
      "step": 71
    },
    {
      "epoch": 0.059113300492610835,
      "grad_norm": 8.219292640686035,
      "learning_rate": 3.6e-05,
      "loss": 0.7402,
      "num_input_tokens_seen": 57984,
      "step": 72
    },
    {
      "epoch": 0.05993431855500821,
      "grad_norm": 5.455821990966797,
      "learning_rate": 3.65e-05,
      "loss": 0.4328,
      "num_input_tokens_seen": 58752,
      "step": 73
    },
    {
      "epoch": 0.060755336617405585,
      "grad_norm": 13.180975914001465,
      "learning_rate": 3.7e-05,
      "loss": 1.5798,
      "num_input_tokens_seen": 59520,
      "step": 74
    },
    {
      "epoch": 0.06157635467980296,
      "grad_norm": 9.903757095336914,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.7415,
      "num_input_tokens_seen": 60416,
      "step": 75
    },
    {
      "epoch": 0.06239737274220033,
      "grad_norm": 8.26589298248291,
      "learning_rate": 3.8e-05,
      "loss": 0.8347,
      "num_input_tokens_seen": 61184,
      "step": 76
    },
    {
      "epoch": 0.06321839080459771,
      "grad_norm": 10.284740447998047,
      "learning_rate": 3.85e-05,
      "loss": 1.5484,
      "num_input_tokens_seen": 61952,
      "step": 77
    },
    {
      "epoch": 0.06403940886699508,
      "grad_norm": 8.761502265930176,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.1347,
      "num_input_tokens_seen": 62720,
      "step": 78
    },
    {
      "epoch": 0.06486042692939245,
      "grad_norm": 7.915812015533447,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.9399,
      "num_input_tokens_seen": 63488,
      "step": 79
    },
    {
      "epoch": 0.06568144499178982,
      "grad_norm": 8.7372407913208,
      "learning_rate": 4e-05,
      "loss": 1.0012,
      "num_input_tokens_seen": 64256,
      "step": 80
    },
    {
      "epoch": 0.0665024630541872,
      "grad_norm": 4.543681621551514,
      "learning_rate": 4.05e-05,
      "loss": 0.2425,
      "num_input_tokens_seen": 65280,
      "step": 81
    },
    {
      "epoch": 0.06732348111658457,
      "grad_norm": 4.63873815536499,
      "learning_rate": 4.1e-05,
      "loss": 0.3552,
      "num_input_tokens_seen": 66176,
      "step": 82
    },
    {
      "epoch": 0.06814449917898194,
      "grad_norm": 4.365788459777832,
      "learning_rate": 4.15e-05,
      "loss": 0.7575,
      "num_input_tokens_seen": 66944,
      "step": 83
    },
    {
      "epoch": 0.06896551724137931,
      "grad_norm": 6.43505334854126,
      "learning_rate": 4.2e-05,
      "loss": 0.5292,
      "num_input_tokens_seen": 67712,
      "step": 84
    },
    {
      "epoch": 0.06978653530377668,
      "grad_norm": 4.862233638763428,
      "learning_rate": 4.25e-05,
      "loss": 0.4454,
      "num_input_tokens_seen": 68480,
      "step": 85
    },
    {
      "epoch": 0.07060755336617405,
      "grad_norm": 10.807046890258789,
      "learning_rate": 4.3e-05,
      "loss": 1.2983,
      "num_input_tokens_seen": 69376,
      "step": 86
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 3.7400972843170166,
      "learning_rate": 4.35e-05,
      "loss": 0.2139,
      "num_input_tokens_seen": 70144,
      "step": 87
    },
    {
      "epoch": 0.0722495894909688,
      "grad_norm": 5.462031841278076,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6245,
      "num_input_tokens_seen": 70912,
      "step": 88
    },
    {
      "epoch": 0.07307060755336617,
      "grad_norm": 10.011672973632812,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.993,
      "num_input_tokens_seen": 71808,
      "step": 89
    },
    {
      "epoch": 0.07389162561576355,
      "grad_norm": 13.862468719482422,
      "learning_rate": 4.5e-05,
      "loss": 0.7855,
      "num_input_tokens_seen": 72576,
      "step": 90
    },
    {
      "epoch": 0.07471264367816093,
      "grad_norm": 7.503834247589111,
      "learning_rate": 4.55e-05,
      "loss": 0.4845,
      "num_input_tokens_seen": 73472,
      "step": 91
    },
    {
      "epoch": 0.0755336617405583,
      "grad_norm": 9.899796485900879,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6817,
      "num_input_tokens_seen": 74240,
      "step": 92
    },
    {
      "epoch": 0.07635467980295567,
      "grad_norm": 11.06462574005127,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.7633,
      "num_input_tokens_seen": 75008,
      "step": 93
    },
    {
      "epoch": 0.07717569786535304,
      "grad_norm": 10.086703300476074,
      "learning_rate": 4.7e-05,
      "loss": 0.8192,
      "num_input_tokens_seen": 75776,
      "step": 94
    },
    {
      "epoch": 0.07799671592775041,
      "grad_norm": 12.958508491516113,
      "learning_rate": 4.75e-05,
      "loss": 1.8526,
      "num_input_tokens_seen": 76544,
      "step": 95
    },
    {
      "epoch": 0.07881773399014778,
      "grad_norm": 11.212610244750977,
      "learning_rate": 4.8e-05,
      "loss": 1.0474,
      "num_input_tokens_seen": 77312,
      "step": 96
    },
    {
      "epoch": 0.07963875205254516,
      "grad_norm": 9.119144439697266,
      "learning_rate": 4.85e-05,
      "loss": 1.0576,
      "num_input_tokens_seen": 78080,
      "step": 97
    },
    {
      "epoch": 0.08045977011494253,
      "grad_norm": 8.183847427368164,
      "learning_rate": 4.9e-05,
      "loss": 0.552,
      "num_input_tokens_seen": 78976,
      "step": 98
    },
    {
      "epoch": 0.0812807881773399,
      "grad_norm": 6.315762519836426,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.8405,
      "num_input_tokens_seen": 79744,
      "step": 99
    },
    {
      "epoch": 0.08210180623973727,
      "grad_norm": 5.49755859375,
      "learning_rate": 5e-05,
      "loss": 0.5618,
      "num_input_tokens_seen": 80512,
      "step": 100
    },
    {
      "epoch": 0.08292282430213464,
      "grad_norm": 6.5723876953125,
      "learning_rate": 4.999984769144476e-05,
      "loss": 0.438,
      "num_input_tokens_seen": 81280,
      "step": 101
    },
    {
      "epoch": 0.08374384236453201,
      "grad_norm": 6.386931896209717,
      "learning_rate": 4.999939076763487e-05,
      "loss": 0.6732,
      "num_input_tokens_seen": 82176,
      "step": 102
    },
    {
      "epoch": 0.08456486042692939,
      "grad_norm": 5.918614864349365,
      "learning_rate": 4.999862923413781e-05,
      "loss": 0.325,
      "num_input_tokens_seen": 83072,
      "step": 103
    },
    {
      "epoch": 0.08538587848932677,
      "grad_norm": 6.519242286682129,
      "learning_rate": 4.999756310023261e-05,
      "loss": 1.1074,
      "num_input_tokens_seen": 83840,
      "step": 104
    },
    {
      "epoch": 0.08620689655172414,
      "grad_norm": 7.991783142089844,
      "learning_rate": 4.9996192378909786e-05,
      "loss": 0.6196,
      "num_input_tokens_seen": 84736,
      "step": 105
    },
    {
      "epoch": 0.08702791461412152,
      "grad_norm": 6.740426540374756,
      "learning_rate": 4.999451708687114e-05,
      "loss": 0.6934,
      "num_input_tokens_seen": 85504,
      "step": 106
    },
    {
      "epoch": 0.08784893267651889,
      "grad_norm": 4.977389812469482,
      "learning_rate": 4.999253724452958e-05,
      "loss": 0.4409,
      "num_input_tokens_seen": 86272,
      "step": 107
    },
    {
      "epoch": 0.08866995073891626,
      "grad_norm": 6.260483264923096,
      "learning_rate": 4.999025287600886e-05,
      "loss": 0.8237,
      "num_input_tokens_seen": 87040,
      "step": 108
    },
    {
      "epoch": 0.08949096880131363,
      "grad_norm": 6.040188789367676,
      "learning_rate": 4.998766400914329e-05,
      "loss": 0.6399,
      "num_input_tokens_seen": 87808,
      "step": 109
    },
    {
      "epoch": 0.090311986863711,
      "grad_norm": 11.274331092834473,
      "learning_rate": 4.99847706754774e-05,
      "loss": 0.6499,
      "num_input_tokens_seen": 88704,
      "step": 110
    },
    {
      "epoch": 0.09113300492610837,
      "grad_norm": 6.608374118804932,
      "learning_rate": 4.998157291026553e-05,
      "loss": 0.4017,
      "num_input_tokens_seen": 89600,
      "step": 111
    },
    {
      "epoch": 0.09195402298850575,
      "grad_norm": 7.3528032302856445,
      "learning_rate": 4.997807075247146e-05,
      "loss": 0.995,
      "num_input_tokens_seen": 90368,
      "step": 112
    },
    {
      "epoch": 0.09277504105090312,
      "grad_norm": 8.307158470153809,
      "learning_rate": 4.997426424476787e-05,
      "loss": 1.1572,
      "num_input_tokens_seen": 91264,
      "step": 113
    },
    {
      "epoch": 0.09359605911330049,
      "grad_norm": 8.229576110839844,
      "learning_rate": 4.997015343353585e-05,
      "loss": 0.3774,
      "num_input_tokens_seen": 92032,
      "step": 114
    },
    {
      "epoch": 0.09441707717569786,
      "grad_norm": 5.742584228515625,
      "learning_rate": 4.996573836886435e-05,
      "loss": 0.2884,
      "num_input_tokens_seen": 92800,
      "step": 115
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 10.66940689086914,
      "learning_rate": 4.996101910454953e-05,
      "loss": 0.8823,
      "num_input_tokens_seen": 93568,
      "step": 116
    },
    {
      "epoch": 0.0960591133004926,
      "grad_norm": 2.1488213539123535,
      "learning_rate": 4.995599569809414e-05,
      "loss": 0.0877,
      "num_input_tokens_seen": 94336,
      "step": 117
    },
    {
      "epoch": 0.09688013136288999,
      "grad_norm": 8.95203685760498,
      "learning_rate": 4.995066821070679e-05,
      "loss": 0.8605,
      "num_input_tokens_seen": 95232,
      "step": 118
    },
    {
      "epoch": 0.09770114942528736,
      "grad_norm": 9.339530944824219,
      "learning_rate": 4.994503670730125e-05,
      "loss": 0.6711,
      "num_input_tokens_seen": 96000,
      "step": 119
    },
    {
      "epoch": 0.09852216748768473,
      "grad_norm": 11.61047077178955,
      "learning_rate": 4.993910125649561e-05,
      "loss": 0.6043,
      "num_input_tokens_seen": 96768,
      "step": 120
    },
    {
      "epoch": 0.0993431855500821,
      "grad_norm": 7.390104293823242,
      "learning_rate": 4.9932861930611454e-05,
      "loss": 0.8351,
      "num_input_tokens_seen": 97536,
      "step": 121
    },
    {
      "epoch": 0.10016420361247948,
      "grad_norm": 7.512075424194336,
      "learning_rate": 4.992631880567301e-05,
      "loss": 0.6672,
      "num_input_tokens_seen": 98432,
      "step": 122
    },
    {
      "epoch": 0.10098522167487685,
      "grad_norm": 10.710936546325684,
      "learning_rate": 4.991947196140618e-05,
      "loss": 0.5889,
      "num_input_tokens_seen": 99200,
      "step": 123
    },
    {
      "epoch": 0.10180623973727422,
      "grad_norm": 9.055704116821289,
      "learning_rate": 4.991232148123761e-05,
      "loss": 1.1129,
      "num_input_tokens_seen": 100096,
      "step": 124
    },
    {
      "epoch": 0.10262725779967159,
      "grad_norm": 12.829014778137207,
      "learning_rate": 4.990486745229364e-05,
      "loss": 1.2973,
      "num_input_tokens_seen": 100864,
      "step": 125
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 8.458701133728027,
      "learning_rate": 4.989710996539926e-05,
      "loss": 0.5418,
      "num_input_tokens_seen": 101632,
      "step": 126
    },
    {
      "epoch": 0.10426929392446634,
      "grad_norm": 11.947745323181152,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 1.1518,
      "num_input_tokens_seen": 102400,
      "step": 127
    },
    {
      "epoch": 0.10509031198686371,
      "grad_norm": 13.01159954071045,
      "learning_rate": 4.988068499954578e-05,
      "loss": 1.8625,
      "num_input_tokens_seen": 103168,
      "step": 128
    },
    {
      "epoch": 0.10591133004926108,
      "grad_norm": 7.519097328186035,
      "learning_rate": 4.987201772071971e-05,
      "loss": 0.3021,
      "num_input_tokens_seen": 103936,
      "step": 129
    },
    {
      "epoch": 0.10673234811165845,
      "grad_norm": 11.305360794067383,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 1.1837,
      "num_input_tokens_seen": 104704,
      "step": 130
    },
    {
      "epoch": 0.10755336617405582,
      "grad_norm": 18.21817970275879,
      "learning_rate": 4.985377409930789e-05,
      "loss": 1.322,
      "num_input_tokens_seen": 105472,
      "step": 131
    },
    {
      "epoch": 0.10837438423645321,
      "grad_norm": 8.388663291931152,
      "learning_rate": 4.984419797901491e-05,
      "loss": 0.8545,
      "num_input_tokens_seen": 106240,
      "step": 132
    },
    {
      "epoch": 0.10919540229885058,
      "grad_norm": 6.451061725616455,
      "learning_rate": 4.983431914000991e-05,
      "loss": 0.3409,
      "num_input_tokens_seen": 107008,
      "step": 133
    },
    {
      "epoch": 0.11001642036124795,
      "grad_norm": 7.117460250854492,
      "learning_rate": 4.982413770266342e-05,
      "loss": 0.5104,
      "num_input_tokens_seen": 107776,
      "step": 134
    },
    {
      "epoch": 0.11083743842364532,
      "grad_norm": 7.384696006774902,
      "learning_rate": 4.9813653791033057e-05,
      "loss": 0.6154,
      "num_input_tokens_seen": 108544,
      "step": 135
    },
    {
      "epoch": 0.1116584564860427,
      "grad_norm": 9.036718368530273,
      "learning_rate": 4.980286753286195e-05,
      "loss": 0.7518,
      "num_input_tokens_seen": 109312,
      "step": 136
    },
    {
      "epoch": 0.11247947454844007,
      "grad_norm": 10.99615478515625,
      "learning_rate": 4.979177905957726e-05,
      "loss": 0.8706,
      "num_input_tokens_seen": 110208,
      "step": 137
    },
    {
      "epoch": 0.11330049261083744,
      "grad_norm": 14.5789213180542,
      "learning_rate": 4.978038850628854e-05,
      "loss": 1.8887,
      "num_input_tokens_seen": 111104,
      "step": 138
    },
    {
      "epoch": 0.11412151067323481,
      "grad_norm": 7.568974018096924,
      "learning_rate": 4.976869601178609e-05,
      "loss": 0.7992,
      "num_input_tokens_seen": 111872,
      "step": 139
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 10.568921089172363,
      "learning_rate": 4.975670171853926e-05,
      "loss": 1.2638,
      "num_input_tokens_seen": 112640,
      "step": 140
    },
    {
      "epoch": 0.11576354679802955,
      "grad_norm": 4.187779903411865,
      "learning_rate": 4.9744405772694725e-05,
      "loss": 0.5064,
      "num_input_tokens_seen": 113408,
      "step": 141
    },
    {
      "epoch": 0.11658456486042693,
      "grad_norm": 6.037436008453369,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 0.8121,
      "num_input_tokens_seen": 114176,
      "step": 142
    },
    {
      "epoch": 0.1174055829228243,
      "grad_norm": 3.962977409362793,
      "learning_rate": 4.971890952617515e-05,
      "loss": 0.32,
      "num_input_tokens_seen": 114944,
      "step": 143
    },
    {
      "epoch": 0.11822660098522167,
      "grad_norm": 8.684307098388672,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 1.0179,
      "num_input_tokens_seen": 115712,
      "step": 144
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 8.66793155670166,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.9401,
      "num_input_tokens_seen": 116480,
      "step": 145
    },
    {
      "epoch": 0.11986863711001643,
      "grad_norm": 8.432429313659668,
      "learning_rate": 4.96784066268247e-05,
      "loss": 0.6334,
      "num_input_tokens_seen": 117248,
      "step": 146
    },
    {
      "epoch": 0.1206896551724138,
      "grad_norm": 12.489288330078125,
      "learning_rate": 4.966430404017424e-05,
      "loss": 0.8673,
      "num_input_tokens_seen": 118016,
      "step": 147
    },
    {
      "epoch": 0.12151067323481117,
      "grad_norm": 7.433355331420898,
      "learning_rate": 4.964990092676263e-05,
      "loss": 0.5921,
      "num_input_tokens_seen": 118784,
      "step": 148
    },
    {
      "epoch": 0.12233169129720854,
      "grad_norm": 6.575742244720459,
      "learning_rate": 4.963519746208726e-05,
      "loss": 1.0044,
      "num_input_tokens_seen": 119552,
      "step": 149
    },
    {
      "epoch": 0.12315270935960591,
      "grad_norm": 6.2380876541137695,
      "learning_rate": 4.962019382530521e-05,
      "loss": 0.8764,
      "num_input_tokens_seen": 120448,
      "step": 150
    },
    {
      "epoch": 0.12397372742200329,
      "grad_norm": 7.264809608459473,
      "learning_rate": 4.960489019923105e-05,
      "loss": 0.8453,
      "num_input_tokens_seen": 121472,
      "step": 151
    },
    {
      "epoch": 0.12479474548440066,
      "grad_norm": 8.024356842041016,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 0.8942,
      "num_input_tokens_seen": 122240,
      "step": 152
    },
    {
      "epoch": 0.12561576354679804,
      "grad_norm": 5.499040126800537,
      "learning_rate": 4.957338372873886e-05,
      "loss": 0.4134,
      "num_input_tokens_seen": 123008,
      "step": 153
    },
    {
      "epoch": 0.12643678160919541,
      "grad_norm": 5.8389997482299805,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 0.872,
      "num_input_tokens_seen": 123776,
      "step": 154
    },
    {
      "epoch": 0.1272577996715928,
      "grad_norm": 8.259109497070312,
      "learning_rate": 4.9540679586191605e-05,
      "loss": 1.0302,
      "num_input_tokens_seen": 124544,
      "step": 155
    },
    {
      "epoch": 0.12807881773399016,
      "grad_norm": 10.935829162597656,
      "learning_rate": 4.952387888372979e-05,
      "loss": 1.2984,
      "num_input_tokens_seen": 125312,
      "step": 156
    },
    {
      "epoch": 0.12889983579638753,
      "grad_norm": 8.442298889160156,
      "learning_rate": 4.9506779365543046e-05,
      "loss": 0.7604,
      "num_input_tokens_seen": 126080,
      "step": 157
    },
    {
      "epoch": 0.1297208538587849,
      "grad_norm": 9.327191352844238,
      "learning_rate": 4.94893812399836e-05,
      "loss": 1.2535,
      "num_input_tokens_seen": 126848,
      "step": 158
    },
    {
      "epoch": 0.13054187192118227,
      "grad_norm": 7.781191349029541,
      "learning_rate": 4.947168471904213e-05,
      "loss": 0.6815,
      "num_input_tokens_seen": 127744,
      "step": 159
    },
    {
      "epoch": 0.13136288998357964,
      "grad_norm": 6.766125202178955,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 0.4864,
      "num_input_tokens_seen": 128512,
      "step": 160
    },
    {
      "epoch": 0.13218390804597702,
      "grad_norm": 5.7757110595703125,
      "learning_rate": 4.94353973571524e-05,
      "loss": 0.8613,
      "num_input_tokens_seen": 129280,
      "step": 161
    },
    {
      "epoch": 0.1330049261083744,
      "grad_norm": 6.647087097167969,
      "learning_rate": 4.94168069583542e-05,
      "loss": 0.9243,
      "num_input_tokens_seen": 130176,
      "step": 162
    },
    {
      "epoch": 0.13382594417077176,
      "grad_norm": 5.612727642059326,
      "learning_rate": 4.939791904846869e-05,
      "loss": 0.4001,
      "num_input_tokens_seen": 130944,
      "step": 163
    },
    {
      "epoch": 0.13464696223316913,
      "grad_norm": 8.993368148803711,
      "learning_rate": 4.937873385763908e-05,
      "loss": 1.0055,
      "num_input_tokens_seen": 131712,
      "step": 164
    },
    {
      "epoch": 0.1354679802955665,
      "grad_norm": 6.558959484100342,
      "learning_rate": 4.9359251619630886e-05,
      "loss": 0.5722,
      "num_input_tokens_seen": 132480,
      "step": 165
    },
    {
      "epoch": 0.13628899835796388,
      "grad_norm": 6.017406463623047,
      "learning_rate": 4.933947257182901e-05,
      "loss": 0.5414,
      "num_input_tokens_seen": 133248,
      "step": 166
    },
    {
      "epoch": 0.13711001642036125,
      "grad_norm": 6.858363151550293,
      "learning_rate": 4.931939695523492e-05,
      "loss": 0.7491,
      "num_input_tokens_seen": 134016,
      "step": 167
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 5.907204627990723,
      "learning_rate": 4.929902501446366e-05,
      "loss": 0.5278,
      "num_input_tokens_seen": 134912,
      "step": 168
    },
    {
      "epoch": 0.138752052545156,
      "grad_norm": 5.08438777923584,
      "learning_rate": 4.9278356997740904e-05,
      "loss": 0.36,
      "num_input_tokens_seen": 135680,
      "step": 169
    },
    {
      "epoch": 0.13957307060755336,
      "grad_norm": 9.283734321594238,
      "learning_rate": 4.925739315689991e-05,
      "loss": 0.5553,
      "num_input_tokens_seen": 136448,
      "step": 170
    },
    {
      "epoch": 0.14039408866995073,
      "grad_norm": 8.952007293701172,
      "learning_rate": 4.9236133747378475e-05,
      "loss": 0.6688,
      "num_input_tokens_seen": 137216,
      "step": 171
    },
    {
      "epoch": 0.1412151067323481,
      "grad_norm": 10.444091796875,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 0.6182,
      "num_input_tokens_seen": 138112,
      "step": 172
    },
    {
      "epoch": 0.14203612479474548,
      "grad_norm": 8.011463165283203,
      "learning_rate": 4.919272926204929e-05,
      "loss": 0.7437,
      "num_input_tokens_seen": 138880,
      "step": 173
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 12.323012351989746,
      "learning_rate": 4.917058471511149e-05,
      "loss": 0.9288,
      "num_input_tokens_seen": 139648,
      "step": 174
    },
    {
      "epoch": 0.14367816091954022,
      "grad_norm": 8.478352546691895,
      "learning_rate": 4.914814565722671e-05,
      "loss": 0.3611,
      "num_input_tokens_seen": 140416,
      "step": 175
    },
    {
      "epoch": 0.1444991789819376,
      "grad_norm": 9.369117736816406,
      "learning_rate": 4.912541236180779e-05,
      "loss": 0.482,
      "num_input_tokens_seen": 141312,
      "step": 176
    },
    {
      "epoch": 0.14532019704433496,
      "grad_norm": 5.709687232971191,
      "learning_rate": 4.910238510585276e-05,
      "loss": 0.6356,
      "num_input_tokens_seen": 142208,
      "step": 177
    },
    {
      "epoch": 0.14614121510673234,
      "grad_norm": 14.732266426086426,
      "learning_rate": 4.907906416994146e-05,
      "loss": 1.1729,
      "num_input_tokens_seen": 142976,
      "step": 178
    },
    {
      "epoch": 0.1469622331691297,
      "grad_norm": 9.379703521728516,
      "learning_rate": 4.905544983823214e-05,
      "loss": 1.1593,
      "num_input_tokens_seen": 143744,
      "step": 179
    },
    {
      "epoch": 0.1477832512315271,
      "grad_norm": 8.5269775390625,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 0.5542,
      "num_input_tokens_seen": 144640,
      "step": 180
    },
    {
      "epoch": 0.14860426929392448,
      "grad_norm": 11.055234909057617,
      "learning_rate": 4.900734214192358e-05,
      "loss": 0.568,
      "num_input_tokens_seen": 145408,
      "step": 181
    },
    {
      "epoch": 0.14942528735632185,
      "grad_norm": 9.141350746154785,
      "learning_rate": 4.898284936350144e-05,
      "loss": 1.1448,
      "num_input_tokens_seen": 146176,
      "step": 182
    },
    {
      "epoch": 0.15024630541871922,
      "grad_norm": 3.4645204544067383,
      "learning_rate": 4.895806436162833e-05,
      "loss": 0.1059,
      "num_input_tokens_seen": 146944,
      "step": 183
    },
    {
      "epoch": 0.1510673234811166,
      "grad_norm": 14.283713340759277,
      "learning_rate": 4.893298743830168e-05,
      "loss": 1.4788,
      "num_input_tokens_seen": 147712,
      "step": 184
    },
    {
      "epoch": 0.15188834154351397,
      "grad_norm": 7.786715030670166,
      "learning_rate": 4.890761889907589e-05,
      "loss": 0.6335,
      "num_input_tokens_seen": 148480,
      "step": 185
    },
    {
      "epoch": 0.15270935960591134,
      "grad_norm": 6.164500713348389,
      "learning_rate": 4.888195905305859e-05,
      "loss": 0.3459,
      "num_input_tokens_seen": 149376,
      "step": 186
    },
    {
      "epoch": 0.1535303776683087,
      "grad_norm": 6.064222812652588,
      "learning_rate": 4.8856008212906925e-05,
      "loss": 0.3951,
      "num_input_tokens_seen": 150272,
      "step": 187
    },
    {
      "epoch": 0.15435139573070608,
      "grad_norm": 3.3713574409484863,
      "learning_rate": 4.882976669482367e-05,
      "loss": 0.0889,
      "num_input_tokens_seen": 151040,
      "step": 188
    },
    {
      "epoch": 0.15517241379310345,
      "grad_norm": 6.079710960388184,
      "learning_rate": 4.880323481855347e-05,
      "loss": 0.4981,
      "num_input_tokens_seen": 151936,
      "step": 189
    },
    {
      "epoch": 0.15599343185550082,
      "grad_norm": 13.397343635559082,
      "learning_rate": 4.877641290737884e-05,
      "loss": 1.0925,
      "num_input_tokens_seen": 152704,
      "step": 190
    },
    {
      "epoch": 0.1568144499178982,
      "grad_norm": 8.555642127990723,
      "learning_rate": 4.874930128811631e-05,
      "loss": 0.663,
      "num_input_tokens_seen": 153472,
      "step": 191
    },
    {
      "epoch": 0.15763546798029557,
      "grad_norm": 11.567663192749023,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 0.5778,
      "num_input_tokens_seen": 154240,
      "step": 192
    },
    {
      "epoch": 0.15845648604269294,
      "grad_norm": 10.885836601257324,
      "learning_rate": 4.869421025023965e-05,
      "loss": 1.0336,
      "num_input_tokens_seen": 155264,
      "step": 193
    },
    {
      "epoch": 0.1592775041050903,
      "grad_norm": 16.57675552368164,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 1.3211,
      "num_input_tokens_seen": 156032,
      "step": 194
    },
    {
      "epoch": 0.16009852216748768,
      "grad_norm": 5.948740482330322,
      "learning_rate": 4.8637964389982926e-05,
      "loss": 0.3694,
      "num_input_tokens_seen": 156800,
      "step": 195
    },
    {
      "epoch": 0.16091954022988506,
      "grad_norm": 8.817464828491211,
      "learning_rate": 4.860940925593703e-05,
      "loss": 0.6543,
      "num_input_tokens_seen": 157568,
      "step": 196
    },
    {
      "epoch": 0.16174055829228243,
      "grad_norm": 6.1254730224609375,
      "learning_rate": 4.858056644869002e-05,
      "loss": 0.5347,
      "num_input_tokens_seen": 158336,
      "step": 197
    },
    {
      "epoch": 0.1625615763546798,
      "grad_norm": 9.902804374694824,
      "learning_rate": 4.855143631968242e-05,
      "loss": 0.7564,
      "num_input_tokens_seen": 159104,
      "step": 198
    },
    {
      "epoch": 0.16338259441707717,
      "grad_norm": 9.242035865783691,
      "learning_rate": 4.852201922385564e-05,
      "loss": 0.4821,
      "num_input_tokens_seen": 159872,
      "step": 199
    },
    {
      "epoch": 0.16420361247947454,
      "grad_norm": 10.192383766174316,
      "learning_rate": 4.849231551964771e-05,
      "loss": 0.7711,
      "num_input_tokens_seen": 160640,
      "step": 200
    },
    {
      "epoch": 0.16502463054187191,
      "grad_norm": 16.13060760498047,
      "learning_rate": 4.84623255689889e-05,
      "loss": 1.5675,
      "num_input_tokens_seen": 161408,
      "step": 201
    },
    {
      "epoch": 0.16584564860426929,
      "grad_norm": 15.277976036071777,
      "learning_rate": 4.843204973729729e-05,
      "loss": 0.7756,
      "num_input_tokens_seen": 162176,
      "step": 202
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 11.768658638000488,
      "learning_rate": 4.840148839347434e-05,
      "loss": 0.7414,
      "num_input_tokens_seen": 163072,
      "step": 203
    },
    {
      "epoch": 0.16748768472906403,
      "grad_norm": 9.151814460754395,
      "learning_rate": 4.837064190990036e-05,
      "loss": 0.4889,
      "num_input_tokens_seen": 163968,
      "step": 204
    },
    {
      "epoch": 0.1683087027914614,
      "grad_norm": 8.546988487243652,
      "learning_rate": 4.8339510662430046e-05,
      "loss": 0.426,
      "num_input_tokens_seen": 164864,
      "step": 205
    },
    {
      "epoch": 0.16912972085385877,
      "grad_norm": 15.78449821472168,
      "learning_rate": 4.830809503038781e-05,
      "loss": 0.5843,
      "num_input_tokens_seen": 165632,
      "step": 206
    },
    {
      "epoch": 0.16995073891625614,
      "grad_norm": 9.902948379516602,
      "learning_rate": 4.827639539656321e-05,
      "loss": 0.7094,
      "num_input_tokens_seen": 166528,
      "step": 207
    },
    {
      "epoch": 0.17077175697865354,
      "grad_norm": 7.178217887878418,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 0.4027,
      "num_input_tokens_seen": 167296,
      "step": 208
    },
    {
      "epoch": 0.17159277504105092,
      "grad_norm": 7.320613384246826,
      "learning_rate": 4.8212145672022844e-05,
      "loss": 0.4303,
      "num_input_tokens_seen": 168064,
      "step": 209
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 10.357924461364746,
      "learning_rate": 4.817959636416969e-05,
      "loss": 0.8305,
      "num_input_tokens_seen": 168832,
      "step": 210
    },
    {
      "epoch": 0.17323481116584566,
      "grad_norm": 9.92581844329834,
      "learning_rate": 4.814676462024988e-05,
      "loss": 0.5472,
      "num_input_tokens_seen": 169728,
      "step": 211
    },
    {
      "epoch": 0.17405582922824303,
      "grad_norm": 13.226572036743164,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 1.1148,
      "num_input_tokens_seen": 170496,
      "step": 212
    },
    {
      "epoch": 0.1748768472906404,
      "grad_norm": 14.836383819580078,
      "learning_rate": 4.808025542782453e-05,
      "loss": 0.2967,
      "num_input_tokens_seen": 171264,
      "step": 213
    },
    {
      "epoch": 0.17569786535303777,
      "grad_norm": 6.940297603607178,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 0.3199,
      "num_input_tokens_seen": 172032,
      "step": 214
    },
    {
      "epoch": 0.17651888341543515,
      "grad_norm": 8.679387092590332,
      "learning_rate": 4.8012621336311016e-05,
      "loss": 0.4959,
      "num_input_tokens_seen": 172800,
      "step": 215
    },
    {
      "epoch": 0.17733990147783252,
      "grad_norm": 10.164342880249023,
      "learning_rate": 4.797838348138086e-05,
      "loss": 1.1097,
      "num_input_tokens_seen": 173696,
      "step": 216
    },
    {
      "epoch": 0.1781609195402299,
      "grad_norm": 11.588460922241211,
      "learning_rate": 4.794386564209953e-05,
      "loss": 1.0097,
      "num_input_tokens_seen": 174464,
      "step": 217
    },
    {
      "epoch": 0.17898193760262726,
      "grad_norm": 15.10843563079834,
      "learning_rate": 4.790906823905599e-05,
      "loss": 1.1774,
      "num_input_tokens_seen": 175360,
      "step": 218
    },
    {
      "epoch": 0.17980295566502463,
      "grad_norm": 9.19372272491455,
      "learning_rate": 4.7873991696245624e-05,
      "loss": 0.699,
      "num_input_tokens_seen": 176128,
      "step": 219
    },
    {
      "epoch": 0.180623973727422,
      "grad_norm": 14.162593841552734,
      "learning_rate": 4.783863644106502e-05,
      "loss": 1.3481,
      "num_input_tokens_seen": 176896,
      "step": 220
    },
    {
      "epoch": 0.18144499178981938,
      "grad_norm": 2.362032890319824,
      "learning_rate": 4.780300290430682e-05,
      "loss": 0.0733,
      "num_input_tokens_seen": 177664,
      "step": 221
    },
    {
      "epoch": 0.18226600985221675,
      "grad_norm": 8.630536079406738,
      "learning_rate": 4.776709152015443e-05,
      "loss": 0.711,
      "num_input_tokens_seen": 178560,
      "step": 222
    },
    {
      "epoch": 0.18308702791461412,
      "grad_norm": 11.188087463378906,
      "learning_rate": 4.773090272617672e-05,
      "loss": 0.6251,
      "num_input_tokens_seen": 179456,
      "step": 223
    },
    {
      "epoch": 0.1839080459770115,
      "grad_norm": 10.233144760131836,
      "learning_rate": 4.769443696332272e-05,
      "loss": 0.7787,
      "num_input_tokens_seen": 180352,
      "step": 224
    },
    {
      "epoch": 0.18472906403940886,
      "grad_norm": 10.465569496154785,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.7329,
      "num_input_tokens_seen": 181120,
      "step": 225
    },
    {
      "epoch": 0.18555008210180624,
      "grad_norm": 13.062928199768066,
      "learning_rate": 4.762067631165049e-05,
      "loss": 1.0101,
      "num_input_tokens_seen": 181888,
      "step": 226
    },
    {
      "epoch": 0.1863711001642036,
      "grad_norm": 10.697338104248047,
      "learning_rate": 4.758338232158252e-05,
      "loss": 0.7434,
      "num_input_tokens_seen": 182912,
      "step": 227
    },
    {
      "epoch": 0.18719211822660098,
      "grad_norm": 9.02255916595459,
      "learning_rate": 4.754581316012785e-05,
      "loss": 0.8673,
      "num_input_tokens_seen": 183680,
      "step": 228
    },
    {
      "epoch": 0.18801313628899835,
      "grad_norm": 5.884128570556641,
      "learning_rate": 4.7507969285054845e-05,
      "loss": 0.2691,
      "num_input_tokens_seen": 184320,
      "step": 229
    },
    {
      "epoch": 0.18883415435139572,
      "grad_norm": 9.889622688293457,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 0.716,
      "num_input_tokens_seen": 185088,
      "step": 230
    },
    {
      "epoch": 0.1896551724137931,
      "grad_norm": 12.578025817871094,
      "learning_rate": 4.743145924185821e-05,
      "loss": 0.5331,
      "num_input_tokens_seen": 185856,
      "step": 231
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.9418833255767822,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 0.1412,
      "num_input_tokens_seen": 186624,
      "step": 232
    },
    {
      "epoch": 0.19129720853858784,
      "grad_norm": 6.305059432983398,
      "learning_rate": 4.73538559209842e-05,
      "loss": 0.5351,
      "num_input_tokens_seen": 187392,
      "step": 233
    },
    {
      "epoch": 0.1921182266009852,
      "grad_norm": 10.72756576538086,
      "learning_rate": 4.731464546130314e-05,
      "loss": 0.6051,
      "num_input_tokens_seen": 188160,
      "step": 234
    },
    {
      "epoch": 0.19293924466338258,
      "grad_norm": 13.948174476623535,
      "learning_rate": 4.72751631047092e-05,
      "loss": 0.6035,
      "num_input_tokens_seen": 189056,
      "step": 235
    },
    {
      "epoch": 0.19376026272577998,
      "grad_norm": 10.891805648803711,
      "learning_rate": 4.723540933228244e-05,
      "loss": 0.4585,
      "num_input_tokens_seen": 189824,
      "step": 236
    },
    {
      "epoch": 0.19458128078817735,
      "grad_norm": 15.784191131591797,
      "learning_rate": 4.719538462841003e-05,
      "loss": 1.4779,
      "num_input_tokens_seen": 190592,
      "step": 237
    },
    {
      "epoch": 0.19540229885057472,
      "grad_norm": 12.720844268798828,
      "learning_rate": 4.715508948078037e-05,
      "loss": 0.9528,
      "num_input_tokens_seen": 191360,
      "step": 238
    },
    {
      "epoch": 0.1962233169129721,
      "grad_norm": 9.025384902954102,
      "learning_rate": 4.71145243803771e-05,
      "loss": 0.544,
      "num_input_tokens_seen": 192128,
      "step": 239
    },
    {
      "epoch": 0.19704433497536947,
      "grad_norm": 5.914089679718018,
      "learning_rate": 4.707368982147318e-05,
      "loss": 0.2652,
      "num_input_tokens_seen": 192896,
      "step": 240
    },
    {
      "epoch": 0.19786535303776684,
      "grad_norm": 10.323225021362305,
      "learning_rate": 4.70325863016248e-05,
      "loss": 0.7046,
      "num_input_tokens_seen": 193664,
      "step": 241
    },
    {
      "epoch": 0.1986863711001642,
      "grad_norm": 6.765173435211182,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 0.1654,
      "num_input_tokens_seen": 194560,
      "step": 242
    },
    {
      "epoch": 0.19950738916256158,
      "grad_norm": 4.727198600769043,
      "learning_rate": 4.694957438569951e-05,
      "loss": 0.1516,
      "num_input_tokens_seen": 195328,
      "step": 243
    },
    {
      "epoch": 0.20032840722495895,
      "grad_norm": 8.337284088134766,
      "learning_rate": 4.690766700109659e-05,
      "loss": 0.5548,
      "num_input_tokens_seen": 196224,
      "step": 244
    },
    {
      "epoch": 0.20114942528735633,
      "grad_norm": 10.06086254119873,
      "learning_rate": 4.6865492678484895e-05,
      "loss": 0.7927,
      "num_input_tokens_seen": 196992,
      "step": 245
    },
    {
      "epoch": 0.2019704433497537,
      "grad_norm": 14.558396339416504,
      "learning_rate": 4.682305193174524e-05,
      "loss": 0.5002,
      "num_input_tokens_seen": 198016,
      "step": 246
    },
    {
      "epoch": 0.20279146141215107,
      "grad_norm": 16.47201919555664,
      "learning_rate": 4.678034527800474e-05,
      "loss": 1.0315,
      "num_input_tokens_seen": 198784,
      "step": 247
    },
    {
      "epoch": 0.20361247947454844,
      "grad_norm": 8.564411163330078,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 0.3477,
      "num_input_tokens_seen": 199552,
      "step": 248
    },
    {
      "epoch": 0.2044334975369458,
      "grad_norm": 11.452707290649414,
      "learning_rate": 4.669413633422322e-05,
      "loss": 0.518,
      "num_input_tokens_seen": 200320,
      "step": 249
    },
    {
      "epoch": 0.20525451559934318,
      "grad_norm": 15.923043251037598,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.3567,
      "num_input_tokens_seen": 201088,
      "step": 250
    },
    {
      "epoch": 0.20607553366174056,
      "grad_norm": 8.993290901184082,
      "learning_rate": 4.6606870048842624e-05,
      "loss": 0.2448,
      "num_input_tokens_seen": 201856,
      "step": 251
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 16.938453674316406,
      "learning_rate": 4.656284173018144e-05,
      "loss": 0.375,
      "num_input_tokens_seen": 202624,
      "step": 252
    },
    {
      "epoch": 0.2077175697865353,
      "grad_norm": 14.268512725830078,
      "learning_rate": 4.65185506750986e-05,
      "loss": 0.9305,
      "num_input_tokens_seen": 203520,
      "step": 253
    },
    {
      "epoch": 0.20853858784893267,
      "grad_norm": 13.326017379760742,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 0.4216,
      "num_input_tokens_seen": 204288,
      "step": 254
    },
    {
      "epoch": 0.20935960591133004,
      "grad_norm": 19.810422897338867,
      "learning_rate": 4.642918251755281e-05,
      "loss": 1.4192,
      "num_input_tokens_seen": 205056,
      "step": 255
    },
    {
      "epoch": 0.21018062397372742,
      "grad_norm": 15.357551574707031,
      "learning_rate": 4.638410650401267e-05,
      "loss": 0.4009,
      "num_input_tokens_seen": 205824,
      "step": 256
    },
    {
      "epoch": 0.2110016420361248,
      "grad_norm": 13.459878921508789,
      "learning_rate": 4.6338769931883185e-05,
      "loss": 0.6303,
      "num_input_tokens_seen": 206592,
      "step": 257
    },
    {
      "epoch": 0.21182266009852216,
      "grad_norm": 5.047397613525391,
      "learning_rate": 4.629317335357619e-05,
      "loss": 0.1286,
      "num_input_tokens_seen": 207488,
      "step": 258
    },
    {
      "epoch": 0.21264367816091953,
      "grad_norm": 3.043764591217041,
      "learning_rate": 4.6247317324671605e-05,
      "loss": 0.0621,
      "num_input_tokens_seen": 208384,
      "step": 259
    },
    {
      "epoch": 0.2134646962233169,
      "grad_norm": 21.012584686279297,
      "learning_rate": 4.620120240391065e-05,
      "loss": 1.4217,
      "num_input_tokens_seen": 209280,
      "step": 260
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 11.871728897094727,
      "learning_rate": 4.615482915318911e-05,
      "loss": 0.61,
      "num_input_tokens_seen": 210048,
      "step": 261
    },
    {
      "epoch": 0.21510673234811165,
      "grad_norm": 15.645122528076172,
      "learning_rate": 4.610819813755038e-05,
      "loss": 0.682,
      "num_input_tokens_seen": 210816,
      "step": 262
    },
    {
      "epoch": 0.21592775041050905,
      "grad_norm": 25.75307846069336,
      "learning_rate": 4.606130992517869e-05,
      "loss": 1.2429,
      "num_input_tokens_seen": 211584,
      "step": 263
    },
    {
      "epoch": 0.21674876847290642,
      "grad_norm": 5.884362697601318,
      "learning_rate": 4.601416508739211e-05,
      "loss": 0.1254,
      "num_input_tokens_seen": 212352,
      "step": 264
    },
    {
      "epoch": 0.2175697865353038,
      "grad_norm": 13.706477165222168,
      "learning_rate": 4.5966764198635606e-05,
      "loss": 0.5887,
      "num_input_tokens_seen": 213120,
      "step": 265
    },
    {
      "epoch": 0.21839080459770116,
      "grad_norm": 8.361759185791016,
      "learning_rate": 4.591910783647404e-05,
      "loss": 0.6654,
      "num_input_tokens_seen": 214016,
      "step": 266
    },
    {
      "epoch": 0.21921182266009853,
      "grad_norm": 9.629776000976562,
      "learning_rate": 4.5871196581585166e-05,
      "loss": 0.4061,
      "num_input_tokens_seen": 214784,
      "step": 267
    },
    {
      "epoch": 0.2200328407224959,
      "grad_norm": 14.8009614944458,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 1.0212,
      "num_input_tokens_seen": 215552,
      "step": 268
    },
    {
      "epoch": 0.22085385878489328,
      "grad_norm": 4.09447717666626,
      "learning_rate": 4.577461173185821e-05,
      "loss": 0.0819,
      "num_input_tokens_seen": 216320,
      "step": 269
    },
    {
      "epoch": 0.22167487684729065,
      "grad_norm": 23.00237274169922,
      "learning_rate": 4.572593931387604e-05,
      "loss": 1.2622,
      "num_input_tokens_seen": 217216,
      "step": 270
    },
    {
      "epoch": 0.22249589490968802,
      "grad_norm": 14.540812492370605,
      "learning_rate": 4.567701435686404e-05,
      "loss": 0.2751,
      "num_input_tokens_seen": 217984,
      "step": 271
    },
    {
      "epoch": 0.2233169129720854,
      "grad_norm": 7.548282146453857,
      "learning_rate": 4.562783745695738e-05,
      "loss": 0.7723,
      "num_input_tokens_seen": 218752,
      "step": 272
    },
    {
      "epoch": 0.22413793103448276,
      "grad_norm": 15.328475952148438,
      "learning_rate": 4.557840921336105e-05,
      "loss": 1.1651,
      "num_input_tokens_seen": 219520,
      "step": 273
    },
    {
      "epoch": 0.22495894909688013,
      "grad_norm": 3.3256046772003174,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 0.1181,
      "num_input_tokens_seen": 220288,
      "step": 274
    },
    {
      "epoch": 0.2257799671592775,
      "grad_norm": 10.024643898010254,
      "learning_rate": 4.54788011072248e-05,
      "loss": 0.6809,
      "num_input_tokens_seen": 221056,
      "step": 275
    },
    {
      "epoch": 0.22660098522167488,
      "grad_norm": 16.069473266601562,
      "learning_rate": 4.542862245837821e-05,
      "loss": 0.7679,
      "num_input_tokens_seen": 221824,
      "step": 276
    },
    {
      "epoch": 0.22742200328407225,
      "grad_norm": 20.503541946411133,
      "learning_rate": 4.537819489321386e-05,
      "loss": 0.9428,
      "num_input_tokens_seen": 222592,
      "step": 277
    },
    {
      "epoch": 0.22824302134646962,
      "grad_norm": 11.5457124710083,
      "learning_rate": 4.532751902617569e-05,
      "loss": 0.5787,
      "num_input_tokens_seen": 223360,
      "step": 278
    },
    {
      "epoch": 0.229064039408867,
      "grad_norm": 13.356400489807129,
      "learning_rate": 4.527659547473317e-05,
      "loss": 0.558,
      "num_input_tokens_seen": 224256,
      "step": 279
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 17.863677978515625,
      "learning_rate": 4.522542485937369e-05,
      "loss": 1.08,
      "num_input_tokens_seen": 225024,
      "step": 280
    },
    {
      "epoch": 0.23070607553366174,
      "grad_norm": 4.182966709136963,
      "learning_rate": 4.5174007803595055e-05,
      "loss": 0.1675,
      "num_input_tokens_seen": 225792,
      "step": 281
    },
    {
      "epoch": 0.2315270935960591,
      "grad_norm": 14.278905868530273,
      "learning_rate": 4.512234493389785e-05,
      "loss": 0.853,
      "num_input_tokens_seen": 226688,
      "step": 282
    },
    {
      "epoch": 0.23234811165845648,
      "grad_norm": 10.72590160369873,
      "learning_rate": 4.5070436879777865e-05,
      "loss": 0.3864,
      "num_input_tokens_seen": 227456,
      "step": 283
    },
    {
      "epoch": 0.23316912972085385,
      "grad_norm": 7.951490879058838,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 0.2299,
      "num_input_tokens_seen": 228224,
      "step": 284
    },
    {
      "epoch": 0.23399014778325122,
      "grad_norm": 14.287750244140625,
      "learning_rate": 4.496588775118232e-05,
      "loss": 0.9464,
      "num_input_tokens_seen": 228992,
      "step": 285
    },
    {
      "epoch": 0.2348111658456486,
      "grad_norm": 16.80156135559082,
      "learning_rate": 4.491324795060491e-05,
      "loss": 0.8935,
      "num_input_tokens_seen": 229760,
      "step": 286
    },
    {
      "epoch": 0.23563218390804597,
      "grad_norm": 7.434930801391602,
      "learning_rate": 4.4860365513385456e-05,
      "loss": 0.2898,
      "num_input_tokens_seen": 230656,
      "step": 287
    },
    {
      "epoch": 0.23645320197044334,
      "grad_norm": 15.832178115844727,
      "learning_rate": 4.480724108387977e-05,
      "loss": 1.0485,
      "num_input_tokens_seen": 231424,
      "step": 288
    },
    {
      "epoch": 0.2372742200328407,
      "grad_norm": 11.155498504638672,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 0.6707,
      "num_input_tokens_seen": 232192,
      "step": 289
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 11.078341484069824,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 1.0429,
      "num_input_tokens_seen": 232960,
      "step": 290
    },
    {
      "epoch": 0.23891625615763548,
      "grad_norm": 4.818967342376709,
      "learning_rate": 4.464642232938505e-05,
      "loss": 0.1024,
      "num_input_tokens_seen": 233728,
      "step": 291
    },
    {
      "epoch": 0.23973727422003285,
      "grad_norm": 14.35914421081543,
      "learning_rate": 4.4592336433146e-05,
      "loss": 0.4204,
      "num_input_tokens_seen": 234624,
      "step": 292
    },
    {
      "epoch": 0.24055829228243022,
      "grad_norm": 17.647911071777344,
      "learning_rate": 4.453801181047047e-05,
      "loss": 0.7589,
      "num_input_tokens_seen": 235392,
      "step": 293
    },
    {
      "epoch": 0.2413793103448276,
      "grad_norm": 9.177369117736816,
      "learning_rate": 4.448344912328686e-05,
      "loss": 0.4243,
      "num_input_tokens_seen": 236288,
      "step": 294
    },
    {
      "epoch": 0.24220032840722497,
      "grad_norm": 11.551233291625977,
      "learning_rate": 4.442864903642428e-05,
      "loss": 0.5498,
      "num_input_tokens_seen": 237056,
      "step": 295
    },
    {
      "epoch": 0.24302134646962234,
      "grad_norm": 13.710370063781738,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 0.7119,
      "num_input_tokens_seen": 237824,
      "step": 296
    },
    {
      "epoch": 0.2438423645320197,
      "grad_norm": 20.799806594848633,
      "learning_rate": 4.431833933743378e-05,
      "loss": 1.4876,
      "num_input_tokens_seen": 238592,
      "step": 297
    },
    {
      "epoch": 0.24466338259441708,
      "grad_norm": 13.753273963928223,
      "learning_rate": 4.426283106939474e-05,
      "loss": 0.6734,
      "num_input_tokens_seen": 239488,
      "step": 298
    },
    {
      "epoch": 0.24548440065681446,
      "grad_norm": 2.6830952167510986,
      "learning_rate": 4.420708808983809e-05,
      "loss": 0.0746,
      "num_input_tokens_seen": 240384,
      "step": 299
    },
    {
      "epoch": 0.24630541871921183,
      "grad_norm": 13.032585144042969,
      "learning_rate": 4.415111107797445e-05,
      "loss": 0.9701,
      "num_input_tokens_seen": 241152,
      "step": 300
    },
    {
      "epoch": 0.2471264367816092,
      "grad_norm": 9.189692497253418,
      "learning_rate": 4.4094900715866064e-05,
      "loss": 0.5352,
      "num_input_tokens_seen": 241920,
      "step": 301
    },
    {
      "epoch": 0.24794745484400657,
      "grad_norm": 6.153103351593018,
      "learning_rate": 4.403845768841842e-05,
      "loss": 0.3657,
      "num_input_tokens_seen": 242688,
      "step": 302
    },
    {
      "epoch": 0.24876847290640394,
      "grad_norm": 5.653210639953613,
      "learning_rate": 4.3981782683372016e-05,
      "loss": 0.1515,
      "num_input_tokens_seen": 243456,
      "step": 303
    },
    {
      "epoch": 0.24958949096880131,
      "grad_norm": 4.685927867889404,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 0.1778,
      "num_input_tokens_seen": 244224,
      "step": 304
    },
    {
      "epoch": 0.2504105090311987,
      "grad_norm": 4.873725891113281,
      "learning_rate": 4.386773950556931e-05,
      "loss": 0.1188,
      "num_input_tokens_seen": 244992,
      "step": 305
    },
    {
      "epoch": 0.2512315270935961,
      "grad_norm": 20.570117950439453,
      "learning_rate": 4.381037272239311e-05,
      "loss": 0.9178,
      "num_input_tokens_seen": 245760,
      "step": 306
    },
    {
      "epoch": 0.25205254515599346,
      "grad_norm": 3.941652774810791,
      "learning_rate": 4.375277674076149e-05,
      "loss": 0.1803,
      "num_input_tokens_seen": 246528,
      "step": 307
    },
    {
      "epoch": 0.25287356321839083,
      "grad_norm": 8.919632911682129,
      "learning_rate": 4.36949522624633e-05,
      "loss": 0.1792,
      "num_input_tokens_seen": 247296,
      "step": 308
    },
    {
      "epoch": 0.2536945812807882,
      "grad_norm": 3.797603130340576,
      "learning_rate": 4.363689999207156e-05,
      "loss": 0.0936,
      "num_input_tokens_seen": 248064,
      "step": 309
    },
    {
      "epoch": 0.2545155993431856,
      "grad_norm": 15.157559394836426,
      "learning_rate": 4.357862063693486e-05,
      "loss": 0.3733,
      "num_input_tokens_seen": 248832,
      "step": 310
    },
    {
      "epoch": 0.25533661740558294,
      "grad_norm": 17.18460464477539,
      "learning_rate": 4.352011490716875e-05,
      "loss": 0.5051,
      "num_input_tokens_seen": 249600,
      "step": 311
    },
    {
      "epoch": 0.2561576354679803,
      "grad_norm": 8.060176849365234,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 0.2054,
      "num_input_tokens_seen": 250368,
      "step": 312
    },
    {
      "epoch": 0.2569786535303777,
      "grad_norm": 38.85480880737305,
      "learning_rate": 4.3402427177993366e-05,
      "loss": 1.7995,
      "num_input_tokens_seen": 251136,
      "step": 313
    },
    {
      "epoch": 0.25779967159277506,
      "grad_norm": 32.362125396728516,
      "learning_rate": 4.334324661257191e-05,
      "loss": 1.0737,
      "num_input_tokens_seen": 252032,
      "step": 314
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 10.02135944366455,
      "learning_rate": 4.3283842540479264e-05,
      "loss": 0.1476,
      "num_input_tokens_seen": 252800,
      "step": 315
    },
    {
      "epoch": 0.2594417077175698,
      "grad_norm": 26.12697410583496,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 1.6197,
      "num_input_tokens_seen": 253568,
      "step": 316
    },
    {
      "epoch": 0.2602627257799672,
      "grad_norm": 4.69610595703125,
      "learning_rate": 4.31643667742744e-05,
      "loss": 0.0618,
      "num_input_tokens_seen": 254336,
      "step": 317
    },
    {
      "epoch": 0.26108374384236455,
      "grad_norm": 10.722268104553223,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 0.3687,
      "num_input_tokens_seen": 255104,
      "step": 318
    },
    {
      "epoch": 0.2619047619047619,
      "grad_norm": 4.12204647064209,
      "learning_rate": 4.304400570245906e-05,
      "loss": 0.054,
      "num_input_tokens_seen": 255872,
      "step": 319
    },
    {
      "epoch": 0.2627257799671593,
      "grad_norm": 19.22003936767578,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 0.7348,
      "num_input_tokens_seen": 256768,
      "step": 320
    },
    {
      "epoch": 0.26354679802955666,
      "grad_norm": 16.952903747558594,
      "learning_rate": 4.292276519126207e-05,
      "loss": 0.6984,
      "num_input_tokens_seen": 257536,
      "step": 321
    },
    {
      "epoch": 0.26436781609195403,
      "grad_norm": 29.687686920166016,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 2.0646,
      "num_input_tokens_seen": 258432,
      "step": 322
    },
    {
      "epoch": 0.2651888341543514,
      "grad_norm": 30.467864990234375,
      "learning_rate": 4.280065114977492e-05,
      "loss": 1.0563,
      "num_input_tokens_seen": 259328,
      "step": 323
    },
    {
      "epoch": 0.2660098522167488,
      "grad_norm": 16.394628524780273,
      "learning_rate": 4.273926841341302e-05,
      "loss": 1.344,
      "num_input_tokens_seen": 260096,
      "step": 324
    },
    {
      "epoch": 0.26683087027914615,
      "grad_norm": 10.679646492004395,
      "learning_rate": 4.267766952966369e-05,
      "loss": 0.1621,
      "num_input_tokens_seen": 260864,
      "step": 325
    },
    {
      "epoch": 0.2676518883415435,
      "grad_norm": 17.523326873779297,
      "learning_rate": 4.261585524908987e-05,
      "loss": 0.3619,
      "num_input_tokens_seen": 261632,
      "step": 326
    },
    {
      "epoch": 0.2684729064039409,
      "grad_norm": 16.214935302734375,
      "learning_rate": 4.2553826324879064e-05,
      "loss": 1.0034,
      "num_input_tokens_seen": 262400,
      "step": 327
    },
    {
      "epoch": 0.26929392446633826,
      "grad_norm": 26.3190860748291,
      "learning_rate": 4.249158351283414e-05,
      "loss": 1.0852,
      "num_input_tokens_seen": 263168,
      "step": 328
    },
    {
      "epoch": 0.27011494252873564,
      "grad_norm": 9.587127685546875,
      "learning_rate": 4.242912757136412e-05,
      "loss": 0.2709,
      "num_input_tokens_seen": 263936,
      "step": 329
    },
    {
      "epoch": 0.270935960591133,
      "grad_norm": 16.599681854248047,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 0.6543,
      "num_input_tokens_seen": 264704,
      "step": 330
    },
    {
      "epoch": 0.2717569786535304,
      "grad_norm": 9.128355026245117,
      "learning_rate": 4.230357934676017e-05,
      "loss": 0.5513,
      "num_input_tokens_seen": 265472,
      "step": 331
    },
    {
      "epoch": 0.27257799671592775,
      "grad_norm": 9.403339385986328,
      "learning_rate": 4.224048859339175e-05,
      "loss": 0.2865,
      "num_input_tokens_seen": 266240,
      "step": 332
    },
    {
      "epoch": 0.2733990147783251,
      "grad_norm": 6.979831695556641,
      "learning_rate": 4.2177187770110576e-05,
      "loss": 0.3451,
      "num_input_tokens_seen": 267008,
      "step": 333
    },
    {
      "epoch": 0.2742200328407225,
      "grad_norm": 5.133752822875977,
      "learning_rate": 4.211367764821722e-05,
      "loss": 0.1597,
      "num_input_tokens_seen": 267904,
      "step": 334
    },
    {
      "epoch": 0.27504105090311987,
      "grad_norm": 19.319869995117188,
      "learning_rate": 4.2049959001562464e-05,
      "loss": 0.9741,
      "num_input_tokens_seen": 268672,
      "step": 335
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 17.12908935546875,
      "learning_rate": 4.198603260653792e-05,
      "loss": 0.5517,
      "num_input_tokens_seen": 269568,
      "step": 336
    },
    {
      "epoch": 0.2766830870279146,
      "grad_norm": 14.07297420501709,
      "learning_rate": 4.192189924206652e-05,
      "loss": 0.4485,
      "num_input_tokens_seen": 270464,
      "step": 337
    },
    {
      "epoch": 0.277504105090312,
      "grad_norm": 16.318143844604492,
      "learning_rate": 4.185755968959308e-05,
      "loss": 0.6271,
      "num_input_tokens_seen": 271232,
      "step": 338
    },
    {
      "epoch": 0.27832512315270935,
      "grad_norm": 6.7434916496276855,
      "learning_rate": 4.179301473307476e-05,
      "loss": 0.2154,
      "num_input_tokens_seen": 272000,
      "step": 339
    },
    {
      "epoch": 0.2791461412151067,
      "grad_norm": 20.133495330810547,
      "learning_rate": 4.172826515897146e-05,
      "loss": 0.8087,
      "num_input_tokens_seen": 272768,
      "step": 340
    },
    {
      "epoch": 0.2799671592775041,
      "grad_norm": 10.345876693725586,
      "learning_rate": 4.166331175623631e-05,
      "loss": 0.3946,
      "num_input_tokens_seen": 273664,
      "step": 341
    },
    {
      "epoch": 0.28078817733990147,
      "grad_norm": 6.519314289093018,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 0.21,
      "num_input_tokens_seen": 274560,
      "step": 342
    },
    {
      "epoch": 0.28160919540229884,
      "grad_norm": 15.985962867736816,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 0.9427,
      "num_input_tokens_seen": 275328,
      "step": 343
    },
    {
      "epoch": 0.2824302134646962,
      "grad_norm": 5.844092845916748,
      "learning_rate": 4.146723650296701e-05,
      "loss": 0.1929,
      "num_input_tokens_seen": 276096,
      "step": 344
    },
    {
      "epoch": 0.2832512315270936,
      "grad_norm": 10.407934188842773,
      "learning_rate": 4.140147572476268e-05,
      "loss": 0.3328,
      "num_input_tokens_seen": 276992,
      "step": 345
    },
    {
      "epoch": 0.28407224958949095,
      "grad_norm": 18.125703811645508,
      "learning_rate": 4.133551509975264e-05,
      "loss": 0.9104,
      "num_input_tokens_seen": 277760,
      "step": 346
    },
    {
      "epoch": 0.2848932676518883,
      "grad_norm": 7.787192344665527,
      "learning_rate": 4.1269355431646274e-05,
      "loss": 0.2194,
      "num_input_tokens_seen": 278656,
      "step": 347
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 7.073241710662842,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 0.1908,
      "num_input_tokens_seen": 279424,
      "step": 348
    },
    {
      "epoch": 0.28653530377668307,
      "grad_norm": 13.057427406311035,
      "learning_rate": 4.113644219309877e-05,
      "loss": 0.6951,
      "num_input_tokens_seen": 280192,
      "step": 349
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 15.07486629486084,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 0.7738,
      "num_input_tokens_seen": 281088,
      "step": 350
    },
    {
      "epoch": 0.2881773399014778,
      "grad_norm": 13.129620552062988,
      "learning_rate": 4.100274248712389e-05,
      "loss": 0.8444,
      "num_input_tokens_seen": 281984,
      "step": 351
    },
    {
      "epoch": 0.2889983579638752,
      "grad_norm": 15.533112525939941,
      "learning_rate": 4.093559974371725e-05,
      "loss": 1.0358,
      "num_input_tokens_seen": 282880,
      "step": 352
    },
    {
      "epoch": 0.28981937602627256,
      "grad_norm": 10.90745735168457,
      "learning_rate": 4.086826283005669e-05,
      "loss": 0.682,
      "num_input_tokens_seen": 283648,
      "step": 353
    },
    {
      "epoch": 0.29064039408866993,
      "grad_norm": 13.098767280578613,
      "learning_rate": 4.080073256662127e-05,
      "loss": 0.7354,
      "num_input_tokens_seen": 284416,
      "step": 354
    },
    {
      "epoch": 0.2914614121510673,
      "grad_norm": 12.45616340637207,
      "learning_rate": 4.073300977624594e-05,
      "loss": 0.4542,
      "num_input_tokens_seen": 285184,
      "step": 355
    },
    {
      "epoch": 0.2922824302134647,
      "grad_norm": 46.87623596191406,
      "learning_rate": 4.066509528411152e-05,
      "loss": 1.096,
      "num_input_tokens_seen": 286080,
      "step": 356
    },
    {
      "epoch": 0.29310344827586204,
      "grad_norm": 22.685047149658203,
      "learning_rate": 4.059698991773466e-05,
      "loss": 1.409,
      "num_input_tokens_seen": 286848,
      "step": 357
    },
    {
      "epoch": 0.2939244663382594,
      "grad_norm": 7.4964189529418945,
      "learning_rate": 4.052869450695776e-05,
      "loss": 0.2906,
      "num_input_tokens_seen": 287616,
      "step": 358
    },
    {
      "epoch": 0.2947454844006568,
      "grad_norm": 18.634550094604492,
      "learning_rate": 4.046020988393885e-05,
      "loss": 0.9907,
      "num_input_tokens_seen": 288384,
      "step": 359
    },
    {
      "epoch": 0.2955665024630542,
      "grad_norm": 18.242691040039062,
      "learning_rate": 4.039153688314145e-05,
      "loss": 0.7038,
      "num_input_tokens_seen": 289152,
      "step": 360
    },
    {
      "epoch": 0.2963875205254516,
      "grad_norm": 13.372392654418945,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 0.6027,
      "num_input_tokens_seen": 289920,
      "step": 361
    },
    {
      "epoch": 0.29720853858784896,
      "grad_norm": 14.725762367248535,
      "learning_rate": 4.02536290975317e-05,
      "loss": 1.4103,
      "num_input_tokens_seen": 290688,
      "step": 362
    },
    {
      "epoch": 0.29802955665024633,
      "grad_norm": 9.91478443145752,
      "learning_rate": 4.018439599308217e-05,
      "loss": 0.4332,
      "num_input_tokens_seen": 291456,
      "step": 363
    },
    {
      "epoch": 0.2988505747126437,
      "grad_norm": 8.490067481994629,
      "learning_rate": 4.011497787155938e-05,
      "loss": 0.629,
      "num_input_tokens_seen": 292608,
      "step": 364
    },
    {
      "epoch": 0.2996715927750411,
      "grad_norm": 21.514741897583008,
      "learning_rate": 4.0045375578801214e-05,
      "loss": 0.9391,
      "num_input_tokens_seen": 293376,
      "step": 365
    },
    {
      "epoch": 0.30049261083743845,
      "grad_norm": 11.72239875793457,
      "learning_rate": 3.997558996288965e-05,
      "loss": 0.417,
      "num_input_tokens_seen": 294144,
      "step": 366
    },
    {
      "epoch": 0.3013136288998358,
      "grad_norm": 15.312572479248047,
      "learning_rate": 3.99056218741404e-05,
      "loss": 1.0804,
      "num_input_tokens_seen": 294912,
      "step": 367
    },
    {
      "epoch": 0.3021346469622332,
      "grad_norm": 12.317953109741211,
      "learning_rate": 3.983547216509254e-05,
      "loss": 0.5169,
      "num_input_tokens_seen": 295680,
      "step": 368
    },
    {
      "epoch": 0.30295566502463056,
      "grad_norm": 5.969217777252197,
      "learning_rate": 3.976514169049814e-05,
      "loss": 0.2494,
      "num_input_tokens_seen": 296448,
      "step": 369
    },
    {
      "epoch": 0.30377668308702793,
      "grad_norm": 8.747509002685547,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.594,
      "num_input_tokens_seen": 297216,
      "step": 370
    },
    {
      "epoch": 0.3045977011494253,
      "grad_norm": 6.961119651794434,
      "learning_rate": 3.962394187468039e-05,
      "loss": 0.3145,
      "num_input_tokens_seen": 297984,
      "step": 371
    },
    {
      "epoch": 0.3054187192118227,
      "grad_norm": 10.617766380310059,
      "learning_rate": 3.955307425393224e-05,
      "loss": 0.3517,
      "num_input_tokens_seen": 298752,
      "step": 372
    },
    {
      "epoch": 0.30623973727422005,
      "grad_norm": 24.175094604492188,
      "learning_rate": 3.948202930856697e-05,
      "loss": 1.0669,
      "num_input_tokens_seen": 299520,
      "step": 373
    },
    {
      "epoch": 0.3070607553366174,
      "grad_norm": 18.550830841064453,
      "learning_rate": 3.941080790424484e-05,
      "loss": 0.5778,
      "num_input_tokens_seen": 300288,
      "step": 374
    },
    {
      "epoch": 0.3078817733990148,
      "grad_norm": 2.4151010513305664,
      "learning_rate": 3.933941090877615e-05,
      "loss": 0.0648,
      "num_input_tokens_seen": 301056,
      "step": 375
    },
    {
      "epoch": 0.30870279146141216,
      "grad_norm": 8.833959579467773,
      "learning_rate": 3.92678391921108e-05,
      "loss": 0.4373,
      "num_input_tokens_seen": 301824,
      "step": 376
    },
    {
      "epoch": 0.30952380952380953,
      "grad_norm": 9.37648868560791,
      "learning_rate": 3.919609362632753e-05,
      "loss": 0.5868,
      "num_input_tokens_seen": 302592,
      "step": 377
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 7.673788070678711,
      "learning_rate": 3.912417508562345e-05,
      "loss": 0.6777,
      "num_input_tokens_seen": 303360,
      "step": 378
    },
    {
      "epoch": 0.3111658456486043,
      "grad_norm": 22.17908477783203,
      "learning_rate": 3.905208444630327e-05,
      "loss": 0.4595,
      "num_input_tokens_seen": 304128,
      "step": 379
    },
    {
      "epoch": 0.31198686371100165,
      "grad_norm": 13.489315032958984,
      "learning_rate": 3.897982258676867e-05,
      "loss": 0.9641,
      "num_input_tokens_seen": 304896,
      "step": 380
    },
    {
      "epoch": 0.312807881773399,
      "grad_norm": 13.348580360412598,
      "learning_rate": 3.8907390387507625e-05,
      "loss": 0.7345,
      "num_input_tokens_seen": 305792,
      "step": 381
    },
    {
      "epoch": 0.3136288998357964,
      "grad_norm": 9.545757293701172,
      "learning_rate": 3.883478873108361e-05,
      "loss": 0.5474,
      "num_input_tokens_seen": 306688,
      "step": 382
    },
    {
      "epoch": 0.31444991789819376,
      "grad_norm": 9.845086097717285,
      "learning_rate": 3.8762018502124894e-05,
      "loss": 0.6224,
      "num_input_tokens_seen": 307456,
      "step": 383
    },
    {
      "epoch": 0.31527093596059114,
      "grad_norm": 8.997702598571777,
      "learning_rate": 3.868908058731376e-05,
      "loss": 0.2838,
      "num_input_tokens_seen": 308224,
      "step": 384
    },
    {
      "epoch": 0.3160919540229885,
      "grad_norm": 17.670082092285156,
      "learning_rate": 3.861597587537568e-05,
      "loss": 0.9703,
      "num_input_tokens_seen": 308992,
      "step": 385
    },
    {
      "epoch": 0.3169129720853859,
      "grad_norm": 6.1378583908081055,
      "learning_rate": 3.85427052570685e-05,
      "loss": 0.2809,
      "num_input_tokens_seen": 309760,
      "step": 386
    },
    {
      "epoch": 0.31773399014778325,
      "grad_norm": 11.585895538330078,
      "learning_rate": 3.8469269625171576e-05,
      "loss": 0.4064,
      "num_input_tokens_seen": 310656,
      "step": 387
    },
    {
      "epoch": 0.3185550082101806,
      "grad_norm": 12.077269554138184,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 0.4019,
      "num_input_tokens_seen": 311424,
      "step": 388
    },
    {
      "epoch": 0.319376026272578,
      "grad_norm": 4.393151760101318,
      "learning_rate": 3.832190690176825e-05,
      "loss": 0.1581,
      "num_input_tokens_seen": 312192,
      "step": 389
    },
    {
      "epoch": 0.32019704433497537,
      "grad_norm": 4.9882659912109375,
      "learning_rate": 3.824798160583012e-05,
      "loss": 0.1539,
      "num_input_tokens_seen": 312960,
      "step": 390
    },
    {
      "epoch": 0.32101806239737274,
      "grad_norm": 5.363707065582275,
      "learning_rate": 3.8173894887416945e-05,
      "loss": 0.1167,
      "num_input_tokens_seen": 313728,
      "step": 391
    },
    {
      "epoch": 0.3218390804597701,
      "grad_norm": 8.677606582641602,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 0.2681,
      "num_input_tokens_seen": 314496,
      "step": 392
    },
    {
      "epoch": 0.3226600985221675,
      "grad_norm": 25.432838439941406,
      "learning_rate": 3.802524079601442e-05,
      "loss": 0.4203,
      "num_input_tokens_seen": 315264,
      "step": 393
    },
    {
      "epoch": 0.32348111658456485,
      "grad_norm": 11.288752555847168,
      "learning_rate": 3.795067523432826e-05,
      "loss": 0.3904,
      "num_input_tokens_seen": 316032,
      "step": 394
    },
    {
      "epoch": 0.3243021346469622,
      "grad_norm": 16.02510643005371,
      "learning_rate": 3.787595187275136e-05,
      "loss": 0.3294,
      "num_input_tokens_seen": 316800,
      "step": 395
    },
    {
      "epoch": 0.3251231527093596,
      "grad_norm": 22.17888069152832,
      "learning_rate": 3.780107162176429e-05,
      "loss": 0.5569,
      "num_input_tokens_seen": 317568,
      "step": 396
    },
    {
      "epoch": 0.32594417077175697,
      "grad_norm": 10.702160835266113,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 0.3944,
      "num_input_tokens_seen": 318336,
      "step": 397
    },
    {
      "epoch": 0.32676518883415434,
      "grad_norm": 8.448606491088867,
      "learning_rate": 3.765084410302909e-05,
      "loss": 0.2557,
      "num_input_tokens_seen": 319104,
      "step": 398
    },
    {
      "epoch": 0.3275862068965517,
      "grad_norm": 16.599193572998047,
      "learning_rate": 3.757549866575588e-05,
      "loss": 0.4726,
      "num_input_tokens_seen": 319872,
      "step": 399
    },
    {
      "epoch": 0.3284072249589491,
      "grad_norm": 14.370526313781738,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.7517,
      "num_input_tokens_seen": 320640,
      "step": 400
    },
    {
      "epoch": 0.32922824302134646,
      "grad_norm": 15.039939880371094,
      "learning_rate": 3.742434902568889e-05,
      "loss": 0.5022,
      "num_input_tokens_seen": 321536,
      "step": 401
    },
    {
      "epoch": 0.33004926108374383,
      "grad_norm": 20.47303581237793,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 0.4435,
      "num_input_tokens_seen": 322304,
      "step": 402
    },
    {
      "epoch": 0.3308702791461412,
      "grad_norm": 13.447274208068848,
      "learning_rate": 3.727259384037852e-05,
      "loss": 0.5583,
      "num_input_tokens_seen": 323072,
      "step": 403
    },
    {
      "epoch": 0.33169129720853857,
      "grad_norm": 15.267746925354004,
      "learning_rate": 3.719649147846832e-05,
      "loss": 0.6558,
      "num_input_tokens_seen": 323840,
      "step": 404
    },
    {
      "epoch": 0.33251231527093594,
      "grad_norm": 11.476744651794434,
      "learning_rate": 3.712024050615843e-05,
      "loss": 0.1278,
      "num_input_tokens_seen": 324608,
      "step": 405
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 20.040441513061523,
      "learning_rate": 3.704384185254288e-05,
      "loss": 1.2576,
      "num_input_tokens_seen": 325376,
      "step": 406
    },
    {
      "epoch": 0.3341543513957307,
      "grad_norm": 17.300575256347656,
      "learning_rate": 3.696729644851518e-05,
      "loss": 0.7133,
      "num_input_tokens_seen": 326144,
      "step": 407
    },
    {
      "epoch": 0.33497536945812806,
      "grad_norm": 13.009435653686523,
      "learning_rate": 3.689060522675689e-05,
      "loss": 1.3828,
      "num_input_tokens_seen": 326912,
      "step": 408
    },
    {
      "epoch": 0.33579638752052543,
      "grad_norm": 17.572885513305664,
      "learning_rate": 3.681376912172636e-05,
      "loss": 0.3839,
      "num_input_tokens_seen": 327808,
      "step": 409
    },
    {
      "epoch": 0.3366174055829228,
      "grad_norm": 15.543412208557129,
      "learning_rate": 3.673678906964727e-05,
      "loss": 0.4658,
      "num_input_tokens_seen": 328704,
      "step": 410
    },
    {
      "epoch": 0.3374384236453202,
      "grad_norm": 17.62051773071289,
      "learning_rate": 3.665966600849728e-05,
      "loss": 0.3022,
      "num_input_tokens_seen": 329600,
      "step": 411
    },
    {
      "epoch": 0.33825944170771755,
      "grad_norm": 10.666467666625977,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 0.413,
      "num_input_tokens_seen": 330368,
      "step": 412
    },
    {
      "epoch": 0.3390804597701149,
      "grad_norm": 13.214375495910645,
      "learning_rate": 3.6504994619596294e-05,
      "loss": 0.3849,
      "num_input_tokens_seen": 331136,
      "step": 413
    },
    {
      "epoch": 0.3399014778325123,
      "grad_norm": 8.363377571105957,
      "learning_rate": 3.642744817646736e-05,
      "loss": 0.0808,
      "num_input_tokens_seen": 331904,
      "step": 414
    },
    {
      "epoch": 0.34072249589490966,
      "grad_norm": 30.252437591552734,
      "learning_rate": 3.634976249348867e-05,
      "loss": 1.8844,
      "num_input_tokens_seen": 332672,
      "step": 415
    },
    {
      "epoch": 0.3415435139573071,
      "grad_norm": 14.573392868041992,
      "learning_rate": 3.627193851723577e-05,
      "loss": 0.7118,
      "num_input_tokens_seen": 333440,
      "step": 416
    },
    {
      "epoch": 0.34236453201970446,
      "grad_norm": 3.3350460529327393,
      "learning_rate": 3.619397719596924e-05,
      "loss": 0.0828,
      "num_input_tokens_seen": 334208,
      "step": 417
    },
    {
      "epoch": 0.34318555008210183,
      "grad_norm": 11.619067192077637,
      "learning_rate": 3.611587947962319e-05,
      "loss": 0.4372,
      "num_input_tokens_seen": 334976,
      "step": 418
    },
    {
      "epoch": 0.3440065681444992,
      "grad_norm": 10.34323787689209,
      "learning_rate": 3.603764631979363e-05,
      "loss": 0.2999,
      "num_input_tokens_seen": 335872,
      "step": 419
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 8.995223045349121,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 0.278,
      "num_input_tokens_seen": 336640,
      "step": 420
    },
    {
      "epoch": 0.34564860426929395,
      "grad_norm": 3.174896717071533,
      "learning_rate": 3.588077748430819e-05,
      "loss": 0.0762,
      "num_input_tokens_seen": 337408,
      "step": 421
    },
    {
      "epoch": 0.3464696223316913,
      "grad_norm": 14.836174011230469,
      "learning_rate": 3.580214372004956e-05,
      "loss": 1.3153,
      "num_input_tokens_seen": 338176,
      "step": 422
    },
    {
      "epoch": 0.3472906403940887,
      "grad_norm": 14.023963928222656,
      "learning_rate": 3.572337833507865e-05,
      "loss": 0.681,
      "num_input_tokens_seen": 338944,
      "step": 423
    },
    {
      "epoch": 0.34811165845648606,
      "grad_norm": 11.039081573486328,
      "learning_rate": 3.564448228912682e-05,
      "loss": 0.5736,
      "num_input_tokens_seen": 339712,
      "step": 424
    },
    {
      "epoch": 0.34893267651888343,
      "grad_norm": 11.17652702331543,
      "learning_rate": 3.556545654351749e-05,
      "loss": 0.4712,
      "num_input_tokens_seen": 340608,
      "step": 425
    },
    {
      "epoch": 0.3497536945812808,
      "grad_norm": 13.669888496398926,
      "learning_rate": 3.548630206115443e-05,
      "loss": 0.5325,
      "num_input_tokens_seen": 341376,
      "step": 426
    },
    {
      "epoch": 0.3505747126436782,
      "grad_norm": 19.009017944335938,
      "learning_rate": 3.540701980651003e-05,
      "loss": 0.696,
      "num_input_tokens_seen": 342144,
      "step": 427
    },
    {
      "epoch": 0.35139573070607555,
      "grad_norm": 20.48150634765625,
      "learning_rate": 3.532761074561355e-05,
      "loss": 1.5897,
      "num_input_tokens_seen": 342912,
      "step": 428
    },
    {
      "epoch": 0.3522167487684729,
      "grad_norm": 12.321639060974121,
      "learning_rate": 3.524807584603932e-05,
      "loss": 0.5499,
      "num_input_tokens_seen": 343680,
      "step": 429
    },
    {
      "epoch": 0.3530377668308703,
      "grad_norm": 12.981033325195312,
      "learning_rate": 3.516841607689501e-05,
      "loss": 0.2037,
      "num_input_tokens_seen": 344448,
      "step": 430
    },
    {
      "epoch": 0.35385878489326766,
      "grad_norm": 16.57249641418457,
      "learning_rate": 3.5088632408809755e-05,
      "loss": 0.9797,
      "num_input_tokens_seen": 345216,
      "step": 431
    },
    {
      "epoch": 0.35467980295566504,
      "grad_norm": 19.519733428955078,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 0.3956,
      "num_input_tokens_seen": 345984,
      "step": 432
    },
    {
      "epoch": 0.3555008210180624,
      "grad_norm": 25.286235809326172,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 0.7845,
      "num_input_tokens_seen": 346880,
      "step": 433
    },
    {
      "epoch": 0.3563218390804598,
      "grad_norm": 5.6482834815979,
      "learning_rate": 3.484854773977378e-05,
      "loss": 0.1543,
      "num_input_tokens_seen": 347648,
      "step": 434
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 10.60593318939209,
      "learning_rate": 3.476827821223184e-05,
      "loss": 0.44,
      "num_input_tokens_seen": 348416,
      "step": 435
    },
    {
      "epoch": 0.3579638752052545,
      "grad_norm": 7.402892589569092,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 0.5042,
      "num_input_tokens_seen": 349184,
      "step": 436
    },
    {
      "epoch": 0.3587848932676519,
      "grad_norm": 14.556804656982422,
      "learning_rate": 3.460738306649509e-05,
      "loss": 0.502,
      "num_input_tokens_seen": 349952,
      "step": 437
    },
    {
      "epoch": 0.35960591133004927,
      "grad_norm": 23.66755485534668,
      "learning_rate": 3.452675940875686e-05,
      "loss": 0.7873,
      "num_input_tokens_seen": 350720,
      "step": 438
    },
    {
      "epoch": 0.36042692939244664,
      "grad_norm": 9.191009521484375,
      "learning_rate": 3.444601967046168e-05,
      "loss": 0.3147,
      "num_input_tokens_seen": 351488,
      "step": 439
    },
    {
      "epoch": 0.361247947454844,
      "grad_norm": 10.38216495513916,
      "learning_rate": 3.436516483539781e-05,
      "loss": 0.2115,
      "num_input_tokens_seen": 352384,
      "step": 440
    },
    {
      "epoch": 0.3620689655172414,
      "grad_norm": 8.338563919067383,
      "learning_rate": 3.428419588875588e-05,
      "loss": 0.2118,
      "num_input_tokens_seen": 353152,
      "step": 441
    },
    {
      "epoch": 0.36288998357963875,
      "grad_norm": 8.886871337890625,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 0.3823,
      "num_input_tokens_seen": 353920,
      "step": 442
    },
    {
      "epoch": 0.3637110016420361,
      "grad_norm": 8.639490127563477,
      "learning_rate": 3.412191960844049e-05,
      "loss": 0.4603,
      "num_input_tokens_seen": 354688,
      "step": 443
    },
    {
      "epoch": 0.3645320197044335,
      "grad_norm": 12.991827964782715,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 0.5776,
      "num_input_tokens_seen": 355584,
      "step": 444
    },
    {
      "epoch": 0.36535303776683087,
      "grad_norm": 15.306493759155273,
      "learning_rate": 3.39591987386325e-05,
      "loss": 1.3285,
      "num_input_tokens_seen": 356480,
      "step": 445
    },
    {
      "epoch": 0.36617405582922824,
      "grad_norm": 11.671769142150879,
      "learning_rate": 3.387767406020343e-05,
      "loss": 0.5183,
      "num_input_tokens_seen": 357248,
      "step": 446
    },
    {
      "epoch": 0.3669950738916256,
      "grad_norm": 17.646533966064453,
      "learning_rate": 3.3796041210117546e-05,
      "loss": 0.6839,
      "num_input_tokens_seen": 358016,
      "step": 447
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 9.208927154541016,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 0.6084,
      "num_input_tokens_seen": 358784,
      "step": 448
    },
    {
      "epoch": 0.36863711001642036,
      "grad_norm": 8.16750431060791,
      "learning_rate": 3.363245497496337e-05,
      "loss": 0.2544,
      "num_input_tokens_seen": 359552,
      "step": 449
    },
    {
      "epoch": 0.3694581280788177,
      "grad_norm": 5.812069416046143,
      "learning_rate": 3.355050358314172e-05,
      "loss": 0.1841,
      "num_input_tokens_seen": 360320,
      "step": 450
    },
    {
      "epoch": 0.3702791461412151,
      "grad_norm": 8.219280242919922,
      "learning_rate": 3.346844800613229e-05,
      "loss": 0.2657,
      "num_input_tokens_seen": 361088,
      "step": 451
    },
    {
      "epoch": 0.37110016420361247,
      "grad_norm": 7.764272212982178,
      "learning_rate": 3.338628924375638e-05,
      "loss": 0.2782,
      "num_input_tokens_seen": 361856,
      "step": 452
    },
    {
      "epoch": 0.37192118226600984,
      "grad_norm": 3.8795723915100098,
      "learning_rate": 3.330402829709258e-05,
      "loss": 0.093,
      "num_input_tokens_seen": 362624,
      "step": 453
    },
    {
      "epoch": 0.3727422003284072,
      "grad_norm": 11.553173065185547,
      "learning_rate": 3.322166616846458e-05,
      "loss": 0.51,
      "num_input_tokens_seen": 363392,
      "step": 454
    },
    {
      "epoch": 0.3735632183908046,
      "grad_norm": 16.355955123901367,
      "learning_rate": 3.313920386142892e-05,
      "loss": 0.9287,
      "num_input_tokens_seen": 364160,
      "step": 455
    },
    {
      "epoch": 0.37438423645320196,
      "grad_norm": 20.230592727661133,
      "learning_rate": 3.305664238076278e-05,
      "loss": 0.7905,
      "num_input_tokens_seen": 364928,
      "step": 456
    },
    {
      "epoch": 0.37520525451559933,
      "grad_norm": 14.314680099487305,
      "learning_rate": 3.2973982732451755e-05,
      "loss": 0.775,
      "num_input_tokens_seen": 365824,
      "step": 457
    },
    {
      "epoch": 0.3760262725779967,
      "grad_norm": 14.982686996459961,
      "learning_rate": 3.289122592367757e-05,
      "loss": 0.8667,
      "num_input_tokens_seen": 366592,
      "step": 458
    },
    {
      "epoch": 0.3768472906403941,
      "grad_norm": 13.449766159057617,
      "learning_rate": 3.2808372962805816e-05,
      "loss": 0.5183,
      "num_input_tokens_seen": 367488,
      "step": 459
    },
    {
      "epoch": 0.37766830870279144,
      "grad_norm": 2.653524398803711,
      "learning_rate": 3.272542485937369e-05,
      "loss": 0.0602,
      "num_input_tokens_seen": 368256,
      "step": 460
    },
    {
      "epoch": 0.3784893267651888,
      "grad_norm": 16.297109603881836,
      "learning_rate": 3.264238262407764e-05,
      "loss": 0.9359,
      "num_input_tokens_seen": 369024,
      "step": 461
    },
    {
      "epoch": 0.3793103448275862,
      "grad_norm": 21.012592315673828,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 1.3211,
      "num_input_tokens_seen": 369792,
      "step": 462
    },
    {
      "epoch": 0.38013136288998356,
      "grad_norm": 15.423666000366211,
      "learning_rate": 3.247601980640217e-05,
      "loss": 0.7997,
      "num_input_tokens_seen": 370560,
      "step": 463
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 21.40675926208496,
      "learning_rate": 3.239270125110117e-05,
      "loss": 1.0075,
      "num_input_tokens_seen": 371456,
      "step": 464
    },
    {
      "epoch": 0.3817733990147783,
      "grad_norm": 6.096110820770264,
      "learning_rate": 3.230929261806842e-05,
      "loss": 0.3432,
      "num_input_tokens_seen": 372224,
      "step": 465
    },
    {
      "epoch": 0.3825944170771757,
      "grad_norm": 12.563095092773438,
      "learning_rate": 3.222579492361179e-05,
      "loss": 0.6332,
      "num_input_tokens_seen": 372992,
      "step": 466
    },
    {
      "epoch": 0.38341543513957305,
      "grad_norm": 16.183549880981445,
      "learning_rate": 3.214220918512434e-05,
      "loss": 1.0758,
      "num_input_tokens_seen": 373760,
      "step": 467
    },
    {
      "epoch": 0.3842364532019704,
      "grad_norm": 20.973430633544922,
      "learning_rate": 3.205853642107192e-05,
      "loss": 1.2318,
      "num_input_tokens_seen": 374528,
      "step": 468
    },
    {
      "epoch": 0.3850574712643678,
      "grad_norm": 14.119485855102539,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 1.4544,
      "num_input_tokens_seen": 375296,
      "step": 469
    },
    {
      "epoch": 0.38587848932676516,
      "grad_norm": 3.57973575592041,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 0.1144,
      "num_input_tokens_seen": 376064,
      "step": 470
    },
    {
      "epoch": 0.3866995073891626,
      "grad_norm": 14.69619083404541,
      "learning_rate": 3.180700617601436e-05,
      "loss": 0.692,
      "num_input_tokens_seen": 376832,
      "step": 471
    },
    {
      "epoch": 0.38752052545155996,
      "grad_norm": 20.37702178955078,
      "learning_rate": 3.172299551538164e-05,
      "loss": 0.9497,
      "num_input_tokens_seen": 377600,
      "step": 472
    },
    {
      "epoch": 0.38834154351395733,
      "grad_norm": 7.477658748626709,
      "learning_rate": 3.163890293717022e-05,
      "loss": 0.15,
      "num_input_tokens_seen": 378368,
      "step": 473
    },
    {
      "epoch": 0.3891625615763547,
      "grad_norm": 6.416485786437988,
      "learning_rate": 3.155472946602162e-05,
      "loss": 0.2371,
      "num_input_tokens_seen": 379136,
      "step": 474
    },
    {
      "epoch": 0.3899835796387521,
      "grad_norm": 14.283238410949707,
      "learning_rate": 3.147047612756302e-05,
      "loss": 1.0928,
      "num_input_tokens_seen": 379904,
      "step": 475
    },
    {
      "epoch": 0.39080459770114945,
      "grad_norm": 13.471212387084961,
      "learning_rate": 3.138614394839476e-05,
      "loss": 0.6384,
      "num_input_tokens_seen": 380672,
      "step": 476
    },
    {
      "epoch": 0.3916256157635468,
      "grad_norm": 5.1565327644348145,
      "learning_rate": 3.130173395607785e-05,
      "loss": 0.1624,
      "num_input_tokens_seen": 381440,
      "step": 477
    },
    {
      "epoch": 0.3924466338259442,
      "grad_norm": 11.363812446594238,
      "learning_rate": 3.121724717912138e-05,
      "loss": 0.396,
      "num_input_tokens_seen": 382208,
      "step": 478
    },
    {
      "epoch": 0.39326765188834156,
      "grad_norm": 10.164100646972656,
      "learning_rate": 3.1132684646970064e-05,
      "loss": 0.3746,
      "num_input_tokens_seen": 382976,
      "step": 479
    },
    {
      "epoch": 0.39408866995073893,
      "grad_norm": 9.290513038635254,
      "learning_rate": 3.104804738999169e-05,
      "loss": 0.7695,
      "num_input_tokens_seen": 383744,
      "step": 480
    },
    {
      "epoch": 0.3949096880131363,
      "grad_norm": 7.348825931549072,
      "learning_rate": 3.0963336439464526e-05,
      "loss": 0.2754,
      "num_input_tokens_seen": 384512,
      "step": 481
    },
    {
      "epoch": 0.3957307060755337,
      "grad_norm": 10.5783052444458,
      "learning_rate": 3.087855282756475e-05,
      "loss": 0.3786,
      "num_input_tokens_seen": 385280,
      "step": 482
    },
    {
      "epoch": 0.39655172413793105,
      "grad_norm": 21.500185012817383,
      "learning_rate": 3.079369758735393e-05,
      "loss": 1.2253,
      "num_input_tokens_seen": 386048,
      "step": 483
    },
    {
      "epoch": 0.3973727422003284,
      "grad_norm": 21.28744888305664,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 0.9085,
      "num_input_tokens_seen": 386816,
      "step": 484
    },
    {
      "epoch": 0.3981937602627258,
      "grad_norm": 4.9040632247924805,
      "learning_rate": 3.062377635859663e-05,
      "loss": 0.1271,
      "num_input_tokens_seen": 387584,
      "step": 485
    },
    {
      "epoch": 0.39901477832512317,
      "grad_norm": 3.6574866771698,
      "learning_rate": 3.053871244048669e-05,
      "loss": 0.0867,
      "num_input_tokens_seen": 388352,
      "step": 486
    },
    {
      "epoch": 0.39983579638752054,
      "grad_norm": 18.82362174987793,
      "learning_rate": 3.045358103491357e-05,
      "loss": 1.1716,
      "num_input_tokens_seen": 389120,
      "step": 487
    },
    {
      "epoch": 0.4006568144499179,
      "grad_norm": 6.047551155090332,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 0.1751,
      "num_input_tokens_seen": 389888,
      "step": 488
    },
    {
      "epoch": 0.4014778325123153,
      "grad_norm": 19.08486557006836,
      "learning_rate": 3.028311991138472e-05,
      "loss": 0.6511,
      "num_input_tokens_seen": 390656,
      "step": 489
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 14.2404203414917,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 0.5409,
      "num_input_tokens_seen": 391424,
      "step": 490
    },
    {
      "epoch": 0.40311986863711,
      "grad_norm": 14.483847618103027,
      "learning_rate": 3.0112401296044757e-05,
      "loss": 0.9337,
      "num_input_tokens_seen": 392320,
      "step": 491
    },
    {
      "epoch": 0.4039408866995074,
      "grad_norm": 11.792492866516113,
      "learning_rate": 3.002694802864912e-05,
      "loss": 0.7484,
      "num_input_tokens_seen": 393216,
      "step": 492
    },
    {
      "epoch": 0.40476190476190477,
      "grad_norm": 7.635273456573486,
      "learning_rate": 2.9941433509478156e-05,
      "loss": 0.2248,
      "num_input_tokens_seen": 393984,
      "step": 493
    },
    {
      "epoch": 0.40558292282430214,
      "grad_norm": 18.182048797607422,
      "learning_rate": 2.98558587804993e-05,
      "loss": 0.8557,
      "num_input_tokens_seen": 394752,
      "step": 494
    },
    {
      "epoch": 0.4064039408866995,
      "grad_norm": 7.090676307678223,
      "learning_rate": 2.9770224884413623e-05,
      "loss": 0.3024,
      "num_input_tokens_seen": 395520,
      "step": 495
    },
    {
      "epoch": 0.4072249589490969,
      "grad_norm": 11.041862487792969,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 0.6108,
      "num_input_tokens_seen": 396416,
      "step": 496
    },
    {
      "epoch": 0.40804597701149425,
      "grad_norm": 10.831998825073242,
      "learning_rate": 2.9598783765318007e-05,
      "loss": 0.5515,
      "num_input_tokens_seen": 397312,
      "step": 497
    },
    {
      "epoch": 0.4088669950738916,
      "grad_norm": 12.623050689697266,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 0.4173,
      "num_input_tokens_seen": 398080,
      "step": 498
    },
    {
      "epoch": 0.409688013136289,
      "grad_norm": 17.49873924255371,
      "learning_rate": 2.9427118507989586e-05,
      "loss": 0.5396,
      "num_input_tokens_seen": 398848,
      "step": 499
    },
    {
      "epoch": 0.41050903119868637,
      "grad_norm": 6.687432289123535,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 0.3427,
      "num_input_tokens_seen": 399616,
      "step": 500
    },
    {
      "epoch": 0.41050903119868637,
      "eval_loss": 0.4677239954471588,
      "eval_runtime": 10.9153,
      "eval_samples_per_second": 111.861,
      "eval_steps_per_second": 14.017,
      "num_input_tokens_seen": 399616,
      "step": 500
    },
    {
      "epoch": 0.41133004926108374,
      "grad_norm": 20.1948299407959,
      "learning_rate": 2.9255237479150816e-05,
      "loss": 0.7814,
      "num_input_tokens_seen": 400384,
      "step": 501
    },
    {
      "epoch": 0.4121510673234811,
      "grad_norm": 5.876784324645996,
      "learning_rate": 2.916921866790256e-05,
      "loss": 0.1574,
      "num_input_tokens_seen": 401152,
      "step": 502
    },
    {
      "epoch": 0.4129720853858785,
      "grad_norm": 11.336938858032227,
      "learning_rate": 2.908314905604056e-05,
      "loss": 0.5059,
      "num_input_tokens_seen": 402048,
      "step": 503
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 10.512534141540527,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 0.572,
      "num_input_tokens_seen": 402816,
      "step": 504
    },
    {
      "epoch": 0.41461412151067323,
      "grad_norm": 18.227027893066406,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 1.0046,
      "num_input_tokens_seen": 403712,
      "step": 505
    },
    {
      "epoch": 0.4154351395730706,
      "grad_norm": 11.201638221740723,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 0.7171,
      "num_input_tokens_seen": 404480,
      "step": 506
    },
    {
      "epoch": 0.41625615763546797,
      "grad_norm": 18.654123306274414,
      "learning_rate": 2.8738383586092745e-05,
      "loss": 1.1919,
      "num_input_tokens_seen": 405248,
      "step": 507
    },
    {
      "epoch": 0.41707717569786534,
      "grad_norm": 15.561388969421387,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 1.0871,
      "num_input_tokens_seen": 406144,
      "step": 508
    },
    {
      "epoch": 0.4178981937602627,
      "grad_norm": 6.6125102043151855,
      "learning_rate": 2.8565723342637796e-05,
      "loss": 0.2237,
      "num_input_tokens_seen": 407040,
      "step": 509
    },
    {
      "epoch": 0.4187192118226601,
      "grad_norm": 19.016321182250977,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 1.321,
      "num_input_tokens_seen": 407936,
      "step": 510
    },
    {
      "epoch": 0.41954022988505746,
      "grad_norm": 11.384791374206543,
      "learning_rate": 2.8392889310857612e-05,
      "loss": 0.358,
      "num_input_tokens_seen": 408704,
      "step": 511
    },
    {
      "epoch": 0.42036124794745483,
      "grad_norm": 9.67564868927002,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 0.2214,
      "num_input_tokens_seen": 409472,
      "step": 512
    },
    {
      "epoch": 0.4211822660098522,
      "grad_norm": 22.276281356811523,
      "learning_rate": 2.8219889914439074e-05,
      "loss": 0.779,
      "num_input_tokens_seen": 410240,
      "step": 513
    },
    {
      "epoch": 0.4220032840722496,
      "grad_norm": 7.773627281188965,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 0.2566,
      "num_input_tokens_seen": 411008,
      "step": 514
    },
    {
      "epoch": 0.42282430213464695,
      "grad_norm": 6.623173713684082,
      "learning_rate": 2.8046733585128687e-05,
      "loss": 0.1126,
      "num_input_tokens_seen": 411776,
      "step": 515
    },
    {
      "epoch": 0.4236453201970443,
      "grad_norm": 7.747298717498779,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 0.7463,
      "num_input_tokens_seen": 412544,
      "step": 516
    },
    {
      "epoch": 0.4244663382594417,
      "grad_norm": 11.178686141967773,
      "learning_rate": 2.787342876232167e-05,
      "loss": 0.3904,
      "num_input_tokens_seen": 413440,
      "step": 517
    },
    {
      "epoch": 0.42528735632183906,
      "grad_norm": 14.824995994567871,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 0.3912,
      "num_input_tokens_seen": 414208,
      "step": 518
    },
    {
      "epoch": 0.42610837438423643,
      "grad_norm": 19.785253524780273,
      "learning_rate": 2.7699983892650573e-05,
      "loss": 1.041,
      "num_input_tokens_seen": 415104,
      "step": 519
    },
    {
      "epoch": 0.4269293924466338,
      "grad_norm": 2.536438226699829,
      "learning_rate": 2.761321158169134e-05,
      "loss": 0.0667,
      "num_input_tokens_seen": 415872,
      "step": 520
    },
    {
      "epoch": 0.4277504105090312,
      "grad_norm": 13.833465576171875,
      "learning_rate": 2.7526407429573657e-05,
      "loss": 0.8432,
      "num_input_tokens_seen": 416768,
      "step": 521
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 13.3749418258667,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 0.5937,
      "num_input_tokens_seen": 417536,
      "step": 522
    },
    {
      "epoch": 0.4293924466338259,
      "grad_norm": 9.688555717468262,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 0.3961,
      "num_input_tokens_seen": 418304,
      "step": 523
    },
    {
      "epoch": 0.4302134646962233,
      "grad_norm": 14.252872467041016,
      "learning_rate": 2.726581450494451e-05,
      "loss": 0.858,
      "num_input_tokens_seen": 419200,
      "step": 524
    },
    {
      "epoch": 0.43103448275862066,
      "grad_norm": 4.124024868011475,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.1262,
      "num_input_tokens_seen": 419968,
      "step": 525
    },
    {
      "epoch": 0.4318555008210181,
      "grad_norm": 13.450277328491211,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 0.8757,
      "num_input_tokens_seen": 420736,
      "step": 526
    },
    {
      "epoch": 0.43267651888341546,
      "grad_norm": 14.50489330291748,
      "learning_rate": 2.7004973108221472e-05,
      "loss": 0.5417,
      "num_input_tokens_seen": 421632,
      "step": 527
    },
    {
      "epoch": 0.43349753694581283,
      "grad_norm": 4.887951374053955,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 0.1194,
      "num_input_tokens_seen": 422528,
      "step": 528
    },
    {
      "epoch": 0.4343185550082102,
      "grad_norm": 19.043344497680664,
      "learning_rate": 2.6830954928190794e-05,
      "loss": 0.7526,
      "num_input_tokens_seen": 423296,
      "step": 529
    },
    {
      "epoch": 0.4351395730706076,
      "grad_norm": 16.311634063720703,
      "learning_rate": 2.674391184360313e-05,
      "loss": 1.0576,
      "num_input_tokens_seen": 424064,
      "step": 530
    },
    {
      "epoch": 0.43596059113300495,
      "grad_norm": 18.779617309570312,
      "learning_rate": 2.6656847510000012e-05,
      "loss": 0.8032,
      "num_input_tokens_seen": 424832,
      "step": 531
    },
    {
      "epoch": 0.4367816091954023,
      "grad_norm": 6.711226463317871,
      "learning_rate": 2.656976298823284e-05,
      "loss": 0.1286,
      "num_input_tokens_seen": 425728,
      "step": 532
    },
    {
      "epoch": 0.4376026272577997,
      "grad_norm": 22.338668823242188,
      "learning_rate": 2.6482659339399045e-05,
      "loss": 0.8904,
      "num_input_tokens_seen": 426496,
      "step": 533
    },
    {
      "epoch": 0.43842364532019706,
      "grad_norm": 15.09996509552002,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 0.6269,
      "num_input_tokens_seen": 427392,
      "step": 534
    },
    {
      "epoch": 0.43924466338259444,
      "grad_norm": 7.991584300994873,
      "learning_rate": 2.63083989060736e-05,
      "loss": 0.2996,
      "num_input_tokens_seen": 428288,
      "step": 535
    },
    {
      "epoch": 0.4400656814449918,
      "grad_norm": 8.233201026916504,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 0.5438,
      "num_input_tokens_seen": 429056,
      "step": 536
    },
    {
      "epoch": 0.4408866995073892,
      "grad_norm": 11.968542098999023,
      "learning_rate": 2.6134074703231344e-05,
      "loss": 0.3258,
      "num_input_tokens_seen": 429824,
      "step": 537
    },
    {
      "epoch": 0.44170771756978655,
      "grad_norm": 16.99650001525879,
      "learning_rate": 2.604689134322999e-05,
      "loss": 1.0114,
      "num_input_tokens_seen": 430592,
      "step": 538
    },
    {
      "epoch": 0.4425287356321839,
      "grad_norm": 13.377982139587402,
      "learning_rate": 2.5959695227188004e-05,
      "loss": 1.0514,
      "num_input_tokens_seen": 431616,
      "step": 539
    },
    {
      "epoch": 0.4433497536945813,
      "grad_norm": 11.483556747436523,
      "learning_rate": 2.587248741756253e-05,
      "loss": 0.4261,
      "num_input_tokens_seen": 432512,
      "step": 540
    },
    {
      "epoch": 0.44417077175697867,
      "grad_norm": 18.530101776123047,
      "learning_rate": 2.578526897695321e-05,
      "loss": 1.0019,
      "num_input_tokens_seen": 433280,
      "step": 541
    },
    {
      "epoch": 0.44499178981937604,
      "grad_norm": 15.86109733581543,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 1.0427,
      "num_input_tokens_seen": 434048,
      "step": 542
    },
    {
      "epoch": 0.4458128078817734,
      "grad_norm": 7.793674468994141,
      "learning_rate": 2.5610804453816333e-05,
      "loss": 0.3745,
      "num_input_tokens_seen": 434816,
      "step": 543
    },
    {
      "epoch": 0.4466338259441708,
      "grad_norm": 14.49538803100586,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 0.3684,
      "num_input_tokens_seen": 435584,
      "step": 544
    },
    {
      "epoch": 0.44745484400656815,
      "grad_norm": 9.543402671813965,
      "learning_rate": 2.5436310160932092e-05,
      "loss": 0.4685,
      "num_input_tokens_seen": 436352,
      "step": 545
    },
    {
      "epoch": 0.4482758620689655,
      "grad_norm": 12.561697959899902,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 0.5201,
      "num_input_tokens_seen": 437120,
      "step": 546
    },
    {
      "epoch": 0.4490968801313629,
      "grad_norm": 9.552421569824219,
      "learning_rate": 2.5261794602906145e-05,
      "loss": 0.4098,
      "num_input_tokens_seen": 437888,
      "step": 547
    },
    {
      "epoch": 0.44991789819376027,
      "grad_norm": 18.278963088989258,
      "learning_rate": 2.517453150744904e-05,
      "loss": 0.6407,
      "num_input_tokens_seen": 438656,
      "step": 548
    },
    {
      "epoch": 0.45073891625615764,
      "grad_norm": 10.732806205749512,
      "learning_rate": 2.5087266285380596e-05,
      "loss": 0.8744,
      "num_input_tokens_seen": 439424,
      "step": 549
    },
    {
      "epoch": 0.451559934318555,
      "grad_norm": 8.666354179382324,
      "learning_rate": 2.5e-05,
      "loss": 0.3382,
      "num_input_tokens_seen": 440192,
      "step": 550
    },
    {
      "epoch": 0.4523809523809524,
      "grad_norm": 14.5958890914917,
      "learning_rate": 2.4912733714619417e-05,
      "loss": 1.0019,
      "num_input_tokens_seen": 441088,
      "step": 551
    },
    {
      "epoch": 0.45320197044334976,
      "grad_norm": 10.740922927856445,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 0.3787,
      "num_input_tokens_seen": 441984,
      "step": 552
    },
    {
      "epoch": 0.4540229885057471,
      "grad_norm": 9.49317741394043,
      "learning_rate": 2.4738205397093864e-05,
      "loss": 0.4059,
      "num_input_tokens_seen": 442752,
      "step": 553
    },
    {
      "epoch": 0.4548440065681445,
      "grad_norm": 9.286373138427734,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 0.3838,
      "num_input_tokens_seen": 443520,
      "step": 554
    },
    {
      "epoch": 0.45566502463054187,
      "grad_norm": 14.147631645202637,
      "learning_rate": 2.4563689839067913e-05,
      "loss": 0.7528,
      "num_input_tokens_seen": 444288,
      "step": 555
    },
    {
      "epoch": 0.45648604269293924,
      "grad_norm": 7.207083225250244,
      "learning_rate": 2.447643950291608e-05,
      "loss": 0.2286,
      "num_input_tokens_seen": 445056,
      "step": 556
    },
    {
      "epoch": 0.4573070607553366,
      "grad_norm": 14.534344673156738,
      "learning_rate": 2.4389195546183673e-05,
      "loss": 0.6723,
      "num_input_tokens_seen": 445824,
      "step": 557
    },
    {
      "epoch": 0.458128078817734,
      "grad_norm": 11.940617561340332,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 0.7514,
      "num_input_tokens_seen": 446592,
      "step": 558
    },
    {
      "epoch": 0.45894909688013136,
      "grad_norm": 8.053288459777832,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 0.2788,
      "num_input_tokens_seen": 447360,
      "step": 559
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 12.990854263305664,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 0.5617,
      "num_input_tokens_seen": 448256,
      "step": 560
    },
    {
      "epoch": 0.4605911330049261,
      "grad_norm": 7.398388862609863,
      "learning_rate": 2.4040304772812002e-05,
      "loss": 0.205,
      "num_input_tokens_seen": 449024,
      "step": 561
    },
    {
      "epoch": 0.4614121510673235,
      "grad_norm": 8.027409553527832,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 0.2218,
      "num_input_tokens_seen": 449792,
      "step": 562
    },
    {
      "epoch": 0.46223316912972084,
      "grad_norm": 6.48585319519043,
      "learning_rate": 2.386592529676866e-05,
      "loss": 0.2083,
      "num_input_tokens_seen": 450688,
      "step": 563
    },
    {
      "epoch": 0.4630541871921182,
      "grad_norm": 20.81690788269043,
      "learning_rate": 2.377875575510967e-05,
      "loss": 0.6856,
      "num_input_tokens_seen": 451456,
      "step": 564
    },
    {
      "epoch": 0.4638752052545156,
      "grad_norm": 14.611818313598633,
      "learning_rate": 2.3691601093926404e-05,
      "loss": 1.1625,
      "num_input_tokens_seen": 452224,
      "step": 565
    },
    {
      "epoch": 0.46469622331691296,
      "grad_norm": 7.892107963562012,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 0.2554,
      "num_input_tokens_seen": 452992,
      "step": 566
    },
    {
      "epoch": 0.46551724137931033,
      "grad_norm": 22.515962600708008,
      "learning_rate": 2.3517340660600964e-05,
      "loss": 1.3319,
      "num_input_tokens_seen": 453760,
      "step": 567
    },
    {
      "epoch": 0.4663382594417077,
      "grad_norm": 7.6892523765563965,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 0.3041,
      "num_input_tokens_seen": 454528,
      "step": 568
    },
    {
      "epoch": 0.4671592775041051,
      "grad_norm": 7.663326740264893,
      "learning_rate": 2.3343152490000004e-05,
      "loss": 0.1885,
      "num_input_tokens_seen": 455296,
      "step": 569
    },
    {
      "epoch": 0.46798029556650245,
      "grad_norm": 16.13599967956543,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 0.4922,
      "num_input_tokens_seen": 456064,
      "step": 570
    },
    {
      "epoch": 0.4688013136288998,
      "grad_norm": 6.655601978302002,
      "learning_rate": 2.3169045071809215e-05,
      "loss": 0.1836,
      "num_input_tokens_seen": 456832,
      "step": 571
    },
    {
      "epoch": 0.4696223316912972,
      "grad_norm": 14.181792259216309,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 0.3763,
      "num_input_tokens_seen": 457600,
      "step": 572
    },
    {
      "epoch": 0.47044334975369456,
      "grad_norm": 20.178579330444336,
      "learning_rate": 2.299502689177853e-05,
      "loss": 0.5843,
      "num_input_tokens_seen": 458368,
      "step": 573
    },
    {
      "epoch": 0.47126436781609193,
      "grad_norm": 17.663183212280273,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 1.0086,
      "num_input_tokens_seen": 459136,
      "step": 574
    },
    {
      "epoch": 0.4720853858784893,
      "grad_norm": 12.09471607208252,
      "learning_rate": 2.2821106431308544e-05,
      "loss": 0.699,
      "num_input_tokens_seen": 460032,
      "step": 575
    },
    {
      "epoch": 0.4729064039408867,
      "grad_norm": 10.705765724182129,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 0.4295,
      "num_input_tokens_seen": 460800,
      "step": 576
    },
    {
      "epoch": 0.47372742200328405,
      "grad_norm": 3.5428481101989746,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 0.0734,
      "num_input_tokens_seen": 461568,
      "step": 577
    },
    {
      "epoch": 0.4745484400656814,
      "grad_norm": 4.921699523925781,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 0.109,
      "num_input_tokens_seen": 462336,
      "step": 578
    },
    {
      "epoch": 0.4753694581280788,
      "grad_norm": 16.57168960571289,
      "learning_rate": 2.247359257042634e-05,
      "loss": 1.5798,
      "num_input_tokens_seen": 463360,
      "step": 579
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 4.411267280578613,
      "learning_rate": 2.238678841830867e-05,
      "loss": 0.1327,
      "num_input_tokens_seen": 464128,
      "step": 580
    },
    {
      "epoch": 0.47701149425287354,
      "grad_norm": 7.466091632843018,
      "learning_rate": 2.230001610734943e-05,
      "loss": 0.2645,
      "num_input_tokens_seen": 464896,
      "step": 581
    },
    {
      "epoch": 0.47783251231527096,
      "grad_norm": 2.0959815979003906,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 0.045,
      "num_input_tokens_seen": 465664,
      "step": 582
    },
    {
      "epoch": 0.47865353037766833,
      "grad_norm": 17.17496109008789,
      "learning_rate": 2.212657123767834e-05,
      "loss": 0.9638,
      "num_input_tokens_seen": 466432,
      "step": 583
    },
    {
      "epoch": 0.4794745484400657,
      "grad_norm": 12.797410011291504,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 0.4969,
      "num_input_tokens_seen": 467200,
      "step": 584
    },
    {
      "epoch": 0.4802955665024631,
      "grad_norm": 16.09156608581543,
      "learning_rate": 2.195326641487132e-05,
      "loss": 0.4997,
      "num_input_tokens_seen": 467968,
      "step": 585
    },
    {
      "epoch": 0.48111658456486045,
      "grad_norm": 9.001835823059082,
      "learning_rate": 2.186666916089239e-05,
      "loss": 0.2874,
      "num_input_tokens_seen": 468864,
      "step": 586
    },
    {
      "epoch": 0.4819376026272578,
      "grad_norm": 7.990081310272217,
      "learning_rate": 2.1780110085560935e-05,
      "loss": 0.1509,
      "num_input_tokens_seen": 469632,
      "step": 587
    },
    {
      "epoch": 0.4827586206896552,
      "grad_norm": 14.595627784729004,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 0.8316,
      "num_input_tokens_seen": 470400,
      "step": 588
    },
    {
      "epoch": 0.48357963875205257,
      "grad_norm": 16.080610275268555,
      "learning_rate": 2.1607110689142393e-05,
      "loss": 0.3077,
      "num_input_tokens_seen": 471168,
      "step": 589
    },
    {
      "epoch": 0.48440065681444994,
      "grad_norm": 16.26651954650879,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 0.9526,
      "num_input_tokens_seen": 471936,
      "step": 590
    },
    {
      "epoch": 0.4852216748768473,
      "grad_norm": 12.001713752746582,
      "learning_rate": 2.1434276657362213e-05,
      "loss": 0.7062,
      "num_input_tokens_seen": 472832,
      "step": 591
    },
    {
      "epoch": 0.4860426929392447,
      "grad_norm": 16.117101669311523,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 0.9466,
      "num_input_tokens_seen": 473600,
      "step": 592
    },
    {
      "epoch": 0.48686371100164205,
      "grad_norm": 22.496397018432617,
      "learning_rate": 2.1261616413907265e-05,
      "loss": 1.2613,
      "num_input_tokens_seen": 474368,
      "step": 593
    },
    {
      "epoch": 0.4876847290640394,
      "grad_norm": 13.430333137512207,
      "learning_rate": 2.117535409289905e-05,
      "loss": 0.4236,
      "num_input_tokens_seen": 475136,
      "step": 594
    },
    {
      "epoch": 0.4885057471264368,
      "grad_norm": 22.9657039642334,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 1.5962,
      "num_input_tokens_seen": 476032,
      "step": 595
    },
    {
      "epoch": 0.48932676518883417,
      "grad_norm": 9.976019859313965,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 0.2964,
      "num_input_tokens_seen": 476800,
      "step": 596
    },
    {
      "epoch": 0.49014778325123154,
      "grad_norm": 27.865398406982422,
      "learning_rate": 2.0916850943959452e-05,
      "loss": 0.939,
      "num_input_tokens_seen": 477568,
      "step": 597
    },
    {
      "epoch": 0.4909688013136289,
      "grad_norm": 24.694934844970703,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 1.3903,
      "num_input_tokens_seen": 478464,
      "step": 598
    },
    {
      "epoch": 0.4917898193760263,
      "grad_norm": 19.81865119934082,
      "learning_rate": 2.0744762520849193e-05,
      "loss": 0.8058,
      "num_input_tokens_seen": 479232,
      "step": 599
    },
    {
      "epoch": 0.49261083743842365,
      "grad_norm": 22.654556274414062,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 0.5539,
      "num_input_tokens_seen": 480000,
      "step": 600
    },
    {
      "epoch": 0.493431855500821,
      "grad_norm": 4.420926570892334,
      "learning_rate": 2.057288149201042e-05,
      "loss": 0.1775,
      "num_input_tokens_seen": 480768,
      "step": 601
    },
    {
      "epoch": 0.4942528735632184,
      "grad_norm": 13.660446166992188,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 0.3115,
      "num_input_tokens_seen": 481536,
      "step": 602
    },
    {
      "epoch": 0.49507389162561577,
      "grad_norm": 11.679529190063477,
      "learning_rate": 2.0401216234681995e-05,
      "loss": 0.9628,
      "num_input_tokens_seen": 482304,
      "step": 603
    },
    {
      "epoch": 0.49589490968801314,
      "grad_norm": 13.602445602416992,
      "learning_rate": 2.031546713535688e-05,
      "loss": 0.4967,
      "num_input_tokens_seen": 483072,
      "step": 604
    },
    {
      "epoch": 0.4967159277504105,
      "grad_norm": 10.41876220703125,
      "learning_rate": 2.022977511558638e-05,
      "loss": 0.4921,
      "num_input_tokens_seen": 483840,
      "step": 605
    },
    {
      "epoch": 0.4975369458128079,
      "grad_norm": 12.710356712341309,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 0.6907,
      "num_input_tokens_seen": 484608,
      "step": 606
    },
    {
      "epoch": 0.49835796387520526,
      "grad_norm": 12.800982475280762,
      "learning_rate": 2.0058566490521847e-05,
      "loss": 0.5532,
      "num_input_tokens_seen": 485376,
      "step": 607
    },
    {
      "epoch": 0.49917898193760263,
      "grad_norm": 9.42807674407959,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 0.5259,
      "num_input_tokens_seen": 486144,
      "step": 608
    },
    {
      "epoch": 0.5,
      "grad_norm": 13.091987609863281,
      "learning_rate": 1.9887598703955242e-05,
      "loss": 0.4229,
      "num_input_tokens_seen": 486912,
      "step": 609
    },
    {
      "epoch": 0.5008210180623974,
      "grad_norm": 39.710201263427734,
      "learning_rate": 1.980220772955602e-05,
      "loss": 1.3913,
      "num_input_tokens_seen": 487680,
      "step": 610
    },
    {
      "epoch": 0.5016420361247947,
      "grad_norm": 17.81380271911621,
      "learning_rate": 1.9716880088615285e-05,
      "loss": 0.9709,
      "num_input_tokens_seen": 488448,
      "step": 611
    },
    {
      "epoch": 0.5024630541871922,
      "grad_norm": 16.567649841308594,
      "learning_rate": 1.963161682082342e-05,
      "loss": 0.6712,
      "num_input_tokens_seen": 489216,
      "step": 612
    },
    {
      "epoch": 0.5032840722495895,
      "grad_norm": 19.674161911010742,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 0.9572,
      "num_input_tokens_seen": 489984,
      "step": 613
    },
    {
      "epoch": 0.5041050903119869,
      "grad_norm": 21.393207550048828,
      "learning_rate": 1.946128755951332e-05,
      "loss": 1.6688,
      "num_input_tokens_seen": 490752,
      "step": 614
    },
    {
      "epoch": 0.5049261083743842,
      "grad_norm": 8.888100624084473,
      "learning_rate": 1.937622364140338e-05,
      "loss": 0.5788,
      "num_input_tokens_seen": 491520,
      "step": 615
    },
    {
      "epoch": 0.5057471264367817,
      "grad_norm": 8.23509407043457,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 0.2046,
      "num_input_tokens_seen": 492288,
      "step": 616
    },
    {
      "epoch": 0.506568144499179,
      "grad_norm": 16.08527183532715,
      "learning_rate": 1.920630241264607e-05,
      "loss": 0.8671,
      "num_input_tokens_seen": 493056,
      "step": 617
    },
    {
      "epoch": 0.5073891625615764,
      "grad_norm": 22.16335678100586,
      "learning_rate": 1.912144717243525e-05,
      "loss": 1.3021,
      "num_input_tokens_seen": 493824,
      "step": 618
    },
    {
      "epoch": 0.5082101806239737,
      "grad_norm": 9.218518257141113,
      "learning_rate": 1.9036663560535483e-05,
      "loss": 0.2235,
      "num_input_tokens_seen": 494592,
      "step": 619
    },
    {
      "epoch": 0.5090311986863711,
      "grad_norm": 17.938072204589844,
      "learning_rate": 1.895195261000831e-05,
      "loss": 0.857,
      "num_input_tokens_seen": 495360,
      "step": 620
    },
    {
      "epoch": 0.5098522167487685,
      "grad_norm": 16.99669647216797,
      "learning_rate": 1.8867315353029935e-05,
      "loss": 1.432,
      "num_input_tokens_seen": 496384,
      "step": 621
    },
    {
      "epoch": 0.5106732348111659,
      "grad_norm": 6.34955358505249,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 0.2452,
      "num_input_tokens_seen": 497152,
      "step": 622
    },
    {
      "epoch": 0.5114942528735632,
      "grad_norm": 15.78809928894043,
      "learning_rate": 1.869826604392216e-05,
      "loss": 0.7179,
      "num_input_tokens_seen": 497920,
      "step": 623
    },
    {
      "epoch": 0.5123152709359606,
      "grad_norm": 8.138509750366211,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 0.2601,
      "num_input_tokens_seen": 498816,
      "step": 624
    },
    {
      "epoch": 0.513136288998358,
      "grad_norm": 25.37901496887207,
      "learning_rate": 1.852952387243698e-05,
      "loss": 1.0086,
      "num_input_tokens_seen": 499584,
      "step": 625
    },
    {
      "epoch": 0.5139573070607554,
      "grad_norm": 22.44985008239746,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 0.7357,
      "num_input_tokens_seen": 500352,
      "step": 626
    },
    {
      "epoch": 0.5147783251231527,
      "grad_norm": 10.470769882202148,
      "learning_rate": 1.8361097062829778e-05,
      "loss": 1.0085,
      "num_input_tokens_seen": 501120,
      "step": 627
    },
    {
      "epoch": 0.5155993431855501,
      "grad_norm": 15.64239501953125,
      "learning_rate": 1.827700448461836e-05,
      "loss": 1.0407,
      "num_input_tokens_seen": 501888,
      "step": 628
    },
    {
      "epoch": 0.5164203612479474,
      "grad_norm": 11.783602714538574,
      "learning_rate": 1.8192993823985643e-05,
      "loss": 0.6848,
      "num_input_tokens_seen": 502656,
      "step": 629
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 19.578615188598633,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 0.8231,
      "num_input_tokens_seen": 503424,
      "step": 630
    },
    {
      "epoch": 0.5180623973727422,
      "grad_norm": 5.958865642547607,
      "learning_rate": 1.802522234901927e-05,
      "loss": 0.1378,
      "num_input_tokens_seen": 504192,
      "step": 631
    },
    {
      "epoch": 0.5188834154351396,
      "grad_norm": 5.203982353210449,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 0.1347,
      "num_input_tokens_seen": 504960,
      "step": 632
    },
    {
      "epoch": 0.5197044334975369,
      "grad_norm": 15.683603286743164,
      "learning_rate": 1.7857790814875663e-05,
      "loss": 0.5797,
      "num_input_tokens_seen": 505728,
      "step": 633
    },
    {
      "epoch": 0.5205254515599343,
      "grad_norm": 12.93187427520752,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 0.5924,
      "num_input_tokens_seen": 506496,
      "step": 634
    },
    {
      "epoch": 0.5213464696223317,
      "grad_norm": 17.61713981628418,
      "learning_rate": 1.7690707381931583e-05,
      "loss": 1.1392,
      "num_input_tokens_seen": 507264,
      "step": 635
    },
    {
      "epoch": 0.5221674876847291,
      "grad_norm": 7.810959815979004,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 0.3556,
      "num_input_tokens_seen": 508032,
      "step": 636
    },
    {
      "epoch": 0.5229885057471264,
      "grad_norm": 11.730880737304688,
      "learning_rate": 1.7523980193597836e-05,
      "loss": 0.5136,
      "num_input_tokens_seen": 508928,
      "step": 637
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 10.953216552734375,
      "learning_rate": 1.744075273123889e-05,
      "loss": 0.5189,
      "num_input_tokens_seen": 509696,
      "step": 638
    },
    {
      "epoch": 0.5246305418719212,
      "grad_norm": 15.332448959350586,
      "learning_rate": 1.735761737592236e-05,
      "loss": 0.5963,
      "num_input_tokens_seen": 510720,
      "step": 639
    },
    {
      "epoch": 0.5254515599343186,
      "grad_norm": 9.791112899780273,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 0.5698,
      "num_input_tokens_seen": 511488,
      "step": 640
    },
    {
      "epoch": 0.5262725779967159,
      "grad_norm": 13.17763900756836,
      "learning_rate": 1.7191627037194186e-05,
      "loss": 0.7093,
      "num_input_tokens_seen": 512256,
      "step": 641
    },
    {
      "epoch": 0.5270935960591133,
      "grad_norm": 4.632844924926758,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 0.0668,
      "num_input_tokens_seen": 513024,
      "step": 642
    },
    {
      "epoch": 0.5279146141215106,
      "grad_norm": 9.580810546875,
      "learning_rate": 1.702601726754825e-05,
      "loss": 0.4274,
      "num_input_tokens_seen": 513792,
      "step": 643
    },
    {
      "epoch": 0.5287356321839081,
      "grad_norm": 8.792949676513672,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 0.6982,
      "num_input_tokens_seen": 514560,
      "step": 644
    },
    {
      "epoch": 0.5295566502463054,
      "grad_norm": 13.29220962524414,
      "learning_rate": 1.686079613857109e-05,
      "loss": 0.8493,
      "num_input_tokens_seen": 515328,
      "step": 645
    },
    {
      "epoch": 0.5303776683087028,
      "grad_norm": 13.920031547546387,
      "learning_rate": 1.677833383153542e-05,
      "loss": 0.6648,
      "num_input_tokens_seen": 516224,
      "step": 646
    },
    {
      "epoch": 0.5311986863711001,
      "grad_norm": 10.832618713378906,
      "learning_rate": 1.6695971702907426e-05,
      "loss": 0.6619,
      "num_input_tokens_seen": 516992,
      "step": 647
    },
    {
      "epoch": 0.5320197044334976,
      "grad_norm": 7.275596618652344,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 0.387,
      "num_input_tokens_seen": 517760,
      "step": 648
    },
    {
      "epoch": 0.5328407224958949,
      "grad_norm": 13.626885414123535,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 0.4459,
      "num_input_tokens_seen": 518528,
      "step": 649
    },
    {
      "epoch": 0.5336617405582923,
      "grad_norm": 15.552661895751953,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 0.7565,
      "num_input_tokens_seen": 519296,
      "step": 650
    },
    {
      "epoch": 0.5344827586206896,
      "grad_norm": 3.835101366043091,
      "learning_rate": 1.6367545025036636e-05,
      "loss": 0.1131,
      "num_input_tokens_seen": 520064,
      "step": 651
    },
    {
      "epoch": 0.535303776683087,
      "grad_norm": 11.158726692199707,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 0.624,
      "num_input_tokens_seen": 520832,
      "step": 652
    },
    {
      "epoch": 0.5361247947454844,
      "grad_norm": 7.085178852081299,
      "learning_rate": 1.6203958789882456e-05,
      "loss": 0.3519,
      "num_input_tokens_seen": 521600,
      "step": 653
    },
    {
      "epoch": 0.5369458128078818,
      "grad_norm": 11.69735050201416,
      "learning_rate": 1.612232593979658e-05,
      "loss": 1.0812,
      "num_input_tokens_seen": 522496,
      "step": 654
    },
    {
      "epoch": 0.5377668308702791,
      "grad_norm": 17.55085563659668,
      "learning_rate": 1.6040801261367493e-05,
      "loss": 1.0624,
      "num_input_tokens_seen": 523264,
      "step": 655
    },
    {
      "epoch": 0.5385878489326765,
      "grad_norm": 6.321938991546631,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 0.3549,
      "num_input_tokens_seen": 524032,
      "step": 656
    },
    {
      "epoch": 0.5394088669950738,
      "grad_norm": 17.929561614990234,
      "learning_rate": 1.5878080391559508e-05,
      "loss": 1.0409,
      "num_input_tokens_seen": 524800,
      "step": 657
    },
    {
      "epoch": 0.5402298850574713,
      "grad_norm": 19.541122436523438,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 0.8706,
      "num_input_tokens_seen": 525696,
      "step": 658
    },
    {
      "epoch": 0.5410509031198686,
      "grad_norm": 6.172135829925537,
      "learning_rate": 1.5715804111244137e-05,
      "loss": 0.1905,
      "num_input_tokens_seen": 526592,
      "step": 659
    },
    {
      "epoch": 0.541871921182266,
      "grad_norm": 10.874724388122559,
      "learning_rate": 1.56348351646022e-05,
      "loss": 0.4397,
      "num_input_tokens_seen": 527360,
      "step": 660
    },
    {
      "epoch": 0.5426929392446633,
      "grad_norm": 11.39532470703125,
      "learning_rate": 1.5553980329538326e-05,
      "loss": 0.4288,
      "num_input_tokens_seen": 528256,
      "step": 661
    },
    {
      "epoch": 0.5435139573070608,
      "grad_norm": 12.87673568725586,
      "learning_rate": 1.547324059124315e-05,
      "loss": 0.4133,
      "num_input_tokens_seen": 529024,
      "step": 662
    },
    {
      "epoch": 0.5443349753694581,
      "grad_norm": 13.37690544128418,
      "learning_rate": 1.539261693350491e-05,
      "loss": 0.5452,
      "num_input_tokens_seen": 529792,
      "step": 663
    },
    {
      "epoch": 0.5451559934318555,
      "grad_norm": 12.10311508178711,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 0.7467,
      "num_input_tokens_seen": 530560,
      "step": 664
    },
    {
      "epoch": 0.5459770114942529,
      "grad_norm": 9.388409614562988,
      "learning_rate": 1.523172178776816e-05,
      "loss": 0.3369,
      "num_input_tokens_seen": 531328,
      "step": 665
    },
    {
      "epoch": 0.5467980295566502,
      "grad_norm": 17.268035888671875,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 1.3506,
      "num_input_tokens_seen": 532096,
      "step": 666
    },
    {
      "epoch": 0.5476190476190477,
      "grad_norm": 7.094603061676025,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 0.2013,
      "num_input_tokens_seen": 532864,
      "step": 667
    },
    {
      "epoch": 0.548440065681445,
      "grad_norm": 22.151700973510742,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 0.7698,
      "num_input_tokens_seen": 533632,
      "step": 668
    },
    {
      "epoch": 0.5492610837438424,
      "grad_norm": 9.911256790161133,
      "learning_rate": 1.4911367591190248e-05,
      "loss": 0.4522,
      "num_input_tokens_seen": 534400,
      "step": 669
    },
    {
      "epoch": 0.5500821018062397,
      "grad_norm": 7.97852087020874,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 0.3489,
      "num_input_tokens_seen": 535168,
      "step": 670
    },
    {
      "epoch": 0.5509031198686372,
      "grad_norm": 14.460739135742188,
      "learning_rate": 1.475192415396068e-05,
      "loss": 0.5294,
      "num_input_tokens_seen": 536064,
      "step": 671
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 19.636701583862305,
      "learning_rate": 1.467238925438646e-05,
      "loss": 0.6917,
      "num_input_tokens_seen": 536832,
      "step": 672
    },
    {
      "epoch": 0.5525451559934319,
      "grad_norm": 10.887221336364746,
      "learning_rate": 1.4592980193489975e-05,
      "loss": 0.5634,
      "num_input_tokens_seen": 537600,
      "step": 673
    },
    {
      "epoch": 0.5533661740558292,
      "grad_norm": 11.40573787689209,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 0.4777,
      "num_input_tokens_seen": 538496,
      "step": 674
    },
    {
      "epoch": 0.5541871921182266,
      "grad_norm": 6.617100238800049,
      "learning_rate": 1.443454345648252e-05,
      "loss": 0.1909,
      "num_input_tokens_seen": 539264,
      "step": 675
    },
    {
      "epoch": 0.555008210180624,
      "grad_norm": 11.772485733032227,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 0.8217,
      "num_input_tokens_seen": 540160,
      "step": 676
    },
    {
      "epoch": 0.5558292282430214,
      "grad_norm": 12.39025592803955,
      "learning_rate": 1.4276621664921357e-05,
      "loss": 0.2368,
      "num_input_tokens_seen": 540928,
      "step": 677
    },
    {
      "epoch": 0.5566502463054187,
      "grad_norm": 12.568672180175781,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 0.7762,
      "num_input_tokens_seen": 541824,
      "step": 678
    },
    {
      "epoch": 0.5574712643678161,
      "grad_norm": 4.777376651763916,
      "learning_rate": 1.4119222515691816e-05,
      "loss": 0.2088,
      "num_input_tokens_seen": 542592,
      "step": 679
    },
    {
      "epoch": 0.5582922824302134,
      "grad_norm": 6.514989852905273,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 0.1638,
      "num_input_tokens_seen": 543360,
      "step": 680
    },
    {
      "epoch": 0.5591133004926109,
      "grad_norm": 5.631386756896973,
      "learning_rate": 1.3962353680206373e-05,
      "loss": 0.2277,
      "num_input_tokens_seen": 544128,
      "step": 681
    },
    {
      "epoch": 0.5599343185550082,
      "grad_norm": 15.708657264709473,
      "learning_rate": 1.388412052037682e-05,
      "loss": 0.5655,
      "num_input_tokens_seen": 545024,
      "step": 682
    },
    {
      "epoch": 0.5607553366174056,
      "grad_norm": 12.326498985290527,
      "learning_rate": 1.380602280403076e-05,
      "loss": 0.4708,
      "num_input_tokens_seen": 545792,
      "step": 683
    },
    {
      "epoch": 0.5615763546798029,
      "grad_norm": 7.353911399841309,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 0.1183,
      "num_input_tokens_seen": 546560,
      "step": 684
    },
    {
      "epoch": 0.5623973727422004,
      "grad_norm": 12.108405113220215,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 0.4835,
      "num_input_tokens_seen": 547328,
      "step": 685
    },
    {
      "epoch": 0.5632183908045977,
      "grad_norm": 4.537341117858887,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 0.1087,
      "num_input_tokens_seen": 548096,
      "step": 686
    },
    {
      "epoch": 0.5640394088669951,
      "grad_norm": 11.051642417907715,
      "learning_rate": 1.349500538040371e-05,
      "loss": 0.5178,
      "num_input_tokens_seen": 548864,
      "step": 687
    },
    {
      "epoch": 0.5648604269293924,
      "grad_norm": 9.059571266174316,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 0.1393,
      "num_input_tokens_seen": 549760,
      "step": 688
    },
    {
      "epoch": 0.5656814449917899,
      "grad_norm": 13.193552017211914,
      "learning_rate": 1.3340333991502724e-05,
      "loss": 0.2669,
      "num_input_tokens_seen": 550528,
      "step": 689
    },
    {
      "epoch": 0.5665024630541872,
      "grad_norm": 5.024933815002441,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 0.1127,
      "num_input_tokens_seen": 551296,
      "step": 690
    },
    {
      "epoch": 0.5673234811165846,
      "grad_norm": 20.736160278320312,
      "learning_rate": 1.3186230878273653e-05,
      "loss": 0.6459,
      "num_input_tokens_seen": 552064,
      "step": 691
    },
    {
      "epoch": 0.5681444991789819,
      "grad_norm": 18.285947799682617,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 0.779,
      "num_input_tokens_seen": 552960,
      "step": 692
    },
    {
      "epoch": 0.5689655172413793,
      "grad_norm": 14.36701774597168,
      "learning_rate": 1.3032703551484832e-05,
      "loss": 0.7472,
      "num_input_tokens_seen": 553728,
      "step": 693
    },
    {
      "epoch": 0.5697865353037767,
      "grad_norm": 33.0629997253418,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 1.6371,
      "num_input_tokens_seen": 554752,
      "step": 694
    },
    {
      "epoch": 0.5706075533661741,
      "grad_norm": 5.916604042053223,
      "learning_rate": 1.2879759493841575e-05,
      "loss": 0.1421,
      "num_input_tokens_seen": 555648,
      "step": 695
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 9.461037635803223,
      "learning_rate": 1.280350852153168e-05,
      "loss": 0.3097,
      "num_input_tokens_seen": 556416,
      "step": 696
    },
    {
      "epoch": 0.5722495894909688,
      "grad_norm": 16.925960540771484,
      "learning_rate": 1.272740615962148e-05,
      "loss": 0.6037,
      "num_input_tokens_seen": 557312,
      "step": 697
    },
    {
      "epoch": 0.5730706075533661,
      "grad_norm": 9.443087577819824,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 0.45,
      "num_input_tokens_seen": 558208,
      "step": 698
    },
    {
      "epoch": 0.5738916256157636,
      "grad_norm": 20.70909309387207,
      "learning_rate": 1.2575650974311119e-05,
      "loss": 1.0535,
      "num_input_tokens_seen": 559104,
      "step": 699
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 12.858237266540527,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 0.5509,
      "num_input_tokens_seen": 560000,
      "step": 700
    },
    {
      "epoch": 0.5755336617405583,
      "grad_norm": 13.08884334564209,
      "learning_rate": 1.2424501334244123e-05,
      "loss": 0.6489,
      "num_input_tokens_seen": 560896,
      "step": 701
    },
    {
      "epoch": 0.5763546798029556,
      "grad_norm": 13.499595642089844,
      "learning_rate": 1.234915589697091e-05,
      "loss": 0.6371,
      "num_input_tokens_seen": 561664,
      "step": 702
    },
    {
      "epoch": 0.577175697865353,
      "grad_norm": 15.82994270324707,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 0.377,
      "num_input_tokens_seen": 562560,
      "step": 703
    },
    {
      "epoch": 0.5779967159277504,
      "grad_norm": 5.878095626831055,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 0.1118,
      "num_input_tokens_seen": 563456,
      "step": 704
    },
    {
      "epoch": 0.5788177339901478,
      "grad_norm": 14.498437881469727,
      "learning_rate": 1.2124048127248644e-05,
      "loss": 0.6052,
      "num_input_tokens_seen": 564224,
      "step": 705
    },
    {
      "epoch": 0.5796387520525451,
      "grad_norm": 11.079229354858398,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 0.3328,
      "num_input_tokens_seen": 565120,
      "step": 706
    },
    {
      "epoch": 0.5804597701149425,
      "grad_norm": 10.78463077545166,
      "learning_rate": 1.19747592039856e-05,
      "loss": 0.2632,
      "num_input_tokens_seen": 565888,
      "step": 707
    },
    {
      "epoch": 0.5812807881773399,
      "grad_norm": 23.664466857910156,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 1.5952,
      "num_input_tokens_seen": 566656,
      "step": 708
    },
    {
      "epoch": 0.5821018062397373,
      "grad_norm": 15.690145492553711,
      "learning_rate": 1.1826105112583061e-05,
      "loss": 0.7778,
      "num_input_tokens_seen": 567424,
      "step": 709
    },
    {
      "epoch": 0.5829228243021346,
      "grad_norm": 16.37021255493164,
      "learning_rate": 1.175201839416988e-05,
      "loss": 0.4,
      "num_input_tokens_seen": 568192,
      "step": 710
    },
    {
      "epoch": 0.583743842364532,
      "grad_norm": 20.498802185058594,
      "learning_rate": 1.167809309823175e-05,
      "loss": 0.8081,
      "num_input_tokens_seen": 569088,
      "step": 711
    },
    {
      "epoch": 0.5845648604269293,
      "grad_norm": 18.823652267456055,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 0.8856,
      "num_input_tokens_seen": 569984,
      "step": 712
    },
    {
      "epoch": 0.5853858784893268,
      "grad_norm": 18.998966217041016,
      "learning_rate": 1.1530730374828422e-05,
      "loss": 1.0077,
      "num_input_tokens_seen": 570752,
      "step": 713
    },
    {
      "epoch": 0.5862068965517241,
      "grad_norm": 13.695626258850098,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 0.33,
      "num_input_tokens_seen": 571520,
      "step": 714
    },
    {
      "epoch": 0.5870279146141215,
      "grad_norm": 6.512822151184082,
      "learning_rate": 1.1384024124624324e-05,
      "loss": 0.2132,
      "num_input_tokens_seen": 572288,
      "step": 715
    },
    {
      "epoch": 0.5878489326765188,
      "grad_norm": 9.200483322143555,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 0.1318,
      "num_input_tokens_seen": 573056,
      "step": 716
    },
    {
      "epoch": 0.5886699507389163,
      "grad_norm": 9.018525123596191,
      "learning_rate": 1.123798149787511e-05,
      "loss": 0.3362,
      "num_input_tokens_seen": 573824,
      "step": 717
    },
    {
      "epoch": 0.5894909688013136,
      "grad_norm": 13.176846504211426,
      "learning_rate": 1.11652112689164e-05,
      "loss": 1.0481,
      "num_input_tokens_seen": 574720,
      "step": 718
    },
    {
      "epoch": 0.590311986863711,
      "grad_norm": 10.703465461730957,
      "learning_rate": 1.109260961249238e-05,
      "loss": 0.4182,
      "num_input_tokens_seen": 575488,
      "step": 719
    },
    {
      "epoch": 0.5911330049261084,
      "grad_norm": 16.050453186035156,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 0.3786,
      "num_input_tokens_seen": 576384,
      "step": 720
    },
    {
      "epoch": 0.5919540229885057,
      "grad_norm": 10.034587860107422,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 0.2221,
      "num_input_tokens_seen": 577152,
      "step": 721
    },
    {
      "epoch": 0.5927750410509032,
      "grad_norm": 11.588698387145996,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 0.3826,
      "num_input_tokens_seen": 577920,
      "step": 722
    },
    {
      "epoch": 0.5935960591133005,
      "grad_norm": 17.81514549255371,
      "learning_rate": 1.0803906373672476e-05,
      "loss": 0.9039,
      "num_input_tokens_seen": 578688,
      "step": 723
    },
    {
      "epoch": 0.5944170771756979,
      "grad_norm": 9.379791259765625,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 0.6338,
      "num_input_tokens_seen": 579456,
      "step": 724
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 25.304672241210938,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 1.7321,
      "num_input_tokens_seen": 580224,
      "step": 725
    },
    {
      "epoch": 0.5960591133004927,
      "grad_norm": 18.832578659057617,
      "learning_rate": 1.058919209575517e-05,
      "loss": 0.5433,
      "num_input_tokens_seen": 580992,
      "step": 726
    },
    {
      "epoch": 0.59688013136289,
      "grad_norm": 4.517862796783447,
      "learning_rate": 1.0517970691433035e-05,
      "loss": 0.0578,
      "num_input_tokens_seen": 581760,
      "step": 727
    },
    {
      "epoch": 0.5977011494252874,
      "grad_norm": 7.733113765716553,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 0.2732,
      "num_input_tokens_seen": 582528,
      "step": 728
    },
    {
      "epoch": 0.5985221674876847,
      "grad_norm": 17.972448348999023,
      "learning_rate": 1.0376058125319613e-05,
      "loss": 0.5347,
      "num_input_tokens_seen": 583296,
      "step": 729
    },
    {
      "epoch": 0.5993431855500821,
      "grad_norm": 8.942221641540527,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 0.5541,
      "num_input_tokens_seen": 584064,
      "step": 730
    },
    {
      "epoch": 0.6001642036124795,
      "grad_norm": 12.005173683166504,
      "learning_rate": 1.0234858309501862e-05,
      "loss": 0.695,
      "num_input_tokens_seen": 584832,
      "step": 731
    },
    {
      "epoch": 0.6009852216748769,
      "grad_norm": 8.950971603393555,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 0.3103,
      "num_input_tokens_seen": 585856,
      "step": 732
    },
    {
      "epoch": 0.6018062397372742,
      "grad_norm": 11.227133750915527,
      "learning_rate": 1.0094378125859602e-05,
      "loss": 0.2743,
      "num_input_tokens_seen": 586624,
      "step": 733
    },
    {
      "epoch": 0.6026272577996716,
      "grad_norm": 15.584318161010742,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 0.4131,
      "num_input_tokens_seen": 587392,
      "step": 734
    },
    {
      "epoch": 0.603448275862069,
      "grad_norm": 18.140230178833008,
      "learning_rate": 9.954624421198792e-06,
      "loss": 1.0949,
      "num_input_tokens_seen": 588160,
      "step": 735
    },
    {
      "epoch": 0.6042692939244664,
      "grad_norm": 5.169501781463623,
      "learning_rate": 9.88502212844063e-06,
      "loss": 0.2147,
      "num_input_tokens_seen": 588928,
      "step": 736
    },
    {
      "epoch": 0.6050903119868637,
      "grad_norm": 10.887882232666016,
      "learning_rate": 9.815604006917839e-06,
      "loss": 0.5903,
      "num_input_tokens_seen": 589696,
      "step": 737
    },
    {
      "epoch": 0.6059113300492611,
      "grad_norm": 17.975553512573242,
      "learning_rate": 9.746370902468311e-06,
      "loss": 0.3916,
      "num_input_tokens_seen": 590464,
      "step": 738
    },
    {
      "epoch": 0.6067323481116584,
      "grad_norm": 6.296069145202637,
      "learning_rate": 9.677323658675594e-06,
      "loss": 0.1827,
      "num_input_tokens_seen": 591232,
      "step": 739
    },
    {
      "epoch": 0.6075533661740559,
      "grad_norm": 13.312274932861328,
      "learning_rate": 9.608463116858542e-06,
      "loss": 0.8229,
      "num_input_tokens_seen": 592000,
      "step": 740
    },
    {
      "epoch": 0.6083743842364532,
      "grad_norm": 11.384603500366211,
      "learning_rate": 9.539790116061151e-06,
      "loss": 0.8143,
      "num_input_tokens_seen": 592768,
      "step": 741
    },
    {
      "epoch": 0.6091954022988506,
      "grad_norm": 16.736656188964844,
      "learning_rate": 9.471305493042243e-06,
      "loss": 1.06,
      "num_input_tokens_seen": 593664,
      "step": 742
    },
    {
      "epoch": 0.6100164203612479,
      "grad_norm": 11.096451759338379,
      "learning_rate": 9.403010082265351e-06,
      "loss": 0.3977,
      "num_input_tokens_seen": 594432,
      "step": 743
    },
    {
      "epoch": 0.6108374384236454,
      "grad_norm": 18.547298431396484,
      "learning_rate": 9.334904715888495e-06,
      "loss": 0.4582,
      "num_input_tokens_seen": 595200,
      "step": 744
    },
    {
      "epoch": 0.6116584564860427,
      "grad_norm": 20.421302795410156,
      "learning_rate": 9.266990223754069e-06,
      "loss": 1.1648,
      "num_input_tokens_seen": 595968,
      "step": 745
    },
    {
      "epoch": 0.6124794745484401,
      "grad_norm": 7.672238349914551,
      "learning_rate": 9.199267433378727e-06,
      "loss": 0.476,
      "num_input_tokens_seen": 596736,
      "step": 746
    },
    {
      "epoch": 0.6133004926108374,
      "grad_norm": 13.551690101623535,
      "learning_rate": 9.131737169943314e-06,
      "loss": 0.5714,
      "num_input_tokens_seen": 597504,
      "step": 747
    },
    {
      "epoch": 0.6141215106732348,
      "grad_norm": 6.519069671630859,
      "learning_rate": 9.064400256282757e-06,
      "loss": 0.2266,
      "num_input_tokens_seen": 598144,
      "step": 748
    },
    {
      "epoch": 0.6149425287356322,
      "grad_norm": 8.515775680541992,
      "learning_rate": 8.997257512876108e-06,
      "loss": 0.2441,
      "num_input_tokens_seen": 598912,
      "step": 749
    },
    {
      "epoch": 0.6157635467980296,
      "grad_norm": 5.938322067260742,
      "learning_rate": 8.930309757836517e-06,
      "loss": 0.2129,
      "num_input_tokens_seen": 599680,
      "step": 750
    },
    {
      "epoch": 0.6165845648604269,
      "grad_norm": 10.898696899414062,
      "learning_rate": 8.863557806901233e-06,
      "loss": 0.4486,
      "num_input_tokens_seen": 600448,
      "step": 751
    },
    {
      "epoch": 0.6174055829228243,
      "grad_norm": 11.974241256713867,
      "learning_rate": 8.797002473421728e-06,
      "loss": 0.4841,
      "num_input_tokens_seen": 601216,
      "step": 752
    },
    {
      "epoch": 0.6182266009852216,
      "grad_norm": 13.570941925048828,
      "learning_rate": 8.73064456835373e-06,
      "loss": 0.3178,
      "num_input_tokens_seen": 602112,
      "step": 753
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 8.63151741027832,
      "learning_rate": 8.664484900247363e-06,
      "loss": 0.4311,
      "num_input_tokens_seen": 602880,
      "step": 754
    },
    {
      "epoch": 0.6198686371100164,
      "grad_norm": 17.88612174987793,
      "learning_rate": 8.598524275237322e-06,
      "loss": 0.65,
      "num_input_tokens_seen": 603648,
      "step": 755
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 8.973336219787598,
      "learning_rate": 8.532763497032987e-06,
      "loss": 0.2286,
      "num_input_tokens_seen": 604544,
      "step": 756
    },
    {
      "epoch": 0.6215106732348111,
      "grad_norm": 22.634733200073242,
      "learning_rate": 8.467203366908707e-06,
      "loss": 1.3171,
      "num_input_tokens_seen": 605440,
      "step": 757
    },
    {
      "epoch": 0.6223316912972086,
      "grad_norm": 15.822813987731934,
      "learning_rate": 8.40184468369396e-06,
      "loss": 0.6653,
      "num_input_tokens_seen": 606208,
      "step": 758
    },
    {
      "epoch": 0.6231527093596059,
      "grad_norm": 10.072162628173828,
      "learning_rate": 8.33668824376369e-06,
      "loss": 0.352,
      "num_input_tokens_seen": 606976,
      "step": 759
    },
    {
      "epoch": 0.6239737274220033,
      "grad_norm": 5.162041187286377,
      "learning_rate": 8.271734841028553e-06,
      "loss": 0.1148,
      "num_input_tokens_seen": 607744,
      "step": 760
    },
    {
      "epoch": 0.6247947454844006,
      "grad_norm": 19.219341278076172,
      "learning_rate": 8.206985266925249e-06,
      "loss": 1.0257,
      "num_input_tokens_seen": 608512,
      "step": 761
    },
    {
      "epoch": 0.625615763546798,
      "grad_norm": 4.620182514190674,
      "learning_rate": 8.142440310406924e-06,
      "loss": 0.0789,
      "num_input_tokens_seen": 609280,
      "step": 762
    },
    {
      "epoch": 0.6264367816091954,
      "grad_norm": 11.274190902709961,
      "learning_rate": 8.078100757933485e-06,
      "loss": 0.318,
      "num_input_tokens_seen": 610176,
      "step": 763
    },
    {
      "epoch": 0.6272577996715928,
      "grad_norm": 14.408896446228027,
      "learning_rate": 8.013967393462094e-06,
      "loss": 0.2558,
      "num_input_tokens_seen": 610944,
      "step": 764
    },
    {
      "epoch": 0.6280788177339901,
      "grad_norm": 20.855451583862305,
      "learning_rate": 7.950040998437542e-06,
      "loss": 1.2465,
      "num_input_tokens_seen": 611712,
      "step": 765
    },
    {
      "epoch": 0.6288998357963875,
      "grad_norm": 10.854013442993164,
      "learning_rate": 7.886322351782783e-06,
      "loss": 0.3118,
      "num_input_tokens_seen": 612480,
      "step": 766
    },
    {
      "epoch": 0.6297208538587848,
      "grad_norm": 11.965919494628906,
      "learning_rate": 7.822812229889428e-06,
      "loss": 0.6181,
      "num_input_tokens_seen": 613248,
      "step": 767
    },
    {
      "epoch": 0.6305418719211823,
      "grad_norm": 17.0259952545166,
      "learning_rate": 7.759511406608255e-06,
      "loss": 0.6438,
      "num_input_tokens_seen": 614016,
      "step": 768
    },
    {
      "epoch": 0.6313628899835796,
      "grad_norm": 15.275613784790039,
      "learning_rate": 7.696420653239833e-06,
      "loss": 0.8048,
      "num_input_tokens_seen": 614784,
      "step": 769
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 14.203573226928711,
      "learning_rate": 7.633540738525066e-06,
      "loss": 0.4731,
      "num_input_tokens_seen": 615552,
      "step": 770
    },
    {
      "epoch": 0.6330049261083743,
      "grad_norm": 16.635746002197266,
      "learning_rate": 7.570872428635889e-06,
      "loss": 0.9057,
      "num_input_tokens_seen": 616320,
      "step": 771
    },
    {
      "epoch": 0.6338259441707718,
      "grad_norm": 17.690088272094727,
      "learning_rate": 7.508416487165862e-06,
      "loss": 0.9542,
      "num_input_tokens_seen": 617088,
      "step": 772
    },
    {
      "epoch": 0.6346469622331691,
      "grad_norm": 16.514558792114258,
      "learning_rate": 7.4461736751209405e-06,
      "loss": 1.2963,
      "num_input_tokens_seen": 617856,
      "step": 773
    },
    {
      "epoch": 0.6354679802955665,
      "grad_norm": 12.924301147460938,
      "learning_rate": 7.384144750910133e-06,
      "loss": 0.3202,
      "num_input_tokens_seen": 618624,
      "step": 774
    },
    {
      "epoch": 0.6362889983579638,
      "grad_norm": 16.880563735961914,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 1.2606,
      "num_input_tokens_seen": 619648,
      "step": 775
    },
    {
      "epoch": 0.6371100164203612,
      "grad_norm": 20.101877212524414,
      "learning_rate": 7.260731586586983e-06,
      "loss": 0.9707,
      "num_input_tokens_seen": 620416,
      "step": 776
    },
    {
      "epoch": 0.6379310344827587,
      "grad_norm": 12.257959365844727,
      "learning_rate": 7.19934885022509e-06,
      "loss": 0.6987,
      "num_input_tokens_seen": 621184,
      "step": 777
    },
    {
      "epoch": 0.638752052545156,
      "grad_norm": 1.7388203144073486,
      "learning_rate": 7.138183009179922e-06,
      "loss": 0.0347,
      "num_input_tokens_seen": 622080,
      "step": 778
    },
    {
      "epoch": 0.6395730706075534,
      "grad_norm": 8.331174850463867,
      "learning_rate": 7.0772348087379315e-06,
      "loss": 0.1904,
      "num_input_tokens_seen": 622848,
      "step": 779
    },
    {
      "epoch": 0.6403940886699507,
      "grad_norm": 24.130739212036133,
      "learning_rate": 7.016504991533726e-06,
      "loss": 1.4763,
      "num_input_tokens_seen": 623616,
      "step": 780
    },
    {
      "epoch": 0.6412151067323482,
      "grad_norm": 7.8478474617004395,
      "learning_rate": 6.9559942975409465e-06,
      "loss": 0.3528,
      "num_input_tokens_seen": 624384,
      "step": 781
    },
    {
      "epoch": 0.6420361247947455,
      "grad_norm": 16.99997901916504,
      "learning_rate": 6.895703464063319e-06,
      "loss": 0.3183,
      "num_input_tokens_seen": 625152,
      "step": 782
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 14.155348777770996,
      "learning_rate": 6.835633225725605e-06,
      "loss": 0.6286,
      "num_input_tokens_seen": 625920,
      "step": 783
    },
    {
      "epoch": 0.6436781609195402,
      "grad_norm": 14.240606307983398,
      "learning_rate": 6.775784314464717e-06,
      "loss": 0.6249,
      "num_input_tokens_seen": 626688,
      "step": 784
    },
    {
      "epoch": 0.6444991789819376,
      "grad_norm": 18.757633209228516,
      "learning_rate": 6.716157459520739e-06,
      "loss": 0.797,
      "num_input_tokens_seen": 627456,
      "step": 785
    },
    {
      "epoch": 0.645320197044335,
      "grad_norm": 12.646520614624023,
      "learning_rate": 6.656753387428089e-06,
      "loss": 0.5777,
      "num_input_tokens_seen": 628224,
      "step": 786
    },
    {
      "epoch": 0.6461412151067324,
      "grad_norm": 25.0958251953125,
      "learning_rate": 6.5975728220066425e-06,
      "loss": 1.3969,
      "num_input_tokens_seen": 628992,
      "step": 787
    },
    {
      "epoch": 0.6469622331691297,
      "grad_norm": 21.93182945251465,
      "learning_rate": 6.538616484352902e-06,
      "loss": 1.5883,
      "num_input_tokens_seen": 629760,
      "step": 788
    },
    {
      "epoch": 0.6477832512315271,
      "grad_norm": 6.595653057098389,
      "learning_rate": 6.47988509283125e-06,
      "loss": 0.2974,
      "num_input_tokens_seen": 630528,
      "step": 789
    },
    {
      "epoch": 0.6486042692939245,
      "grad_norm": 11.330392837524414,
      "learning_rate": 6.421379363065142e-06,
      "loss": 0.7948,
      "num_input_tokens_seen": 631296,
      "step": 790
    },
    {
      "epoch": 0.6494252873563219,
      "grad_norm": 14.543255805969238,
      "learning_rate": 6.363100007928446e-06,
      "loss": 0.6978,
      "num_input_tokens_seen": 632064,
      "step": 791
    },
    {
      "epoch": 0.6502463054187192,
      "grad_norm": 21.39632225036621,
      "learning_rate": 6.305047737536707e-06,
      "loss": 0.8734,
      "num_input_tokens_seen": 632832,
      "step": 792
    },
    {
      "epoch": 0.6510673234811166,
      "grad_norm": 17.385032653808594,
      "learning_rate": 6.247223259238511e-06,
      "loss": 0.6854,
      "num_input_tokens_seen": 633600,
      "step": 793
    },
    {
      "epoch": 0.6518883415435139,
      "grad_norm": 16.559593200683594,
      "learning_rate": 6.189627277606894e-06,
      "loss": 0.6631,
      "num_input_tokens_seen": 634496,
      "step": 794
    },
    {
      "epoch": 0.6527093596059114,
      "grad_norm": 9.431892395019531,
      "learning_rate": 6.1322604944307e-06,
      "loss": 0.4503,
      "num_input_tokens_seen": 635392,
      "step": 795
    },
    {
      "epoch": 0.6535303776683087,
      "grad_norm": 12.22728157043457,
      "learning_rate": 6.075123608706093e-06,
      "loss": 0.439,
      "num_input_tokens_seen": 636160,
      "step": 796
    },
    {
      "epoch": 0.6543513957307061,
      "grad_norm": 10.175436019897461,
      "learning_rate": 6.01821731662798e-06,
      "loss": 0.2965,
      "num_input_tokens_seen": 636928,
      "step": 797
    },
    {
      "epoch": 0.6551724137931034,
      "grad_norm": 10.307927131652832,
      "learning_rate": 5.961542311581586e-06,
      "loss": 0.6001,
      "num_input_tokens_seen": 637696,
      "step": 798
    },
    {
      "epoch": 0.6559934318555009,
      "grad_norm": 20.868690490722656,
      "learning_rate": 5.905099284133952e-06,
      "loss": 0.7269,
      "num_input_tokens_seen": 638464,
      "step": 799
    },
    {
      "epoch": 0.6568144499178982,
      "grad_norm": 14.195584297180176,
      "learning_rate": 5.848888922025553e-06,
      "loss": 0.4641,
      "num_input_tokens_seen": 639232,
      "step": 800
    },
    {
      "epoch": 0.6576354679802956,
      "grad_norm": 3.4189250469207764,
      "learning_rate": 5.792911910161922e-06,
      "loss": 0.095,
      "num_input_tokens_seen": 640128,
      "step": 801
    },
    {
      "epoch": 0.6584564860426929,
      "grad_norm": 24.694826126098633,
      "learning_rate": 5.737168930605272e-06,
      "loss": 1.3323,
      "num_input_tokens_seen": 640896,
      "step": 802
    },
    {
      "epoch": 0.6592775041050903,
      "grad_norm": 12.288629531860352,
      "learning_rate": 5.681660662566224e-06,
      "loss": 0.6783,
      "num_input_tokens_seen": 641664,
      "step": 803
    },
    {
      "epoch": 0.6600985221674877,
      "grad_norm": 13.565925598144531,
      "learning_rate": 5.626387782395512e-06,
      "loss": 0.279,
      "num_input_tokens_seen": 642432,
      "step": 804
    },
    {
      "epoch": 0.6609195402298851,
      "grad_norm": 7.926794528961182,
      "learning_rate": 5.571350963575728e-06,
      "loss": 0.245,
      "num_input_tokens_seen": 643200,
      "step": 805
    },
    {
      "epoch": 0.6617405582922824,
      "grad_norm": 10.908744812011719,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 0.2494,
      "num_input_tokens_seen": 644224,
      "step": 806
    },
    {
      "epoch": 0.6625615763546798,
      "grad_norm": 16.43061065673828,
      "learning_rate": 5.461988189529529e-06,
      "loss": 0.5127,
      "num_input_tokens_seen": 644992,
      "step": 807
    },
    {
      "epoch": 0.6633825944170771,
      "grad_norm": 9.322676658630371,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 0.4827,
      "num_input_tokens_seen": 645760,
      "step": 808
    },
    {
      "epoch": 0.6642036124794746,
      "grad_norm": 17.415088653564453,
      "learning_rate": 5.3535776706149505e-06,
      "loss": 0.356,
      "num_input_tokens_seen": 646528,
      "step": 809
    },
    {
      "epoch": 0.6650246305418719,
      "grad_norm": 7.7870001792907715,
      "learning_rate": 5.299731159831953e-06,
      "loss": 0.2107,
      "num_input_tokens_seen": 647296,
      "step": 810
    },
    {
      "epoch": 0.6658456486042693,
      "grad_norm": 16.5262393951416,
      "learning_rate": 5.24612469060774e-06,
      "loss": 0.8308,
      "num_input_tokens_seen": 648064,
      "step": 811
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 5.382978916168213,
      "learning_rate": 5.192758916120236e-06,
      "loss": 0.1261,
      "num_input_tokens_seen": 648832,
      "step": 812
    },
    {
      "epoch": 0.6674876847290641,
      "grad_norm": 15.849308967590332,
      "learning_rate": 5.139634486614544e-06,
      "loss": 0.4055,
      "num_input_tokens_seen": 649856,
      "step": 813
    },
    {
      "epoch": 0.6683087027914614,
      "grad_norm": 8.878789901733398,
      "learning_rate": 5.086752049395094e-06,
      "loss": 0.6037,
      "num_input_tokens_seen": 650624,
      "step": 814
    },
    {
      "epoch": 0.6691297208538588,
      "grad_norm": 16.2998104095459,
      "learning_rate": 5.034112248817685e-06,
      "loss": 1.1107,
      "num_input_tokens_seen": 651392,
      "step": 815
    },
    {
      "epoch": 0.6699507389162561,
      "grad_norm": 13.217856407165527,
      "learning_rate": 4.981715726281666e-06,
      "loss": 0.8136,
      "num_input_tokens_seen": 652160,
      "step": 816
    },
    {
      "epoch": 0.6707717569786535,
      "grad_norm": 5.616348743438721,
      "learning_rate": 4.929563120222141e-06,
      "loss": 0.0954,
      "num_input_tokens_seen": 653056,
      "step": 817
    },
    {
      "epoch": 0.6715927750410509,
      "grad_norm": 10.714183807373047,
      "learning_rate": 4.877655066102149e-06,
      "loss": 0.4528,
      "num_input_tokens_seen": 653824,
      "step": 818
    },
    {
      "epoch": 0.6724137931034483,
      "grad_norm": 19.045137405395508,
      "learning_rate": 4.825992196404957e-06,
      "loss": 0.811,
      "num_input_tokens_seen": 654592,
      "step": 819
    },
    {
      "epoch": 0.6732348111658456,
      "grad_norm": 13.92477798461914,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 0.5684,
      "num_input_tokens_seen": 655488,
      "step": 820
    },
    {
      "epoch": 0.674055829228243,
      "grad_norm": 16.26996421813965,
      "learning_rate": 4.723404525266839e-06,
      "loss": 0.7396,
      "num_input_tokens_seen": 656256,
      "step": 821
    },
    {
      "epoch": 0.6748768472906403,
      "grad_norm": 22.439102172851562,
      "learning_rate": 4.672480973824311e-06,
      "loss": 1.6343,
      "num_input_tokens_seen": 657024,
      "step": 822
    },
    {
      "epoch": 0.6756978653530378,
      "grad_norm": 3.098475933074951,
      "learning_rate": 4.621805106786142e-06,
      "loss": 0.0556,
      "num_input_tokens_seen": 657920,
      "step": 823
    },
    {
      "epoch": 0.6765188834154351,
      "grad_norm": 8.271759986877441,
      "learning_rate": 4.571377541621788e-06,
      "loss": 0.1851,
      "num_input_tokens_seen": 658688,
      "step": 824
    },
    {
      "epoch": 0.6773399014778325,
      "grad_norm": 14.98885726928711,
      "learning_rate": 4.521198892775203e-06,
      "loss": 1.089,
      "num_input_tokens_seen": 659584,
      "step": 825
    },
    {
      "epoch": 0.6781609195402298,
      "grad_norm": 11.19965648651123,
      "learning_rate": 4.4712697716574e-06,
      "loss": 0.2844,
      "num_input_tokens_seen": 660352,
      "step": 826
    },
    {
      "epoch": 0.6789819376026273,
      "grad_norm": 3.2606709003448486,
      "learning_rate": 4.421590786638951e-06,
      "loss": 0.0755,
      "num_input_tokens_seen": 661120,
      "step": 827
    },
    {
      "epoch": 0.6798029556650246,
      "grad_norm": 5.6584577560424805,
      "learning_rate": 4.372162543042624e-06,
      "loss": 0.0716,
      "num_input_tokens_seen": 661888,
      "step": 828
    },
    {
      "epoch": 0.680623973727422,
      "grad_norm": 13.482076644897461,
      "learning_rate": 4.322985643135952e-06,
      "loss": 0.5484,
      "num_input_tokens_seen": 662656,
      "step": 829
    },
    {
      "epoch": 0.6814449917898193,
      "grad_norm": 2.5739614963531494,
      "learning_rate": 4.274060686123959e-06,
      "loss": 0.0538,
      "num_input_tokens_seen": 663424,
      "step": 830
    },
    {
      "epoch": 0.6822660098522167,
      "grad_norm": 15.925752639770508,
      "learning_rate": 4.225388268141797e-06,
      "loss": 0.5722,
      "num_input_tokens_seen": 664192,
      "step": 831
    },
    {
      "epoch": 0.6830870279146142,
      "grad_norm": 17.684066772460938,
      "learning_rate": 4.176968982247514e-06,
      "loss": 0.558,
      "num_input_tokens_seen": 665088,
      "step": 832
    },
    {
      "epoch": 0.6839080459770115,
      "grad_norm": 7.6343159675598145,
      "learning_rate": 4.128803418414839e-06,
      "loss": 0.1803,
      "num_input_tokens_seen": 665984,
      "step": 833
    },
    {
      "epoch": 0.6847290640394089,
      "grad_norm": 5.5694804191589355,
      "learning_rate": 4.08089216352596e-06,
      "loss": 0.0889,
      "num_input_tokens_seen": 666752,
      "step": 834
    },
    {
      "epoch": 0.6855500821018062,
      "grad_norm": 20.156829833984375,
      "learning_rate": 4.0332358013644016e-06,
      "loss": 0.8566,
      "num_input_tokens_seen": 667520,
      "step": 835
    },
    {
      "epoch": 0.6863711001642037,
      "grad_norm": 13.543784141540527,
      "learning_rate": 3.985834912607894e-06,
      "loss": 0.7846,
      "num_input_tokens_seen": 668416,
      "step": 836
    },
    {
      "epoch": 0.687192118226601,
      "grad_norm": 16.76779556274414,
      "learning_rate": 3.938690074821313e-06,
      "loss": 1.1756,
      "num_input_tokens_seen": 669184,
      "step": 837
    },
    {
      "epoch": 0.6880131362889984,
      "grad_norm": 8.98763656616211,
      "learning_rate": 3.891801862449629e-06,
      "loss": 0.5462,
      "num_input_tokens_seen": 669952,
      "step": 838
    },
    {
      "epoch": 0.6888341543513957,
      "grad_norm": 11.744629859924316,
      "learning_rate": 3.845170846810902e-06,
      "loss": 0.3203,
      "num_input_tokens_seen": 670848,
      "step": 839
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 15.948540687561035,
      "learning_rate": 3.798797596089351e-06,
      "loss": 0.5095,
      "num_input_tokens_seen": 671616,
      "step": 840
    },
    {
      "epoch": 0.6904761904761905,
      "grad_norm": 3.5625157356262207,
      "learning_rate": 3.752682675328406e-06,
      "loss": 0.0743,
      "num_input_tokens_seen": 672512,
      "step": 841
    },
    {
      "epoch": 0.6912972085385879,
      "grad_norm": 10.449302673339844,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 0.2963,
      "num_input_tokens_seen": 673280,
      "step": 842
    },
    {
      "epoch": 0.6921182266009852,
      "grad_norm": 17.26885414123535,
      "learning_rate": 3.661230068116811e-06,
      "loss": 0.8512,
      "num_input_tokens_seen": 674048,
      "step": 843
    },
    {
      "epoch": 0.6929392446633826,
      "grad_norm": 16.436233520507812,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 0.6307,
      "num_input_tokens_seen": 674816,
      "step": 844
    },
    {
      "epoch": 0.69376026272578,
      "grad_norm": 8.65957260131836,
      "learning_rate": 3.5708174824471947e-06,
      "loss": 0.1619,
      "num_input_tokens_seen": 675584,
      "step": 845
    },
    {
      "epoch": 0.6945812807881774,
      "grad_norm": 10.90273380279541,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 0.3507,
      "num_input_tokens_seen": 676480,
      "step": 846
    },
    {
      "epoch": 0.6954022988505747,
      "grad_norm": 8.058173179626465,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 0.2165,
      "num_input_tokens_seen": 677248,
      "step": 847
    },
    {
      "epoch": 0.6962233169129721,
      "grad_norm": 17.34020233154297,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 0.7135,
      "num_input_tokens_seen": 678016,
      "step": 848
    },
    {
      "epoch": 0.6970443349753694,
      "grad_norm": 12.925020217895508,
      "learning_rate": 3.393129951157384e-06,
      "loss": 0.4079,
      "num_input_tokens_seen": 678912,
      "step": 849
    },
    {
      "epoch": 0.6978653530377669,
      "grad_norm": 9.161526679992676,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.3685,
      "num_input_tokens_seen": 679808,
      "step": 850
    },
    {
      "epoch": 0.6986863711001642,
      "grad_norm": 9.275216102600098,
      "learning_rate": 3.305863665776793e-06,
      "loss": 0.4319,
      "num_input_tokens_seen": 680704,
      "step": 851
    },
    {
      "epoch": 0.6995073891625616,
      "grad_norm": 12.991884231567383,
      "learning_rate": 3.262626762369525e-06,
      "loss": 0.4483,
      "num_input_tokens_seen": 681472,
      "step": 852
    },
    {
      "epoch": 0.7003284072249589,
      "grad_norm": 13.627768516540527,
      "learning_rate": 3.219654721995266e-06,
      "loss": 0.9095,
      "num_input_tokens_seen": 682240,
      "step": 853
    },
    {
      "epoch": 0.7011494252873564,
      "grad_norm": 9.812932968139648,
      "learning_rate": 3.176948068254762e-06,
      "loss": 0.2221,
      "num_input_tokens_seen": 683008,
      "step": 854
    },
    {
      "epoch": 0.7019704433497537,
      "grad_norm": 3.907743453979492,
      "learning_rate": 3.1345073215151066e-06,
      "loss": 0.0937,
      "num_input_tokens_seen": 683776,
      "step": 855
    },
    {
      "epoch": 0.7027914614121511,
      "grad_norm": 3.574496030807495,
      "learning_rate": 3.092332998903416e-06,
      "loss": 0.0779,
      "num_input_tokens_seen": 684544,
      "step": 856
    },
    {
      "epoch": 0.7036124794745484,
      "grad_norm": 15.858413696289062,
      "learning_rate": 3.0504256143004866e-06,
      "loss": 0.4165,
      "num_input_tokens_seen": 685312,
      "step": 857
    },
    {
      "epoch": 0.7044334975369458,
      "grad_norm": 10.74299144744873,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 0.6372,
      "num_input_tokens_seen": 686080,
      "step": 858
    },
    {
      "epoch": 0.7052545155993432,
      "grad_norm": 13.101698875427246,
      "learning_rate": 2.967413698375196e-06,
      "loss": 0.8844,
      "num_input_tokens_seen": 686848,
      "step": 859
    },
    {
      "epoch": 0.7060755336617406,
      "grad_norm": 15.435114860534668,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 0.5737,
      "num_input_tokens_seen": 687744,
      "step": 860
    },
    {
      "epoch": 0.7068965517241379,
      "grad_norm": 7.1315202713012695,
      "learning_rate": 2.8854756196229016e-06,
      "loss": 0.1819,
      "num_input_tokens_seen": 688512,
      "step": 861
    },
    {
      "epoch": 0.7077175697865353,
      "grad_norm": 10.445186614990234,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 0.4067,
      "num_input_tokens_seen": 689280,
      "step": 862
    },
    {
      "epoch": 0.7085385878489326,
      "grad_norm": 3.2412872314453125,
      "learning_rate": 2.8046153715899692e-06,
      "loss": 0.0838,
      "num_input_tokens_seen": 690048,
      "step": 863
    },
    {
      "epoch": 0.7093596059113301,
      "grad_norm": 10.18289852142334,
      "learning_rate": 2.764590667717562e-06,
      "loss": 0.3808,
      "num_input_tokens_seen": 690816,
      "step": 864
    },
    {
      "epoch": 0.7101806239737274,
      "grad_norm": 3.509728193283081,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 0.1214,
      "num_input_tokens_seen": 691712,
      "step": 865
    },
    {
      "epoch": 0.7110016420361248,
      "grad_norm": 13.812498092651367,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 0.7067,
      "num_input_tokens_seen": 692608,
      "step": 866
    },
    {
      "epoch": 0.7118226600985221,
      "grad_norm": 18.48629379272461,
      "learning_rate": 2.646144079015797e-06,
      "loss": 0.9629,
      "num_input_tokens_seen": 693376,
      "step": 867
    },
    {
      "epoch": 0.7126436781609196,
      "grad_norm": 10.03013801574707,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 0.2281,
      "num_input_tokens_seen": 694272,
      "step": 868
    },
    {
      "epoch": 0.7134646962233169,
      "grad_norm": 12.464762687683105,
      "learning_rate": 2.5685407581417907e-06,
      "loss": 0.4208,
      "num_input_tokens_seen": 695040,
      "step": 869
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 3.8254153728485107,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 0.0843,
      "num_input_tokens_seen": 695808,
      "step": 870
    },
    {
      "epoch": 0.7151067323481116,
      "grad_norm": 18.28668975830078,
      "learning_rate": 2.492030714945162e-06,
      "loss": 0.6975,
      "num_input_tokens_seen": 696576,
      "step": 871
    },
    {
      "epoch": 0.715927750410509,
      "grad_norm": 13.109434127807617,
      "learning_rate": 2.454186839872158e-06,
      "loss": 0.5943,
      "num_input_tokens_seen": 697344,
      "step": 872
    },
    {
      "epoch": 0.7167487684729064,
      "grad_norm": 16.14527130126953,
      "learning_rate": 2.4166176784174795e-06,
      "loss": 0.5074,
      "num_input_tokens_seen": 698112,
      "step": 873
    },
    {
      "epoch": 0.7175697865353038,
      "grad_norm": 16.23388671875,
      "learning_rate": 2.379323688349516e-06,
      "loss": 0.3759,
      "num_input_tokens_seen": 698880,
      "step": 874
    },
    {
      "epoch": 0.7183908045977011,
      "grad_norm": 12.27357292175293,
      "learning_rate": 2.3423053240837515e-06,
      "loss": 0.411,
      "num_input_tokens_seen": 699648,
      "step": 875
    },
    {
      "epoch": 0.7192118226600985,
      "grad_norm": 15.272795677185059,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 1.2305,
      "num_input_tokens_seen": 700544,
      "step": 876
    },
    {
      "epoch": 0.7200328407224958,
      "grad_norm": 10.39535140991211,
      "learning_rate": 2.269097273823287e-06,
      "loss": 0.2418,
      "num_input_tokens_seen": 701312,
      "step": 877
    },
    {
      "epoch": 0.7208538587848933,
      "grad_norm": 18.372224807739258,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 0.5643,
      "num_input_tokens_seen": 702080,
      "step": 878
    },
    {
      "epoch": 0.7216748768472906,
      "grad_norm": 17.6353816986084,
      "learning_rate": 2.1969970956931762e-06,
      "loss": 0.9948,
      "num_input_tokens_seen": 702848,
      "step": 879
    },
    {
      "epoch": 0.722495894909688,
      "grad_norm": 17.885025024414062,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 0.5563,
      "num_input_tokens_seen": 703616,
      "step": 880
    },
    {
      "epoch": 0.7233169129720853,
      "grad_norm": 12.775643348693848,
      "learning_rate": 2.1260083037543817e-06,
      "loss": 0.4817,
      "num_input_tokens_seen": 704384,
      "step": 881
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 14.09769344329834,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 0.5038,
      "num_input_tokens_seen": 705280,
      "step": 882
    },
    {
      "epoch": 0.7249589490968801,
      "grad_norm": 15.7207612991333,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 0.8204,
      "num_input_tokens_seen": 706048,
      "step": 883
    },
    {
      "epoch": 0.7257799671592775,
      "grad_norm": 13.917156219482422,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 0.5247,
      "num_input_tokens_seen": 706816,
      "step": 884
    },
    {
      "epoch": 0.7266009852216748,
      "grad_norm": 23.178537368774414,
      "learning_rate": 1.9873786636889906e-06,
      "loss": 1.104,
      "num_input_tokens_seen": 707712,
      "step": 885
    },
    {
      "epoch": 0.7274220032840722,
      "grad_norm": 9.932891845703125,
      "learning_rate": 1.95342121028749e-06,
      "loss": 0.6181,
      "num_input_tokens_seen": 708480,
      "step": 886
    },
    {
      "epoch": 0.7282430213464697,
      "grad_norm": 11.366857528686523,
      "learning_rate": 1.9197445721754776e-06,
      "loss": 0.4353,
      "num_input_tokens_seen": 709504,
      "step": 887
    },
    {
      "epoch": 0.729064039408867,
      "grad_norm": 13.787958145141602,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 0.941,
      "num_input_tokens_seen": 710272,
      "step": 888
    },
    {
      "epoch": 0.7298850574712644,
      "grad_norm": 17.96038055419922,
      "learning_rate": 1.8532353797501318e-06,
      "loss": 0.7677,
      "num_input_tokens_seen": 711040,
      "step": 889
    },
    {
      "epoch": 0.7307060755336617,
      "grad_norm": 16.733049392700195,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 0.7782,
      "num_input_tokens_seen": 711808,
      "step": 890
    },
    {
      "epoch": 0.7315270935960592,
      "grad_norm": 12.365982055664062,
      "learning_rate": 1.787854327977162e-06,
      "loss": 0.3494,
      "num_input_tokens_seen": 712704,
      "step": 891
    },
    {
      "epoch": 0.7323481116584565,
      "grad_norm": 15.429789543151855,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 0.7328,
      "num_input_tokens_seen": 713472,
      "step": 892
    },
    {
      "epoch": 0.7331691297208539,
      "grad_norm": 18.425220489501953,
      "learning_rate": 1.7236046034367958e-06,
      "loss": 1.1091,
      "num_input_tokens_seen": 714240,
      "step": 893
    },
    {
      "epoch": 0.7339901477832512,
      "grad_norm": 11.87182331085205,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 0.3405,
      "num_input_tokens_seen": 715008,
      "step": 894
    },
    {
      "epoch": 0.7348111658456487,
      "grad_norm": 15.499215126037598,
      "learning_rate": 1.6604893375699594e-06,
      "loss": 0.3741,
      "num_input_tokens_seen": 715776,
      "step": 895
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 10.710899353027344,
      "learning_rate": 1.629358090099639e-06,
      "loss": 0.2621,
      "num_input_tokens_seen": 716544,
      "step": 896
    },
    {
      "epoch": 0.7364532019704434,
      "grad_norm": 9.3466157913208,
      "learning_rate": 1.5985116065256684e-06,
      "loss": 0.2378,
      "num_input_tokens_seen": 717312,
      "step": 897
    },
    {
      "epoch": 0.7372742200328407,
      "grad_norm": 18.574790954589844,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 1.0773,
      "num_input_tokens_seen": 718080,
      "step": 898
    },
    {
      "epoch": 0.7380952380952381,
      "grad_norm": 2.4764504432678223,
      "learning_rate": 1.5376744310111019e-06,
      "loss": 0.0692,
      "num_input_tokens_seen": 718848,
      "step": 899
    },
    {
      "epoch": 0.7389162561576355,
      "grad_norm": 4.500818729400635,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 0.1124,
      "num_input_tokens_seen": 719616,
      "step": 900
    },
    {
      "epoch": 0.7397372742200329,
      "grad_norm": 25.013216018676758,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 1.3041,
      "num_input_tokens_seen": 720384,
      "step": 901
    },
    {
      "epoch": 0.7405582922824302,
      "grad_norm": 6.637896537780762,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 0.23,
      "num_input_tokens_seen": 721152,
      "step": 902
    },
    {
      "epoch": 0.7413793103448276,
      "grad_norm": 7.819329738616943,
      "learning_rate": 1.4194335513099761e-06,
      "loss": 0.1583,
      "num_input_tokens_seen": 722048,
      "step": 903
    },
    {
      "epoch": 0.7422003284072249,
      "grad_norm": 7.915853023529053,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 0.254,
      "num_input_tokens_seen": 722816,
      "step": 904
    },
    {
      "epoch": 0.7430213464696224,
      "grad_norm": 18.868309020996094,
      "learning_rate": 1.362035610017079e-06,
      "loss": 0.5823,
      "num_input_tokens_seen": 723584,
      "step": 905
    },
    {
      "epoch": 0.7438423645320197,
      "grad_norm": 11.381902694702148,
      "learning_rate": 1.333768497107593e-06,
      "loss": 0.3931,
      "num_input_tokens_seen": 724352,
      "step": 906
    },
    {
      "epoch": 0.7446633825944171,
      "grad_norm": 22.59927749633789,
      "learning_rate": 1.305789749760361e-06,
      "loss": 0.7129,
      "num_input_tokens_seen": 725120,
      "step": 907
    },
    {
      "epoch": 0.7454844006568144,
      "grad_norm": 9.87371826171875,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 0.3406,
      "num_input_tokens_seen": 725888,
      "step": 908
    },
    {
      "epoch": 0.7463054187192119,
      "grad_norm": 20.740827560424805,
      "learning_rate": 1.250698711883691e-06,
      "loss": 0.867,
      "num_input_tokens_seen": 726656,
      "step": 909
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 8.361727714538574,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 0.3519,
      "num_input_tokens_seen": 727296,
      "step": 910
    },
    {
      "epoch": 0.7479474548440066,
      "grad_norm": 8.199491500854492,
      "learning_rate": 1.1967651814465354e-06,
      "loss": 0.2737,
      "num_input_tokens_seen": 728064,
      "step": 911
    },
    {
      "epoch": 0.7487684729064039,
      "grad_norm": 18.6651554107666,
      "learning_rate": 1.170233305176327e-06,
      "loss": 0.3448,
      "num_input_tokens_seen": 728960,
      "step": 912
    },
    {
      "epoch": 0.7495894909688013,
      "grad_norm": 11.681044578552246,
      "learning_rate": 1.1439917870930793e-06,
      "loss": 0.4215,
      "num_input_tokens_seen": 729728,
      "step": 913
    },
    {
      "epoch": 0.7504105090311987,
      "grad_norm": 11.649226188659668,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 0.2136,
      "num_input_tokens_seen": 730624,
      "step": 914
    },
    {
      "epoch": 0.7512315270935961,
      "grad_norm": 4.683927059173584,
      "learning_rate": 1.0923811009241142e-06,
      "loss": 0.1046,
      "num_input_tokens_seen": 731392,
      "step": 915
    },
    {
      "epoch": 0.7520525451559934,
      "grad_norm": 15.573007583618164,
      "learning_rate": 1.067012561698319e-06,
      "loss": 0.5125,
      "num_input_tokens_seen": 732416,
      "step": 916
    },
    {
      "epoch": 0.7528735632183908,
      "grad_norm": 14.004725456237793,
      "learning_rate": 1.0419356383716688e-06,
      "loss": 0.3779,
      "num_input_tokens_seen": 733184,
      "step": 917
    },
    {
      "epoch": 0.7536945812807881,
      "grad_norm": 6.405529975891113,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 0.0926,
      "num_input_tokens_seen": 733952,
      "step": 918
    },
    {
      "epoch": 0.7545155993431856,
      "grad_norm": 12.244012832641602,
      "learning_rate": 9.926578580764234e-07,
      "loss": 0.4966,
      "num_input_tokens_seen": 734720,
      "step": 919
    },
    {
      "epoch": 0.7553366174055829,
      "grad_norm": 15.323378562927246,
      "learning_rate": 9.684576015420278e-07,
      "loss": 0.7944,
      "num_input_tokens_seen": 735616,
      "step": 920
    },
    {
      "epoch": 0.7561576354679803,
      "grad_norm": 4.018073081970215,
      "learning_rate": 9.445501617678654e-07,
      "loss": 0.0714,
      "num_input_tokens_seen": 736384,
      "step": 921
    },
    {
      "epoch": 0.7569786535303776,
      "grad_norm": 8.31979751586914,
      "learning_rate": 9.209358300585474e-07,
      "loss": 0.2642,
      "num_input_tokens_seen": 737152,
      "step": 922
    },
    {
      "epoch": 0.7577996715927751,
      "grad_norm": 11.877938270568848,
      "learning_rate": 8.976148941472501e-07,
      "loss": 0.5919,
      "num_input_tokens_seen": 737920,
      "step": 923
    },
    {
      "epoch": 0.7586206896551724,
      "grad_norm": 14.711834907531738,
      "learning_rate": 8.745876381922147e-07,
      "loss": 0.4032,
      "num_input_tokens_seen": 738816,
      "step": 924
    },
    {
      "epoch": 0.7594417077175698,
      "grad_norm": 10.157482147216797,
      "learning_rate": 8.51854342773295e-07,
      "loss": 0.4156,
      "num_input_tokens_seen": 739712,
      "step": 925
    },
    {
      "epoch": 0.7602627257799671,
      "grad_norm": 12.955118179321289,
      "learning_rate": 8.294152848885157e-07,
      "loss": 0.6922,
      "num_input_tokens_seen": 740480,
      "step": 926
    },
    {
      "epoch": 0.7610837438423645,
      "grad_norm": 27.065048217773438,
      "learning_rate": 8.072707379507216e-07,
      "loss": 1.1291,
      "num_input_tokens_seen": 741248,
      "step": 927
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 4.502167701721191,
      "learning_rate": 7.854209717842231e-07,
      "loss": 0.0754,
      "num_input_tokens_seen": 742016,
      "step": 928
    },
    {
      "epoch": 0.7627257799671593,
      "grad_norm": 11.032127380371094,
      "learning_rate": 7.638662526215284e-07,
      "loss": 0.5384,
      "num_input_tokens_seen": 742784,
      "step": 929
    },
    {
      "epoch": 0.7635467980295566,
      "grad_norm": 25.78217887878418,
      "learning_rate": 7.426068431000882e-07,
      "loss": 1.4324,
      "num_input_tokens_seen": 743552,
      "step": 930
    },
    {
      "epoch": 0.764367816091954,
      "grad_norm": 16.888940811157227,
      "learning_rate": 7.216430022591008e-07,
      "loss": 0.6127,
      "num_input_tokens_seen": 744320,
      "step": 931
    },
    {
      "epoch": 0.7651888341543513,
      "grad_norm": 18.767845153808594,
      "learning_rate": 7.009749855363456e-07,
      "loss": 0.6892,
      "num_input_tokens_seen": 745216,
      "step": 932
    },
    {
      "epoch": 0.7660098522167488,
      "grad_norm": 14.686138153076172,
      "learning_rate": 6.806030447650879e-07,
      "loss": 0.4024,
      "num_input_tokens_seen": 745984,
      "step": 933
    },
    {
      "epoch": 0.7668308702791461,
      "grad_norm": 14.595267295837402,
      "learning_rate": 6.605274281709928e-07,
      "loss": 0.5131,
      "num_input_tokens_seen": 746752,
      "step": 934
    },
    {
      "epoch": 0.7676518883415435,
      "grad_norm": 6.225856304168701,
      "learning_rate": 6.407483803691216e-07,
      "loss": 0.1093,
      "num_input_tokens_seen": 747520,
      "step": 935
    },
    {
      "epoch": 0.7684729064039408,
      "grad_norm": 17.41413688659668,
      "learning_rate": 6.212661423609184e-07,
      "loss": 0.7815,
      "num_input_tokens_seen": 748288,
      "step": 936
    },
    {
      "epoch": 0.7692939244663383,
      "grad_norm": 12.25471305847168,
      "learning_rate": 6.020809515313142e-07,
      "loss": 0.582,
      "num_input_tokens_seen": 749056,
      "step": 937
    },
    {
      "epoch": 0.7701149425287356,
      "grad_norm": 17.010068893432617,
      "learning_rate": 5.83193041645802e-07,
      "loss": 0.6527,
      "num_input_tokens_seen": 749824,
      "step": 938
    },
    {
      "epoch": 0.770935960591133,
      "grad_norm": 9.895889282226562,
      "learning_rate": 5.646026428476031e-07,
      "loss": 0.4176,
      "num_input_tokens_seen": 750720,
      "step": 939
    },
    {
      "epoch": 0.7717569786535303,
      "grad_norm": 12.411416053771973,
      "learning_rate": 5.463099816548579e-07,
      "loss": 0.6993,
      "num_input_tokens_seen": 751616,
      "step": 940
    },
    {
      "epoch": 0.7725779967159278,
      "grad_norm": 12.248148918151855,
      "learning_rate": 5.283152809578751e-07,
      "loss": 0.3655,
      "num_input_tokens_seen": 752384,
      "step": 941
    },
    {
      "epoch": 0.7733990147783252,
      "grad_norm": 14.221388816833496,
      "learning_rate": 5.106187600163987e-07,
      "loss": 0.4544,
      "num_input_tokens_seen": 753152,
      "step": 942
    },
    {
      "epoch": 0.7742200328407225,
      "grad_norm": 10.72766399383545,
      "learning_rate": 4.932206344569562e-07,
      "loss": 0.2426,
      "num_input_tokens_seen": 753920,
      "step": 943
    },
    {
      "epoch": 0.7750410509031199,
      "grad_norm": 6.284206390380859,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 0.2314,
      "num_input_tokens_seen": 754688,
      "step": 944
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 7.1588358879089355,
      "learning_rate": 4.5932041380840065e-07,
      "loss": 0.3857,
      "num_input_tokens_seen": 755456,
      "step": 945
    },
    {
      "epoch": 0.7766830870279147,
      "grad_norm": 14.685179710388184,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 0.817,
      "num_input_tokens_seen": 756224,
      "step": 946
    },
    {
      "epoch": 0.777504105090312,
      "grad_norm": 16.90203857421875,
      "learning_rate": 4.26616271261146e-07,
      "loss": 0.6746,
      "num_input_tokens_seen": 757120,
      "step": 947
    },
    {
      "epoch": 0.7783251231527094,
      "grad_norm": 10.731839179992676,
      "learning_rate": 4.107132296653549e-07,
      "loss": 0.4803,
      "num_input_tokens_seen": 758016,
      "step": 948
    },
    {
      "epoch": 0.7791461412151067,
      "grad_norm": 10.234256744384766,
      "learning_rate": 3.95109800768953e-07,
      "loss": 0.3418,
      "num_input_tokens_seen": 758784,
      "step": 949
    },
    {
      "epoch": 0.7799671592775042,
      "grad_norm": 5.80551290512085,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 0.2202,
      "num_input_tokens_seen": 759552,
      "step": 950
    },
    {
      "epoch": 0.7807881773399015,
      "grad_norm": 16.64049530029297,
      "learning_rate": 3.6480253791274786e-07,
      "loss": 0.578,
      "num_input_tokens_seen": 760320,
      "step": 951
    },
    {
      "epoch": 0.7816091954022989,
      "grad_norm": 9.2662935256958,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 0.2849,
      "num_input_tokens_seen": 761088,
      "step": 952
    },
    {
      "epoch": 0.7824302134646962,
      "grad_norm": 6.245517253875732,
      "learning_rate": 3.3569595982576583e-07,
      "loss": 0.3238,
      "num_input_tokens_seen": 761984,
      "step": 953
    },
    {
      "epoch": 0.7832512315270936,
      "grad_norm": 7.268908500671387,
      "learning_rate": 3.215933731753024e-07,
      "loss": 0.1805,
      "num_input_tokens_seen": 762752,
      "step": 954
    },
    {
      "epoch": 0.784072249589491,
      "grad_norm": 9.245594024658203,
      "learning_rate": 3.077914851215585e-07,
      "loss": 0.2495,
      "num_input_tokens_seen": 763776,
      "step": 955
    },
    {
      "epoch": 0.7848932676518884,
      "grad_norm": 10.987914085388184,
      "learning_rate": 2.942904638361804e-07,
      "loss": 0.6127,
      "num_input_tokens_seen": 764544,
      "step": 956
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 14.957464218139648,
      "learning_rate": 2.810904738248549e-07,
      "loss": 0.3085,
      "num_input_tokens_seen": 765440,
      "step": 957
    },
    {
      "epoch": 0.7865353037766831,
      "grad_norm": 28.42295265197754,
      "learning_rate": 2.681916759252917e-07,
      "loss": 1.4177,
      "num_input_tokens_seen": 766208,
      "step": 958
    },
    {
      "epoch": 0.7873563218390804,
      "grad_norm": 8.656065940856934,
      "learning_rate": 2.555942273052753e-07,
      "loss": 0.228,
      "num_input_tokens_seen": 766976,
      "step": 959
    },
    {
      "epoch": 0.7881773399014779,
      "grad_norm": 6.7783403396606445,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 0.1539,
      "num_input_tokens_seen": 767744,
      "step": 960
    },
    {
      "epoch": 0.7889983579638752,
      "grad_norm": 16.75303077697754,
      "learning_rate": 2.3130398821391007e-07,
      "loss": 0.5141,
      "num_input_tokens_seen": 768512,
      "step": 961
    },
    {
      "epoch": 0.7898193760262726,
      "grad_norm": 12.446864128112793,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 0.326,
      "num_input_tokens_seen": 769280,
      "step": 962
    },
    {
      "epoch": 0.7906403940886699,
      "grad_norm": 18.429611206054688,
      "learning_rate": 2.0822094042274032e-07,
      "loss": 0.7592,
      "num_input_tokens_seen": 770048,
      "step": 963
    },
    {
      "epoch": 0.7914614121510674,
      "grad_norm": 9.833617210388184,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 0.2317,
      "num_input_tokens_seen": 770944,
      "step": 964
    },
    {
      "epoch": 0.7922824302134647,
      "grad_norm": 18.99622917175293,
      "learning_rate": 1.8634620896695043e-07,
      "loss": 1.0732,
      "num_input_tokens_seen": 771712,
      "step": 965
    },
    {
      "epoch": 0.7931034482758621,
      "grad_norm": 3.7822227478027344,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 0.1227,
      "num_input_tokens_seen": 772608,
      "step": 966
    },
    {
      "epoch": 0.7939244663382594,
      "grad_norm": 5.321859359741211,
      "learning_rate": 1.6568085999008888e-07,
      "loss": 0.1258,
      "num_input_tokens_seen": 773376,
      "step": 967
    },
    {
      "epoch": 0.7947454844006568,
      "grad_norm": 18.86443328857422,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 0.8028,
      "num_input_tokens_seen": 774144,
      "step": 968
    },
    {
      "epoch": 0.7955665024630542,
      "grad_norm": 23.222768783569336,
      "learning_rate": 1.4622590069211516e-07,
      "loss": 1.4217,
      "num_input_tokens_seen": 774912,
      "step": 969
    },
    {
      "epoch": 0.7963875205254516,
      "grad_norm": 8.55573844909668,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 0.2514,
      "num_input_tokens_seen": 775680,
      "step": 970
    },
    {
      "epoch": 0.7972085385878489,
      "grad_norm": 6.93865442276001,
      "learning_rate": 1.2798227928029482e-07,
      "loss": 0.1309,
      "num_input_tokens_seen": 776448,
      "step": 971
    },
    {
      "epoch": 0.7980295566502463,
      "grad_norm": 13.180190086364746,
      "learning_rate": 1.193150004542204e-07,
      "loss": 0.6615,
      "num_input_tokens_seen": 777216,
      "step": 972
    },
    {
      "epoch": 0.7988505747126436,
      "grad_norm": 10.811609268188477,
      "learning_rate": 1.109508849230001e-07,
      "loss": 0.2667,
      "num_input_tokens_seen": 778112,
      "step": 973
    },
    {
      "epoch": 0.7996715927750411,
      "grad_norm": 19.88156509399414,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 0.8507,
      "num_input_tokens_seen": 779008,
      "step": 974
    },
    {
      "epoch": 0.8004926108374384,
      "grad_norm": 9.75300407409668,
      "learning_rate": 9.513254770636137e-08,
      "loss": 0.1358,
      "num_input_tokens_seen": 779776,
      "step": 975
    },
    {
      "epoch": 0.8013136288998358,
      "grad_norm": 19.808162689208984,
      "learning_rate": 8.767851876239074e-08,
      "loss": 0.5421,
      "num_input_tokens_seen": 780544,
      "step": 976
    },
    {
      "epoch": 0.8021346469622331,
      "grad_norm": 15.224970817565918,
      "learning_rate": 8.052803859382174e-08,
      "loss": 0.4878,
      "num_input_tokens_seen": 781312,
      "step": 977
    },
    {
      "epoch": 0.8029556650246306,
      "grad_norm": 5.815689563751221,
      "learning_rate": 7.368119432699383e-08,
      "loss": 0.2517,
      "num_input_tokens_seen": 782080,
      "step": 978
    },
    {
      "epoch": 0.8037766830870279,
      "grad_norm": 12.610015869140625,
      "learning_rate": 6.71380693885476e-08,
      "loss": 0.595,
      "num_input_tokens_seen": 782848,
      "step": 979
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 34.80797576904297,
      "learning_rate": 6.089874350439506e-08,
      "loss": 0.3651,
      "num_input_tokens_seen": 783616,
      "step": 980
    },
    {
      "epoch": 0.8054187192118226,
      "grad_norm": 6.471802711486816,
      "learning_rate": 5.496329269875089e-08,
      "loss": 0.1536,
      "num_input_tokens_seen": 784384,
      "step": 981
    },
    {
      "epoch": 0.80623973727422,
      "grad_norm": 14.027620315551758,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 0.8613,
      "num_input_tokens_seen": 785152,
      "step": 982
    },
    {
      "epoch": 0.8070607553366174,
      "grad_norm": 4.0894927978515625,
      "learning_rate": 4.400430190586724e-08,
      "loss": 0.105,
      "num_input_tokens_seen": 785920,
      "step": 983
    },
    {
      "epoch": 0.8078817733990148,
      "grad_norm": 14.901893615722656,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 0.5748,
      "num_input_tokens_seen": 786688,
      "step": 984
    },
    {
      "epoch": 0.8087027914614121,
      "grad_norm": 13.256287574768066,
      "learning_rate": 3.426163113565417e-08,
      "loss": 0.4205,
      "num_input_tokens_seen": 787584,
      "step": 985
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 11.720132827758789,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 0.6171,
      "num_input_tokens_seen": 788480,
      "step": 986
    },
    {
      "epoch": 0.8103448275862069,
      "grad_norm": 16.0886173248291,
      "learning_rate": 2.5735755232134118e-08,
      "loss": 0.9786,
      "num_input_tokens_seen": 789376,
      "step": 987
    },
    {
      "epoch": 0.8111658456486043,
      "grad_norm": 34.74221420288086,
      "learning_rate": 2.192924752854042e-08,
      "loss": 1.1383,
      "num_input_tokens_seen": 790144,
      "step": 988
    },
    {
      "epoch": 0.8119868637110016,
      "grad_norm": 4.763917922973633,
      "learning_rate": 1.842708973447127e-08,
      "loss": 0.0578,
      "num_input_tokens_seen": 790912,
      "step": 989
    },
    {
      "epoch": 0.812807881773399,
      "grad_norm": 12.603562355041504,
      "learning_rate": 1.522932452260595e-08,
      "loss": 0.5051,
      "num_input_tokens_seen": 791680,
      "step": 990
    },
    {
      "epoch": 0.8136288998357963,
      "grad_norm": 12.768315315246582,
      "learning_rate": 1.233599085671e-08,
      "loss": 0.5956,
      "num_input_tokens_seen": 792448,
      "step": 991
    },
    {
      "epoch": 0.8144499178981938,
      "grad_norm": 3.3200089931488037,
      "learning_rate": 9.747123991141194e-09,
      "loss": 0.0669,
      "num_input_tokens_seen": 793216,
      "step": 992
    },
    {
      "epoch": 0.8152709359605911,
      "grad_norm": 9.588541030883789,
      "learning_rate": 7.462755470422078e-09,
      "loss": 0.1982,
      "num_input_tokens_seen": 793984,
      "step": 993
    },
    {
      "epoch": 0.8160919540229885,
      "grad_norm": 14.093292236328125,
      "learning_rate": 5.48291312886251e-09,
      "loss": 0.4733,
      "num_input_tokens_seen": 794880,
      "step": 994
    },
    {
      "epoch": 0.8169129720853858,
      "grad_norm": 7.825611591339111,
      "learning_rate": 3.807621090218261e-09,
      "loss": 0.3173,
      "num_input_tokens_seen": 795648,
      "step": 995
    },
    {
      "epoch": 0.8177339901477833,
      "grad_norm": 14.921428680419922,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 0.8491,
      "num_input_tokens_seen": 796416,
      "step": 996
    },
    {
      "epoch": 0.8185550082101807,
      "grad_norm": 19.54855728149414,
      "learning_rate": 1.3707658621964215e-09,
      "loss": 0.9054,
      "num_input_tokens_seen": 797184,
      "step": 997
    },
    {
      "epoch": 0.819376026272578,
      "grad_norm": 13.367032051086426,
      "learning_rate": 6.092323651313292e-10,
      "loss": 0.5318,
      "num_input_tokens_seen": 797952,
      "step": 998
    },
    {
      "epoch": 0.8201970443349754,
      "grad_norm": 8.03046989440918,
      "learning_rate": 1.5230855524017708e-10,
      "loss": 0.3967,
      "num_input_tokens_seen": 798720,
      "step": 999
    },
    {
      "epoch": 0.8210180623973727,
      "grad_norm": 2.507641077041626,
      "learning_rate": 0.0,
      "loss": 0.0536,
      "num_input_tokens_seen": 799744,
      "step": 1000
    },
    {
      "epoch": 0.8210180623973727,
      "eval_loss": 0.4565405547618866,
      "eval_runtime": 11.0251,
      "eval_samples_per_second": 110.747,
      "eval_steps_per_second": 13.877,
      "num_input_tokens_seen": 799744,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 799744,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.6028463412412416e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
